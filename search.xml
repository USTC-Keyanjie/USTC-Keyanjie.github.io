<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>分布式与云计算复习笔记(updating)</title>
      <link href="/studynote-of-distributed-system/"/>
      <url>/studynote-of-distributed-system/</url>
      
        <content type="html"><![CDATA[<p>分布式与云计算2019春季课程笔记</p><blockquote><p>这个东西吧，太简单了，我们这里不讲了</p><p>这个东西吧，比较难，后续我们在讲吧(后续后续。。。)</p><p>这里吧，(喝口水，ppt翻页)，直接跳过了</p><p>天文地理鲁迅balabala，石竹老师先退票再买票，最后没抢到票我感到非常高兴233333</p></blockquote><p>end</p><a id="more"></a><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=3&id=902907641&auto=1&height=66"></iframe><h1 id="Chapter-1-概述"><a href="#Chapter-1-概述" class="headerlink" title="Chapter 1 概述"></a>Chapter 1 概述</h1><h2 id="分布式系统的定义"><a href="#分布式系统的定义" class="headerlink" title="分布式系统的定义"></a>分布式系统的定义</h2><blockquote><p>分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关的系统。</p></blockquote><p>为了使种类各异的计算机和网络都呈现为单个的系统，分布式系统常常通过一个”软件层”组织起来，该”软件层”在逻辑上位于有用户和应用程序组成的高层与有操作系统组成的低层之间。这样的分布式系统又称为<strong>中间件(middleware)</strong>。</p><p><img src="/studynote-of-distributed-system/中间件.png" alt="中间件"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><h3 id="使资源可访问"><a href="#使资源可访问" class="headerlink" title="使资源可访问"></a>使资源可访问</h3><p>分布式系统最<strong>主要</strong>的目标是使用户能够方便地访问远程资源，并且以一种受控的方式与其他用户共享这些资源。</p><h3 id="透明性"><a href="#透明性" class="headerlink" title="透明性"></a>透明性</h3><p>分布式系统的<strong>重要</strong>目标之一是将它的进程和资源实际上在多台计算机上分布这样一个事实隐藏起来。如果一个分布式系统在用户和应用程序面前呈现为单个计算机系统，这样的分布式系统就称为是<strong>透明的</strong>。</p><h4 id="透明的类型"><a href="#透明的类型" class="headerlink" title="透明的类型"></a>透明的类型</h4><ul><li>访问透明性：对不同数据表示形式以及资源访问方式的隐藏；</li><li>位置透明性：用户无法判别资源在系统中的物理位置；</li><li>迁移透明性：如果分布式系统中的资源移动不会影响该资源的访问方式，就可以说这种分布式系统能提供；</li><li>重定位透明性：如果资源可以在接受访问的同时进行重新定位，而不引起用户和应用程序的注意，拥有这种资源的系统能提供。</li><li>复制透明性：对同一个资源存在多个副本这样一个事实的隐藏；</li><li>并发透明性：隐藏资源是否由若干相互竞争的用户共享这一事实；</li><li>故障透明性：隐藏资源的故障与恢复。</li></ul><h4 id="透明度"><a href="#透明度" class="headerlink" title="透明度"></a>透明度</h4><p>在设计并实现分布式系统时，把实现分布的透明性作为目标是正确的，但是应该将它和其他方面的问题（比如性能)结合起来考虑。</p><h3 id="开放性"><a href="#开放性" class="headerlink" title="开放性"></a>开放性</h3><p>开放式的分布式系统：根据一系列准则来提供服务，这些准则描述了所提供服务的语法和语义。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>系统的可扩展性至少可以通过三个方面来度量：</p><ol><li>规模可扩展：可方便向系统里加入更多用户和资源；</li><li>地域可扩展：可使系统中的用户与资源相隔十分遥远；</li><li>管理可扩展：即使分布式系统跨越多个独立的管理机构，仍然可以方便的对其进行管理。</li></ol><h4 id="可扩展性问题"><a href="#可扩展性问题" class="headerlink" title="可扩展性问题"></a>可扩展性问题</h4><p>分布式算法：</p><ol><li>没有任何计算机拥有整个系统的全局信息；</li><li>计算机只根据本地信息做出决策；</li><li>某台计算机的故障不会使算法崩溃；</li><li>不存在<strong>全局时钟</strong>。</li></ol><h4 id="扩展技术"><a href="#扩展技术" class="headerlink" title="扩展技术"></a>扩展技术</h4><h5 id="隐藏通信等待时间"><a href="#隐藏通信等待时间" class="headerlink" title="隐藏通信等待时间"></a>隐藏通信等待时间</h5><ol><li>使用<strong>异步通信</strong>，通常用于批处理系统；</li><li>启动一个新的控制线程来执行请求。</li></ol><p><strong>问题</strong>：有许多应用程序不适用此方式。</p><h5 id="分布技术"><a href="#分布技术" class="headerlink" title="分布技术"></a>分布技术</h5><p>把某个组件分割成多个部分，然后再把他们分散到系统中去。</p><p>例：DNS、万维网WWW</p><h5 id="复制技术"><a href="#复制技术" class="headerlink" title="复制技术"></a>复制技术</h5><p>对组件进行复制并将副本分部到系统各处。</p><p><strong>缓存</strong>是复制的一种特殊形式。缓存一般是在访问资源的客户<strong>附近</strong>制作该资源的副本。</p><h2 id="分布式系统的类型"><a href="#分布式系统的类型" class="headerlink" title="分布式系统的类型"></a>分布式系统的类型</h2><h3 id="分布式计算系统"><a href="#分布式计算系统" class="headerlink" title="分布式计算系统"></a>分布式计算系统</h3><p>用于高性能计算任务的系统。</p><h4 id="集群计算系统"><a href="#集群计算系统" class="headerlink" title="集群计算系统"></a>集群计算系统</h4><p>底层硬件是由类似的工作站或PC集组成，通过高速的局域网紧密连接起来的。而且，每个节点运行的是相同的操作系统。</p><p>集群计算系统的特点是同构性，大多数情况下，集群中的计算机都是相同的：</p><ul><li>有相同的操作系统</li><li>通过同一网络连接</li></ul><p><img src="/studynote-of-distributed-system/集群计算系统一个示例.png" alt="集群计算系统一个示例"></p><h4 id="网格计算系统"><a href="#网格计算系统" class="headerlink" title="网格计算系统"></a>网格计算系统</h4><p>组成分布式系统的子分组通常构建成一个计算机系统联盟，每个系统归属于不同的管理域。</p><p>网格计算系统具有高度异构性：</p><ul><li>硬件</li><li>操作系统</li><li>网络</li><li>管理域</li><li>安全策略</li></ul><p>都不尽相同。</p><p>以<strong>虚拟组织</strong>的方式，把来自不同计算机组织的资源集中起来，是一组人或机构协调工作。属于同一虚拟组织的人，具有访问提供给该组织的资源的权限。</p><p><img src="/studynote-of-distributed-system/网格计算系统的分层体系结构.png" alt="网格计算系统的分层体系结构"></p><p><strong>光纤层</strong>：在特定站点提供对局部资源的接口。这些接口都进行了定制，以允许在某个虚拟组织中实现资源共享。</p><p><strong>连接层</strong>：由通信协议组成，用于支持网格事务处理，延伸多个资源的使用。例如，用于在资源之间传输数据或从远程地点访问资源的协议。另外，连接层还有安全协议，用于进行用户和资源的认证。</p><p><strong>资源层</strong>：负责管理单个资源。它使用由连接层提供的功能，直接调用对光线层可用的接口。</p><p><strong>汇集层</strong>：负责处理对多个资源的访问，通常由资源分派、把任务分配和调度到多资源以及数据复制等服务组成。连接层和资源层由相对较小、较标准的协议集组成，而汇集层由很多用于不同目的的不同协议组成。</p><p><strong>应用层</strong>：由应用程序组成。</p><h3 id="分布式信息系统"><a href="#分布式信息系统" class="headerlink" title="分布式信息系统"></a>分布式信息系统</h3><h3 id="分布式普适系统"><a href="#分布式普适系统" class="headerlink" title="分布式普适系统"></a>分布式普适系统</h3><h1 id="Chapter-2-体系结构"><a href="#Chapter-2-体系结构" class="headerlink" title="Chapter 2 体系结构"></a>Chapter 2 体系结构</h1><h2 id="体系结构的样式"><a href="#体系结构的样式" class="headerlink" title="体系结构的样式"></a>体系结构的样式</h2><p>根据组件、组件之间相互的连接方式、组件之间的数据交换以及这些元素如何集成到一个系统中来定义。</p><ul><li><strong>组件Compinent</strong>：一个模块单元，可以提供良好定义接口，在其环境中是可替换的。</li><li><strong>链接器Connector</strong>：在组件之间传递通信、使组件相互协调和协作。</li></ul><p>根据组件和连接器的使用，划分成<strong>不同体系结构</strong>：</p><ol><li><p>分层体系结构</p><p><img src="/studynote-of-distributed-system/分层体系结构样式.png" alt="分层体系结构样式"></p><p>组件组成了不同的层，其中$L_i$层中的组员可以调用下面的层$L_{i-1}$。</p></li><li><p>基于对象的体系结构</p><p><img src="/studynote-of-distributed-system/对象体系结构样式.png" alt="对象体系结构样式"></p><p>每个对象都对应一个组件，这些组件是通过(远程)过程调用机制来连接的。</p></li><li><p>以数据为中心的体系结构</p><p>通过一个公用(被动或主动的)仓库进行通信。</p></li><li><p>基于事件的体系结构</p><p><img src="/studynote-of-distributed-system/基于事件的体系结构样式.png" alt="基于事件的体系结构样式"></p><p><img src="/studynote-of-distributed-system/基于共享数据空间的体系结构样式.png" alt="基于共享数据空间的体系结构样式"></p><p>通过事件的传播来通信。</p></li></ol><h2 id="系统体系结构"><a href="#系统体系结构" class="headerlink" title="系统体系结构"></a>系统体系结构</h2><h3 id="集中式体系结构"><a href="#集中式体系结构" class="headerlink" title="集中式体系结构"></a>集中式体系结构</h3><p><strong>客户-服务器</strong>的交互方式，又被称为<strong>请求-回复</strong>行为。</p><p>如果某个操作可以重复多次而无害处，那么称它是<strong>幂等</strong>的。</p><h4 id="应用分层"><a href="#应用分层" class="headerlink" title="应用分层"></a>应用分层</h4><p><strong>客户-服务器</strong>模型分为三层：</p><ol><li>用户接口层，用于与用户交互；</li><li>处理层，包含应用程序；</li><li>数据层，管理要使用的实际数据。</li></ol><h4 id="多层体系结构"><a href="#多层体系结构" class="headerlink" title="多层体系结构"></a>多层体系结构</h4><p><img src="/studynote-of-distributed-system/各种客户-服务器组织结构.png" alt="各种客户-服务器组织结构"></p><h3 id="非集中式体系结构"><a href="#非集中式体系结构" class="headerlink" title="非集中式体系结构"></a>非集中式体系结构</h3><h4 id="结构化的点对点体系结构"><a href="#结构化的点对点体系结构" class="headerlink" title="结构化的点对点体系结构"></a>结构化的点对点体系结构</h4><p><strong>点对点</strong>体系结构：在结构化的点对点体系机构中，覆盖网络是一个确定性的过程来构成的。这个使用最多的进程是通过一个<strong>分布式哈希表</strong>来组织进程的。</p><p><img src="/studynote-of-distributed-system/Chord系统中，从数据项到结点的映射.png" alt="Chord系统中，从数据项到结点的映射"></p><p>####非结构化的点对点体系结构 </p><h4 id="覆盖网络的拓扑管理"><a href="#覆盖网络的拓扑管理" class="headerlink" title="覆盖网络的拓扑管理"></a>覆盖网络的拓扑管理</h4><h4 id="超级对等体"><a href="#超级对等体" class="headerlink" title="超级对等体"></a>超级对等体</h4><p><img src="/studynote-of-distributed-system/超级对等体网络中结点的分层组织结构.png" alt="超级对等体网络中结点的分层组织结构"></p><p>能维护一个索引或者充当一个代理程序的结点。控制管理多个常规对等体。</p>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed system </tag>
            
            <tag> study note </tag>
            
            <tag> cloud computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab3：词频统计</title>
      <link href="/hadoop-demo-wordcount/"/>
      <url>/hadoop-demo-wordcount/</url>
      
        <content type="html"><![CDATA[<p>使用hadoop写一次词频统计的demo。</p><a id="more"></a><p>具体的操作细节有大佬已经写好了<a href="https://github.com/wangyu-/files/wiki/ds_lab3" target="_blank" rel="noopener">wiki</a>，有需要请移步，我这里只分析java代码细节。</p><h1 id="全部代码"><a href="#全部代码" class="headerlink" title="全部代码"></a>全部代码</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">  sum += val.get();</span><br><span class="line">  &#125;</span><br><span class="line">  result.set(sum);</span><br><span class="line">  context.write(key, result);</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h1 id="Map操作"><a href="#Map操作" class="headerlink" title="Map操作"></a>Map操作</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">      word.set(itr.nextToken());</span><br><span class="line">      context.write(word, one);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>自己定义一个类，这里取名为TokenizerMapper。这个类继承自Mapper类，继承时要指定4个泛型，分别表示：</p><ul><li>输入键key的参数类型</li><li>输入值value的参数类型</li><li>输出键key的参数类型</li><li>输出值value的参数类型</li></ul><p>比如这里，输入的是文本信息，输入的value是文本中的一行文字，类型为Text，而输入的key表示该行首字母相对于文本文件首地址的偏移量，类型为java最大的类Object。 而输出的信息是词频信息，key表示一个单词，类型为Text，value表示其词频，类型为IntWritabe。</p><p>第5行定义了一个常量对象one，就表示数量1，后面出现一个单词就记录出现1次。</p><p>第6行开始实现了map方法，参数有输入信息的key和value，还有上下文context。</p><p>第7行new了一个StringTokenizer类的实例itr，在构造对象时，就把value的字符创按分隔符分成了一个个单词。</p><ul><li><code>itr.hasMoreTokens()</code>表示是否后面还有单词</li><li><code>itr.nextToken()</code>表示下一个单词</li></ul><p><code>context.write(word, one);</code>表示将这个单词的词频记为1，写入context用以记录。</p><h1 id="Reduce操作"><a href="#Reduce操作" class="headerlink" title="Reduce操作"></a>Reduce操作</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">sum += val.get();</span><br><span class="line">&#125;</span><br><span class="line">result.set(sum);</span><br><span class="line">context.write(key, result);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>自己定义一个类，取名为IntSumReducer，继承自Reduce类，同样指定4个泛型，分别表示：</p><ul><li>输入键key的参数类型</li><li>输入值value的参数类型</li><li>输出键key的参数类型</li><li>输出值value的参数类型</li></ul><p>在这里输入是Map操作的输出，即词频信息，而输出是整合好的词频信息。</p><p>第4行实现reduce方法，参数key是单词，<code>Iterable&lt;IntWritable&gt; values</code>是一个可迭代的集合，表示这个单词所有的词频信息，context是上下文。</p><p>5到8行遍历统计词频，用sum来计数，最后在第10行写入到这个key的value。</p><h1 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">  job.setJarByClass(WordCount.class);</span><br><span class="line">  job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">  job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">  job.setReducerClass(IntSumReducer.class);</span><br><span class="line">  job.setOutputKeyClass(Text.class);</span><br><span class="line">  job.setOutputValueClass(IntWritable.class);</span><br><span class="line">  FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><ul><li><code>Configuration conf = new Configuration();</code>从Hadoop的配置文件里读取参数；</li><li><code>job.setJarByClass(WordCount.class);</code>根据WordCount类的位置设置Jar文件；</li><li><code>job.setMapperClass(TokenizerMapper.class);</code> 设置Mapper ；</li><li><code>job.setCombinerClass(IntSumReducer.class);</code>这句代码要提一下，因为如果所有的slave都分别做map操作，然后把信息全部返回给master节点，导致master节点负载很大，也会加大网络通信量。所以这个combiner操作相当于slave节点上自己先做一次reduce操作，再把信息传给master节点reduce，有助于提高性能；</li><li><code>job.setReducerClass(IntSumReducer.class);</code>设置Reduce；</li><li><code>job.setOutputKeyClass(Text.class);</code>和<code>job.setOutputValueClass(IntWritable.class);</code> 分别设置输出key的类型和value的类型；</li><li><code>FileInputFormat.addInputPath(job, new Path(args[0]));</code>设置输入文件，它是args第一个参数 ；</li><li><code>FileOutputFormat.setOutputPath(job, new Path(args[1]));</code>设置输出文件，将输出结果写入这个文件里，它是args第二个参数 ;</li><li><code>System.exit(job.waitForCompletion(true) ? 0 : 1);</code>等待执行结果，成功执行就退出码设置为0，否则为1。</li></ul>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed system </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab2：Hadoop Distributed File System(HDFS)上的基本操作</title>
      <link href="/Basic-operations-on-HDFS/"/>
      <url>/Basic-operations-on-HDFS/</url>
      
        <content type="html"><![CDATA[<p>分布式第二次实验。</p><a id="more"></a><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>使用master节点进行操作，另外两个节点也要开机。</p><h2 id="先进入root账户"><a href="#先进入root账户" class="headerlink" title="先进入root账户"></a>先进入root账户</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div><h2 id="将bin目录和sbin目录里的命令配入环境变量"><a href="#将bin目录和sbin目录里的命令配入环境变量" class="headerlink" title="将bin目录和sbin目录里的命令配入环境变量"></a>将bin目录和sbin目录里的命令配入环境变量</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>按下<code>i</code>，在最后添加一行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/hadoop-2.8.5/sbin:/usr/local/hadoop/hadoop-2.8.5/bin</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p><h2 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure></div><h2 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h2><h3 id="查看HDFS上当前目录下所有文件"><a href="#查看HDFS上当前目录下所有文件" class="headerlink" title="查看HDFS上当前目录下所有文件"></a>查看HDFS上当前目录下所有文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure></div><h3 id="递归查看HDFS上当前目录下所有文件"><a href="#递归查看HDFS上当前目录下所有文件" class="headerlink" title="递归查看HDFS上当前目录下所有文件"></a>递归查看HDFS上当前目录下所有文件</h3><p>这条命令会递归进入每个文件夹，展示出所有文件。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls -R /</span><br></pre></td></tr></table></figure></div><h3 id="在HDFS上新建文件夹"><a href="#在HDFS上新建文件夹" class="headerlink" title="在HDFS上新建文件夹"></a>在HDFS上新建文件夹</h3><h4 id="方式1-逐个建立文件夹"><a href="#方式1-逐个建立文件夹" class="headerlink" title="方式1 逐个建立文件夹"></a>方式1 逐个建立文件夹</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /user</span><br><span class="line">hadoop fs -mkdir /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h4 id="方式2-递归建立文件夹"><a href="#方式2-递归建立文件夹" class="headerlink" title="方式2 递归建立文件夹"></a>方式2 递归建立文件夹</h4><p>这种方式下，如果要建立的文件夹父目录不存在则同时建立父目录的文件夹。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h3 id="本地准备一份文件"><a href="#本地准备一份文件" class="headerlink" title="本地准备一份文件"></a>本地准备一份文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line">mkdir charles1</span><br><span class="line">mkdir charles2   # 为后续操作做准备</span><br><span class="line">cd charles1</span><br><span class="line">vim ds2019.txt</span><br></pre></td></tr></table></figure></div><p>写入：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Distributed System 2019Spring</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p><h3 id="向HDFS上传文件"><a href="#向HDFS上传文件" class="headerlink" title="向HDFS上传文件"></a>向HDFS上传文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put /tmp/charles1/ds2019.txt /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h3 id="查看是否上传成功"><a href="#查看是否上传成功" class="headerlink" title="查看是否上传成功"></a>查看是否上传成功</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls -R /</span><br></pre></td></tr></table></figure></div><h3 id="在Terminal显示文件内容"><a href="#在Terminal显示文件内容" class="headerlink" title="在Terminal显示文件内容"></a>在Terminal显示文件内容</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /user/hadoop-user/ds2019.txt</span><br></pre></td></tr></table></figure></div><h3 id="下载HDFS上的文件"><a href="#下载HDFS上的文件" class="headerlink" title="下载HDFS上的文件"></a>下载HDFS上的文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /user/hadoop-user/ds2019.txt /tmp/charles2</span><br></pre></td></tr></table></figure></div><h3 id="删除HDFS上的文件"><a href="#删除HDFS上的文件" class="headerlink" title="删除HDFS上的文件"></a>删除HDFS上的文件</h3><h4 id="方式1-删除某个文件"><a href="#方式1-删除某个文件" class="headerlink" title="方式1 删除某个文件"></a>方式1 删除某个文件</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm /user/hadoop-user/ds2019.txt</span><br></pre></td></tr></table></figure></div><h4 id="方式2-递归删除文件夹"><a href="#方式2-递归删除文件夹" class="headerlink" title="方式2 递归删除文件夹"></a>方式2 递归删除文件夹</h4><p>若被删除的文件夹下还有文件，则一同删除</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r /user/</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> distributed system </tag>
            
            <tag> lab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab1：Ubuntu16.04上Hadoop环境安装</title>
      <link href="/Hadoop-installed-in-Ubuntu1604/"/>
      <url>/Hadoop-installed-in-Ubuntu1604/</url>
      
        <content type="html"><![CDATA[<p>分布式第一次实验，怎么坑这么多！🤯</p><a id="more"></a><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>你需要开3台虚拟机，一台作为master主节点，两个作为slave从节点，分别叫做slave1和slave2。</p><h2 id="VMware虚拟机桥接模式设置"><a href="#VMware虚拟机桥接模式设置" class="headerlink" title="VMware虚拟机桥接模式设置"></a>VMware虚拟机桥接模式设置</h2><p>参考这个<a href="https://blog.csdn.net/ccyhummer/article/details/80714430" target="_blank" rel="noopener">文章</a>，（没用过VMware23333）</p><h2 id="Parallels虚拟机桥接模式设置"><a href="#Parallels虚拟机桥接模式设置" class="headerlink" title="Parallels虚拟机桥接模式设置"></a>Parallels虚拟机桥接模式设置</h2><p>参考这个<a href="https://blog.csdn.net/wuxiangmujingli/article/details/52671448" target="_blank" rel="noopener">文章</a>，IP的起始地址设为192.168.1.11，结束地址设为192.168.1.31</p><h2 id="hostname的配置操作"><a href="#hostname的配置操作" class="headerlink" title="hostname的配置操作"></a>hostname的配置操作</h2><ol><li><p>修改hosts文件，如下（master节点、slave1节点、slave2节点都做）：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br><span class="line">gedit /etc/hosts</span><br></pre></td></tr></table></figure></div><p>修改内容如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.11master</span><br><span class="line">192.168.1.12slave1</span><br><span class="line">192.168.1.13slave2</span><br></pre></td></tr></table></figure></div></li><li><p>修改hostname</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> master节点上修改为master，salve1节点和slave2节点上分别修改为slave1、slave2</span><br><span class="line">hostname master/slave1/slave2</span><br></pre></td></tr></table></figure></div><p>这样不能彻底修改hostname（主机名），重启后还会还原到默认的Ubuntu，要彻底修改要修改/etc/hostname，namenode和datanode各自修改为自己的hostname</p><p>直接用编辑器打开<strong>/etc/hostname</strong>这个文件<strong>，把原来的名称</strong>删掉<strong>，</strong>不要用#注释，直接删掉，因为#没用，<strong>修改内容</strong>：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> master节点上修改为master，salve1节点和slave2节点上分别修改为slave1、slave2</span><br><span class="line">master/slave1/slave2</span><br></pre></td></tr></table></figure></div><p>退出shell客户端，重新进入，并且换成root操作(这是教程上的一步，没看懂这是什么操作)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div></li><li><p>这些工作都做好了，互相ping一下看看能不能ping通，ping 节点名称</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping master/slave1/slave2</span><br></pre></td></tr></table></figure></div></li></ol><h2 id="安装jdk-所有节点都做"><a href="#安装jdk-所有节点都做" class="headerlink" title="安装jdk(所有节点都做)"></a>安装jdk(所有节点都做)</h2><ol><li><p>前往<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">oracle Java官网</a>下载JDK</p></li><li><p>确保当前是系统账户</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div></li><li><p>解压缩到指定目录（以jdk-8u201-linux-x64.tar.gz为例）</p><p>创建目录:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/lib/jvm</span><br></pre></td></tr></table></figure></div><p>解压缩到该目录:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u201-linux-x64.tar.gz -C /usr/lib/jvm</span><br></pre></td></tr></table></figure></div></li><li><p>修改环境变量，如果提示没有装vim就使用<code>apt install vim</code>装一个:　　</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>在文件末尾追加下面内容：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>set oracle jdk environment</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201  # 这里要注意目录要换成自己解压的jdk 目录</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre  </span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib  </span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure></div></li><li><p>使环境变量马上生效：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></div></li><li><p>系统注册此jdk</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_201/bin/java 300</span><br></pre></td></tr></table></figure></div></li><li><p>查看java版本，看看是否安装成功：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></div></li></ol><h2 id="SSH无密码验证配置"><a href="#SSH无密码验证配置" class="headerlink" title="SSH无密码验证配置"></a>SSH无密码验证配置</h2><h3 id="在master节点上："><a href="#在master节点上：" class="headerlink" title="在master节点上："></a>在master节点上：</h3><ol><li><p>先安装ssh</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure></div></li><li><p>先创建本地公钥：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.ssh</span><br><span class="line">cd ~/.ssh               </span><br><span class="line">rm ./id_rsa*            # 删除之前生成的公匙（如果有）</span><br><span class="line">ssh-keygen -t rsa       # 一直按回车就可以</span><br></pre></td></tr></table></figure></div></li><li><p>让 Master 节点需能无密码 SSH 本机，在 Master 节点上执行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br></pre></td></tr></table></figure></div></li><li><p>完成后可执行 <code>ssh Master</code> 验证一下（可能需要输入 yes，成功后执行 <code>exit</code> 返回原来的终端）。接着在 master 节点将上公匙传输到 slave1 节点和slave2节点：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp ./id_rsa.pub parallels@slave1:/home/parallels/  # 向slave1节点传</span><br><span class="line">scp ./id_rsa.pub parallels@slave2:/home/parallels/  # 向slave2节点传</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在slave1节点上（slave2同理）："><a href="#在slave1节点上（slave2同理）：" class="headerlink" title="在slave1节点上（slave2同理）："></a>在slave1节点上（slave2同理）：</h3><ol><li><p>将 ssh 公匙加入授权：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略</span><br><span class="line">cat /home/parallels/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在master节点上测试："><a href="#在master节点上测试：" class="headerlink" title="在master节点上测试："></a>在master节点上测试：</h3><p>执行<code>ssh slave1</code>，会有这样的结果：</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_1.png" alt="ssh slave1"></p><p>那么说明成功了，再执行<code>ssh slave2</code>看看。</p><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><p><strong>所有节点上</strong>，执行<code>ufw disable</code>就好。</p><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><h3 id="在master节点上：-1"><a href="#在master节点上：-1" class="headerlink" title="在master节点上："></a>在master节点上：</h3><p>[注]全程在root账户下运行</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br></pre></td></tr></table></figure></div><ol><li><p>到Hadoop<a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener">官网</a>下载binary的hadoop，我下载的是<strong>hadoop-2.8.5.tar.gz</strong>文件</p></li><li><p>在/usr/local目录下建立hadoop目录</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/local/hadoop</span><br></pre></td></tr></table></figure></div></li><li><p>把<strong>hadoop-2.8.5.tar.gz</strong>拷贝到/usr/local/<strong>hadoop</strong>目录下，然后解压，这里的<code>&lt;path&gt;</code>写自己的下载位置</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &lt;path&gt;/hadoop-2.8.5.tar.gz /usr/local/hadoop</span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">tar –zxvf hadoop-2.8.5.tar.gz</span><br><span class="line">cd hadoop-2.8.5</span><br></pre></td></tr></table></figure></div></li><li><p>在/usr/local/<strong>hadoop</strong>目录下新建tmp文件夹和hdfs文件夹</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">mkdir tmp</span><br><span class="line">mkdir hdfs</span><br></pre></td></tr></table></figure></div></li><li><p>编辑（可用<code>vim hadoop-env.sh</code>也可双击打开编辑）hadoop-2.8.5/etc/hadoop/<strong>hadoop-env.sh</strong>文件，把JAVA_HOME设置成Java安装根路径，如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/hadoop-2.8.5/etc/hadoop</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201</span><br></pre></td></tr></table></figure></div></li><li><p>新建<strong>slaves</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi slaves</span><br></pre></td></tr></table></figure></div><p>添加这两条</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></div></li><li><p>编辑<strong>master</strong>文件，全文改为<code>master</code></p></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>core-site.xml</strong>文件(我在文件系统里使用的sublime修改的文件，也可以直接双击使用gedit打开)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>hdfs-site.xml</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>mapred-site.xml</strong>文件(可能需要先重命名，默认文件名为 mapred-site.xml.template)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>yarn-site.xml</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>将配置好的文件复制到<strong>各个slave</strong>节点上：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo rm -r ./hadoop/tmp     # 删除 Hadoop 临时文件</span><br><span class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制</span><br><span class="line">cd ~</span><br><span class="line">scp ./hadoop.master.tar.gz slave1:/home/parallels</span><br><span class="line">scp ./hadoop.master.tar.gz slave2:/home/parallels</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在slave1节点上："><a href="#在slave1节点上：" class="headerlink" title="在slave1节点上："></a>在slave1节点上：</h3><ol><li><p>解压文件，并修改<strong>owner</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br><span class="line">rm -r /usr/local/hadoop    # 删掉旧的（如果存在）</span><br><span class="line">tar -zxf /home/parallels/hadoop.master.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="回到master节点"><a href="#回到master节点" class="headerlink" title="回到master节点"></a>回到master节点</h3><ol><li><p>将bin目录和sbin目录里的命令配入环境变量</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>按下<code>i</code>，在最后添加一行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/hadoop-2.8.5/sbin:/usr/local/hadoop/hadoop-2.8.5/bin</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p></li><li><p>在 Master 节点执行 NameNode 的格式化</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure></div></li><li><p>接着可以启动 hadoop 了，启动需要在 <strong>Master</strong> 节点的<strong>sbin</strong>文件夹中进行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></div><p><img src="/Hadoop-installed-in-Ubuntu1604/3_5.png" alt="hadoop启动"></p><p>通过命令 <code>jps</code> 可以查看各个节点所启动的进程。正确的话，在 Master 节点上可以看到 NameNode、ResourceManager、SecondrryNameNode、JobHistoryServer 进程。</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_2.png" alt="Master节点上的进程"></p></li><li><p>分别进入slave1节点和slave2节点，使用<code>jps</code>命令查看运行情况</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_6.png" alt="slave1运行情况"></p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_8.png" alt="slave2运行情况"></p></li><li><p>在namenode上查看集群状态</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -report</span><br></pre></td></tr></table></figure></div><p>此时可以通过在浏览器中打开<a href="http://master:50070" target="_blank" rel="noopener">http://master:50070</a>查看。</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_3.png" alt="浏览器查看结果"></p></li></ol><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>如各位同学发现问题欢迎评论指正，评论功能终于打开了233333！</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢苏璐岩、<a href="wanghj.net">wanghj</a>、汪宇同学为本文指正错误！</p>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> distributed system </tag>
            
            <tag> lab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>round(number[, ndigits])</title>
      <link href="/round(number%5B,%20ndigits%5D)/"/>
      <url>/round(number%5B,%20ndigits%5D)/</url>
      
        <content type="html"><![CDATA[<p><strong>round函数是Python中常用的四舍五入的函数，但是今天试用了一下发现有点小坑</strong>。</p><a id="more"></a><p><strong>对0.5进行四舍五入，结果应该为1，可是Python解释器给的结果是：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(0.5)</span><br><span class="line">0</span><br></pre></td></tr></table></figure></div><p><strong>紧接着我又试了几个数，发现：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(1.5)</span><br><span class="line">2</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(2.5)</span><br><span class="line">2</span><br></pre></td></tr></table></figure></div><p><strong>round(1.5)还算正常，round(2.5)又很奇怪了，然后看了看Python文档，里面这样解释round函数：</strong></p><blockquote><p><strong>For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both round(0.5) and round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits (positive, zero, or negative). The return value is an integer if ndigits is omitted or None. Otherwise the return value has the same type as number.</strong></p></blockquote><p><strong>意思是会把这个输入的数保留到小数点后指定的ndigit位数，并且按照一定规则圆整。规则就是看看这个数更靠近左右两端中哪一端，然后就将这个数圆整为这一端，比如0.4就圆整为0，0.6就圆整为1。那如果是像0.5这样的到左右两端距离一样的情况，就圆整到偶数的一端，比如0.5就圆整到0，1.5就圆整到2。</strong></p><p><strong>文档里还写了这么一句话：</strong></p><blockquote><p><strong>Note: The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67instead of the expected 2.68. This is not a bug: it’s a result of the fact that most decimal fractions can’t be represented exactly as a float. See Floating Point Arithmetic: Issues and Limitations for more information.</strong></p></blockquote><p><strong>就是按照上述规则，round(2.675, 2)的结果应该是2.68，可是实际结果是：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(2.675, 2)</span><br><span class="line">2.67</span><br></pre></td></tr></table></figure></div><p><strong>这不是我们写了个bug，这是因为机器在存储浮点数2.675的时候，不会存储到这么精确，存储的数比2.675小一点点，应该差不多是2.674999999这样，所以才会被圆整到2.67，这个问题没法解决。</strong></p>]]></content>
      
      
      <categories>
          
          <category> note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> round </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大软院 – 人工智能期末考试复习</title>
      <link href="/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2-%E2%80%93-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/"/>
      <url>/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2-%E2%80%93-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>但愿看我博客复习的同学不会被8%的概率dropout吧~</p><a id="more"></a><h1 id="AI的四大主流流派"><a href="#AI的四大主流流派" class="headerlink" title="AI的四大主流流派"></a>AI的四大主流流派</h1><ol><li>符号主义（Symbolism）例：知识图谱</li><li>连接主义（Connectionism）例：深度神经网络</li><li>行为主义（Behaviourism）例：机器人</li><li>统计主义（Statisticsism）例：机器学习</li></ol><h1 id="AI、机器学习、深度学习三者之间的异同和关联"><a href="#AI、机器学习、深度学习三者之间的异同和关联" class="headerlink" title="AI、机器学习、深度学习三者之间的异同和关联"></a>AI、机器学习、深度学习三者之间的异同和关联</h1><ol><li>三者属于包含关系：AI包含机器学习，机器学习包含深度学习，深度学习主要指机器学习中的深度神经网络；</li></ol><p><img src="/科大软院-–-人工智能期末考试复习/三者关系.png" alt="三者关系"></p><ol start="2"><li><p>在机器学习中，需要先进行人工特征提取，再定义Model；而大部分深度学习只构建一个端对端的模型，没有人工特征提取；</p></li><li><p>在深度学习中，有时需要先进行数据清洗、格式转换、特征提取（卷积）等操作，再将数据喂给全连接神经网络。</p></li></ol><h1 id="机器学习的两个阶段"><a href="#机器学习的两个阶段" class="headerlink" title="机器学习的两个阶段"></a>机器学习的两个阶段</h1><ol><li><p>训练：“三步曲” on training set</p><ul><li><p>定义Model；</p></li><li><p>定义Loss/cost/error/objective  function；</p></li><li><p>如何找到Model中的最佳function：利用梯度下降来迭代调整参数，以使得损失函数达到最小值。深度神经网络通过反向传播来降低损失，优化模型。</p></li></ul></li><li><p>预测：on Devset and testing set</p><ul><li>前向传播：预测输出，计算Loss；</li><li>通过开发集（Devset）可以用于超参数调优，模型经过训练集训练，和开发集调优，然后交给测试集测试性能。</li></ul></li></ol><h2 id="几个基本概念："><a href="#几个基本概念：" class="headerlink" title="几个基本概念："></a>几个基本概念：</h2><ul><li>Epoch：对整个数据集进行一次forward和backward过程，被称为一个Epoch；</li><li>Batch_size：一次forward/backward过程中的训练样例数，被称为Batch_size；</li><li>Batch：使用训练集中一小部分样本对模型权重进行一次反向传播的参数更新，这一小部分样本被称为一个Batch；</li><li>Iteration：1个Iteration等于使用batchsize个样本训练一次。</li></ul><p>举个栗子🌰：假设我的训练集中总样本数为2048、Batch_size=128，那么我需要迭代(Iterations)16次才能完成一个Epoch。</p><h1 id="机器学习分类的定义及差异"><a href="#机器学习分类的定义及差异" class="headerlink" title="机器学习分类的定义及差异"></a>机器学习分类的定义及差异</h1><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习的目标是建立一个学习过程，将预测结果与“训练数据”（即输入数据）的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。</p><p>例：手写字符识别、肿瘤分类、预测天气、支持向量机、线性判别。</p><p>特点：训练样本数据和待分类的类别已知，且训练样本数据皆为标签数据。</p><h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><p>非监督学习从无标记的训练数据中推断结论，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。</p><p>例：聚类分析、主成分分析。</p><p>特点：训练样本数据和待分类的类别已知，但训练样本数据皆为非标签数据。</p><h2 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h2><p>半监督学习的训练数据通常是少量有标记数据及大量未标记数据，介于无监督学习和监督学习之间。</p><p>例：聚类假设、流形假设。</p><p>特点：训练样本数据和待分类的类别已知，然而训练样本既有标签数据，也有非标签数据。</p><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>强化学习的输入数据作为对模型的反馈，强调如何基于环境而行动，以取得最大化的预期利益。</p><p>与监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。</p><p>例：学习下围棋、打星际争霸、DotA2、双人德州扑克。（全部都是1v1的场景，目前强化学习领域对于多人博弈研究的很少）</p><p>特点：决策流程，激励系统，学习一系列的行动。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>迁移学习是通过从已学习的相关任务中迁移其知识来对需要学习的新任务进行提高。</p><p>例：牛津的VGG模型、谷歌的Inception模型和word2vec模型、微软的ResNet模型。</p><p>特点：需求的训练数据集合较小、训练时间较小、可以方便的进行迁移以满足个性化。</p><h1 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h1><p>对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</p><ul><li>任务T：分类，翻译等机器学习的目标任务</li><li>性能度量P：准确率</li><li>经验E：训练集</li></ul><h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><p>欠拟合：模型拟合不够，在训练集上拟合情况很差。往往会出现偏差大、方差小的情况；</p><p>过拟合：模型过度拟合，在训练集上拟合情况很好，但是在测试集上拟合情况很差。往往会出现偏差小、方差大的情况。</p><h2 id="机器学习中，出现欠拟合时，解决办法有："><a href="#机器学习中，出现欠拟合时，解决办法有：" class="headerlink" title="机器学习中，出现欠拟合时，解决办法有："></a>机器学习中，出现欠拟合时，解决办法有：</h2><ol><li>增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间；</li><li>尝试非线性模型，比如核SVM 、决策树、DNN等模型；</li><li>如果有正则项可以减小正则项参数λ；</li><li>Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。</li></ol><h2 id="深度学习中，出现欠拟合时，解决办法有："><a href="#深度学习中，出现欠拟合时，解决办法有：" class="headerlink" title="深度学习中，出现欠拟合时，解决办法有："></a>深度学习中，出现欠拟合时，解决办法有：</h2><ol><li>选择合适的损失函数；</li><li>尝试采用Mini-batch以及Batch Norm的方法进行优化；</li><li>选用其他激活函数；</li><li>采用自适应学习率的优化算法；</li><li>进行优化时考虑Momentum。</li></ol><h2 id="机器学习中，出现过拟合时，解决办法有："><a href="#机器学习中，出现过拟合时，解决办法有：" class="headerlink" title="机器学习中，出现过拟合时，解决办法有："></a>机器学习中，出现过拟合时，解决办法有：</h2><ol><li>交叉检验，通过交叉检验得到较优的模型参数;</li><li>特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</li><li>正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</li><li>如果有正则项则可以考虑增大正则项参数λ;</li><li>增加训练数据有限程度上可以避免过拟合;</li><li>Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等。</li></ol><h2 id="深度学习中，出现过拟合时，解决办法有："><a href="#深度学习中，出现过拟合时，解决办法有：" class="headerlink" title="深度学习中，出现过拟合时，解决办法有："></a>深度学习中，出现过拟合时，解决办法有：</h2><ol><li>早停，训练时可以每过n个Epoch就在验证集上检查误差，如果发现验证误差不降反增就可以停止训练了；</li><li>正则化，和机器学习一样，在深度学习中使用正则化完成权值衰减可以防止过拟合；</li><li>Dropout，Dropout也是一种正则化手段，指暂时丢弃一部分神经元及其连接。随机丢弃神经元可以防止过拟合，同时可以高效地连接不同网络架构。</li></ol><h1 id="误差来源分析"><a href="#误差来源分析" class="headerlink" title="误差来源分析"></a>误差来源分析</h1><h2 id="偏差bias"><a href="#偏差bias" class="headerlink" title="偏差bias"></a>偏差bias</h2><p>期望预测与真实标记的误差，偏差越大偏离理论值越大。</p><p>在一个训练集$D$上模型$f$对测试样本$x$预测输出为$f(x;D)$, 那么学习算法$f$对测试样本$x$的期望预测为：<br>$$<br>\bar{f}(x)=E_D[f(x;D)]<br>$$<br>这里用偏差的平方来表示偏差的计算公式：<br>$$<br>Bias^2(x)=(\bar{f}(x)-\hat{y})^2<br>$$</p><h2 id="方差variance"><a href="#方差variance" class="headerlink" title="方差variance"></a>方差variance</h2><p>预测模型的离散程度，方差越大离散程度越大。使用样本数相同的不同训练集产生的方差为:<br>$$<br>var(x)=E_D[(f(x;D)-\bar{f}(x))^2<br>$$</p><h2 id="噪声noise"><a href="#噪声noise" class="headerlink" title="噪声noise"></a>噪声noise</h2><p>真实标记与数据集中的实际标记间的偏差（$y_D$表示在数据集中的标记，$\hat{y}$表示真实标记，这两个可能不等）：</p><p>$$<br>\epsilon=E_D[(y_D-\hat{y})^2]<br>$$</p><h2 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h2><p>学习器在新样本上的误差称为“泛化误差”。可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). </p><p>以回归任务为例, 学习算法的平方预测误差期望为：</p><p>$$<br>Err(x)=E_D[(y_D-f(x;D))^2]<br>$$</p><p>对算法的期望泛化误差进行分解，就会发现：泛化误差=偏差的平方+方差+噪声:</p><p><img src="/科大软院-–-人工智能期末考试复习/bias-variance-proof.png" alt="bias-variance-proof"></p><p><img src="/科大软院-–-人工智能期末考试复习/bias-variance.png" alt="bias-variance"></p><h1 id="三类数据集"><a href="#三类数据集" class="headerlink" title="三类数据集"></a>三类数据集</h1><ul><li>训练集：用于学习参数</li><li>开发/验证集：用于挑选超参数</li><li>测试集：用于估计泛化误差</li></ul><h1 id="Cross-Validation-–-交叉验证"><a href="#Cross-Validation-–-交叉验证" class="headerlink" title="Cross Validation – 交叉验证"></a>Cross Validation – 交叉验证</h1><p>将数据集D划分成k个大小相似的互斥子集，每次用k-1个子集作为训练集，余下的子集做测试集，最终返回k个训练结果的平均值。交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。适用于数据集不是特别大时。</p><p><img src="/科大软院-–-人工智能期末考试复习/cross validation.png" alt="cross validation"></p><h1 id="参数-v-s-超参数"><a href="#参数-v-s-超参数" class="headerlink" title="参数 v.s. 超参数"></a>参数 v.s. 超参数</h1><p>模型参数是模型内部的配置变量，通过学习算法进行优化。例：神经网络中，层与层之间的权值W与偏置b。</p><p>超参数是一个学习算法的参数。它是不会被学习算法本身影响的，它优于训练，在训练中是保持不变的。例：学习率$\eta$，正则系数$\lambda$，模型阶数，模型类型，batch_size等。</p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>L1-norm: 向量元素绝对值之和，即：</p><p>$$<br>\left | \theta \right |_1=\sum_{i=1}^{n}\left | \theta_i \right |<br>$$</p><p>$$<br>L’(\theta)=L(\theta)+\lambda\left | \theta \right |_1<br>$$</p><p><em>——LASSO model: Tibshirani,1996</em></p><p>L2-norm: 各个元素的平方和，即：</p><p>$$<br>\left | \theta \right |_2=\sum_{i=1}^{n}\theta_i^2<br>$$</p><p>$$<br>L’(\theta)=L(\theta)+\lambda\left | \theta \right |_2<br>$$</p><p><em>——Ridge model: Hoerl,1970</em></p><p>Elastic Net (L1+L2):</p><p>$$<br>L’(\theta)=L(\theta)+\lambda[\rho\left | \theta \right |_1+(1-\rho)\left | \theta \right |_2 ]<br>$$</p><h2 id="正则化的作用："><a href="#正则化的作用：" class="headerlink" title="正则化的作用："></a>正则化的作用：</h2><p>对模型进行降阶，缩小模型空间，以解决过拟合的问题。</p><h1 id="如何加快模型的训练"><a href="#如何加快模型的训练" class="headerlink" title="如何加快模型的训练"></a>如何加快模型的训练</h1><h2 id="特征缩放-标准化"><a href="#特征缩放-标准化" class="headerlink" title="特征缩放/标准化"></a>特征缩放/标准化</h2><h3 id="Feature-Scaling-–-特征缩放-归一化"><a href="#Feature-Scaling-–-特征缩放-归一化" class="headerlink" title="Feature Scaling – 特征缩放/归一化"></a>Feature Scaling – 特征缩放/归一化</h3><p>输入值减去样本中最小值，然后除以样本范围，这样会使得结果永远在0~1的范围内，所有特征参数差不多一个速度优化到最低点。式子如下：</p><p>$$<br>x’=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p><h3 id="Mean-Normalization-–-均值归一化"><a href="#Mean-Normalization-–-均值归一化" class="headerlink" title="Mean Normalization – 均值归一化"></a>Mean Normalization – 均值归一化</h3><p>输入值减去样本均值，然后除以样本范围。式子如下：</p><p>$$<br>x’=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p><h3 id="zero-mean-normalization-0均值标准化"><a href="#zero-mean-normalization-0均值标准化" class="headerlink" title="zero-mean normalization - 0均值标准化"></a><em>zero-mean normalization</em> - 0均值标准化</h3><p>标准化：让输入的值减去样本平均数μ，再除以样本标准差σ。经过这样的处理，数据符合标准正态分布，即均值为0，标准差为1。</p><p>$$<br>{x}’=\frac{x-\mu}{\sigma}<br>$$</p><h2 id="梯度下降的变种"><a href="#梯度下降的变种" class="headerlink" title="梯度下降的变种"></a>梯度下降的变种</h2><h3 id="Gradient-Descent-–-梯度下降"><a href="#Gradient-Descent-–-梯度下降" class="headerlink" title="Gradient Descent – 梯度下降"></a>Gradient Descent – 梯度下降</h3><p>如果需要找到一个函数的局部极小值，必须朝着函数上当前点所对应梯度（或者是近似梯度）的反方向，前进规定步长的距离进行迭代搜索。</p><p>$$<br>w’ \leftarrow w - \eta \frac{\partial L(w,b)}{\partial w}<br>$$</p><p>$$<br>b’ \leftarrow b - \eta \frac{\partial L(w,b)}{\partial b}<br>$$</p><h3 id="为什么要以梯度的反方向为更新方向？"><a href="#为什么要以梯度的反方向为更新方向？" class="headerlink" title="为什么要以梯度的反方向为更新方向？"></a>为什么要以梯度的反方向为更新方向？</h3><p>因为梯度方向是函数方向导数最大的方向，所以沿着梯度方向的反方向更新的话，函数下降的变化率最大。</p><h3 id="Stochastic-Gradient-Descent-–-随机梯度下降"><a href="#Stochastic-Gradient-Descent-–-随机梯度下降" class="headerlink" title="Stochastic Gradient Descent – 随机梯度下降"></a>Stochastic Gradient Descent – 随机梯度下降</h3><p>随机梯度下降的损失函数（使用MSE作为损失函数）：</p><p>$$<br>L^{(i)}(w,b) = \frac{1}{2}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>通过公式可以看到，随机梯度下降每次更新只用到了一个样本，如果这个训练集有m个样本，那么梯度下降更新一次参数，随机梯度下降已经更新了m次参数了。</p><p>随机梯度下降的好处是：随机梯度下降的更新速度更快；</p><p>随机梯度下降所带来的坏处是：随机梯度下降的更新只参考了一个样本，所以更新时候的抖动现象很明显。</p><h3 id="Mini-batch-Gradient-Descent-–-Mini-batch梯度下降"><a href="#Mini-batch-Gradient-Descent-–-Mini-batch梯度下降" class="headerlink" title="Mini-batch Gradient Descent – Mini-batch梯度下降"></a>Mini-batch Gradient Descent – Mini-batch梯度下降</h3><p>Mini-batch梯度下降是梯度下降和随机梯度下降的中和版本，Mini-batch梯度下降每次更新所考虑的样本是可以被指定的，如果总共有m个样本，那就可以在1~m中任意指定。</p><p>如果每次更新时所参考的样本数合适，那么既兼顾了随机梯度下降更新速度快的特性，又兼顾了梯度下降更新的稳定性。</p><h2 id="调整学习率"><a href="#调整学习率" class="headerlink" title="调整学习率"></a>调整学习率</h2><p>当我们在训练过程中，发现loss下降的很慢时，可以适当增大学习率；发现loss不降反增的时候，要降低学习率。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad算法的学习率会根据迭代次数来放缓学习率，从而达到学习率越来越小的目的。通过公式可以看到，学习率是会一直除以前面所有梯度的平方和再开根号的，这一定是一个大于0的数，所以学习率会越来越小。但是防止一开始的时候梯度就是0，如果让分母变为0会导致错误的，所以后面还要跟一个很小的正数$\epsilon$，最终的式子是这样的：</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2+\epsilon}}g^t<br>$$</p><p>Adagrad算法也有很多不足：</p><ol><li>如果初始的学习率设置过大的话，这个学习率要除以一个较大梯度，那么此算法会对梯度的调节太大；</li><li>在训练的中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束。</li></ol><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>Adagrad算法的改进版RMSprop算法：</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1-\alpha)(g^t)^2}<br>$$</p><p>Adagrad和RMSprop算法这两个算法很相近，不同之处在于RMSprop算法增加了一个衰减系数α来控制历史信息的获取多少。</p><h3 id="SGD-with-Momentum-SGD-M"><a href="#SGD-with-Momentum-SGD-M" class="headerlink" title="SGD with Momentum (SGD-M)"></a>SGD with Momentum (SGD-M)</h3><p>SGD 在遇到沟壑时容易出现抖动现象。为此，可以为其引入动量 Momentum，加速 SGD 在正确方向的下降并抑制震荡。</p><p>$$<br>m_t \leftarrow \gamma m_{t-1} +\eta g^t \qquad w^t \leftarrow w^{t-1} - m_t<br>$$</p><p>这里多了一个$m_t$，可以将其想象为动量或者惯性，意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。如果上一次梯度和只一次同方向，$m_t$会越来越大，参数也会更新越来越快；如果方向不同，$m_t$会比上次更小，参数更新速度减慢。$\gamma$是取上一次更新的动量大小，通常取 0.9 左右。</p><p>这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。</p><h3 id="SGD-with-Nesterov-NAG"><a href="#SGD-with-Nesterov-NAG" class="headerlink" title="SGD with Nesterov(NAG)"></a>SGD with Nesterov(NAG)</h3><p>$$<br>g^t\leftarrow \frac{\partial L(w^t- \gamma m_{t-1})}{\partial w} \qquad m_t \leftarrow \gamma m_{t-1} +\eta g^t \qquad w^t \leftarrow w^{t-1}-m_t<br>$$</p><p>NAG算法在SGD-M上进一步改进，计算$g^t$时有所不同。简单解释来说就是，在SGD-M算法中，更新参数要用到上一次更新的动量；换句话说，下一次更新也会用到这一次更新的动量。那么可以通过这一次更新的动量大概预估出下一次把参数更新到哪里，然后提前去那个地方看看梯度，如果梯度方向改变很小，那么就知道下一次更新和这一次更新方向差不多，是朝着最低点前进，那么步子就可以迈大一点；如果梯度方向改变很大，那么就知道下一次更新在不断震荡的过程中，那么步子迈小一点，减小震荡幅度。</p><p>这个解释很不严谨，上面的式子可以转换为二阶导的形式，也就是每次更新，要看上一次更新的动量，当前点的梯度值，还有一个二阶导数。有兴趣看看这个文章一起愉快的推公式吧~ </p><h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><h2 id="如何区分回归问题与分类问题"><a href="#如何区分回归问题与分类问题" class="headerlink" title="如何区分回归问题与分类问题"></a>如何区分回归问题与分类问题</h2><p>输出值可以有限枚举出来就是分类问题，否则就是回归问题。</p><h2 id="为什么不可以用线性回归的模型解决分类问题"><a href="#为什么不可以用线性回归的模型解决分类问题" class="headerlink" title="为什么不可以用线性回归的模型解决分类问题"></a>为什么不可以用线性回归的模型解决分类问题</h2><p>多分类问题中，如果使用线性回归模型，按输出值进行分类，那么就必须人为划分分类区间，使得有些分类距离很近，有些分类距离很远，极大影响了分类器性能。</p><h2 id="在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><a href="#在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE" class="headerlink" title="在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE"></a>在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</h2><p>Logistic Regression的假设函数如下：</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}} \quad z(x) = wx+b<br>$$</p><p>σ(z)分别对w和b求导，结果为：</p><p>$$<br>\frac{\partial \sigma(z)}{\partial w} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial w}= \sigma(z)(1-\sigma(z))\times x<br>$$</p><p>$$<br>\frac{\partial \sigma(z)}{\partial b} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial b}= \sigma(z)(1-\sigma(z))<br>$$</p><p>如果使用MSE作为损失函数的话，那写出来是这样的：</p><p>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>当我们使用梯度下降来进行凸优化的时候，分别需要计算L(w,b)对w和b的偏导数：</p><p>$$<br>\frac{\partial L(w,b)}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b)}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))<br>$$</p><p>所以在σ(x)接近于1或者0的时候，也就是预测的结果和真实结果很相近或者很不相近的时候，σ(x)和1-σ(x)中总有一个会特别小，这样会导致梯度很小，从而使得优化速度大大减缓。</p><p>而当使用Cross Entropy作为损失函数时，损失函数为：</p><p>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}(-\hat{y}^{(i)}\log(\sigma_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-\sigma_{w,b}(x^{(i)})))<br>$$</p><p>$L(w,b)$分别对$w$和$b$求偏导，结果如下：</p><p>$$<br>\frac{\partial L(w,b) }{\partial w}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b) }{\partial b}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})<br>$$</p><p>这样梯度始终和预测值与真实值之差挂钩，预测值与真实值偏离很大时，梯度也会很大，偏离很小时，梯度会很小。所以我们更倾向于使用Cross Entropy而不使用MSE。</p><h2 id="级联逻辑回归-Cascading-logistic-regression-model-模型是啥？为什么要引入这个概念？"><a href="#级联逻辑回归-Cascading-logistic-regression-model-模型是啥？为什么要引入这个概念？" class="headerlink" title="级联逻辑回归(Cascading logistic regression model)模型是啥？为什么要引入这个概念？"></a>级联逻辑回归(Cascading logistic regression model)模型是啥？为什么要引入这个概念？</h2><p>级联逻辑回归模型是将很多的逻辑回归接到一起，以进行特征转换再用一个逻辑回归来进行分类。</p><p>级联逻辑回归模型是神经网络的雏形。</p><h2 id="Loss-Function-–-损失函数"><a href="#Loss-Function-–-损失函数" class="headerlink" title="Loss Function – 损失函数"></a>Loss Function – 损失函数</h2><p>回归任务假设函数：</p><p>$$<br>h_{w,b}(x) = wx+b<br>$$</p><p>分类任务假设函数：</p><p>$$<br>h_{w,b}(x) = \sigma(wx+b)<br>$$</p><p>在回归任务中，多使用均方误差作为损失函数：</p><p>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>在分类任务中，多使用交叉熵作为损失函数：</p><p>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}[-\hat{y}^{(i)}\ln(h_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\ln(1-h_{w,b}(x^{(i)}))]<br>$$</p><h2 id="Sigmoid和Softmax"><a href="#Sigmoid和Softmax" class="headerlink" title="Sigmoid和Softmax"></a>Sigmoid和Softmax</h2><p>Sigmoid Function不具体表示哪一个函数，而是表示一类S型函数，常用的有逻辑函数σ(z)：<br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>Softmax，或称归一化指数函数，是逻辑函数的一种推广，二分类情况下，Softmax退化为逻辑函数。该函数的形式通常按下面的式子给出：</p><p>$$<br>\sigma(z)_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \quad j = 1,…,K<br>$$</p><h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h2 id="Deep-Learning三步曲"><a href="#Deep-Learning三步曲" class="headerlink" title="Deep Learning三步曲"></a>Deep Learning三步曲</h2><ul><li>定义一个Model，深度学习里Model是Neural Network Structure；</li><li>定义这个Model好坏，使用合适的Loss Function来衡量损失；</li><li>找出最佳参数，使用反向传播不断优化参数，从而找出最佳参数。</li></ul><h2 id="梯度不稳定问题"><a href="#梯度不稳定问题" class="headerlink" title="梯度不稳定问题"></a>梯度不稳定问题</h2><p>根本原因在于靠近输入层的梯度是来自于靠近输出层上梯度的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。</p><h3 id="Vanishing-Gradient-Problem-梯度消失问题"><a href="#Vanishing-Gradient-Problem-梯度消失问题" class="headerlink" title="Vanishing Gradient Problem - 梯度消失问题"></a>Vanishing Gradient Problem - 梯度消失问题</h3><p>在多层网络中，影响梯度大小的因素主要有两个：权重和激活函数的偏导。深层的梯度是多个激活函数偏导乘积的形式来计算，如果这些激活函数的偏导比较小（小于1）或者为0，那么梯度随时间很容易vanishing。</p><p>反向传播时，需要计算偏导数，如果激活函数是Sigmoid函数，对其求导后，发现Sigmoid函数的导数最大也就0.25（当input=0时），而且很多情况input不会为0的，所以Sigmoid的导数会更小。那么计算靠近输入层参数的偏导数，难免会乘上几次Sigmoid函数的偏导，梯度就这样消失了。</p><h3 id="Exploding-Gradient-Problem-梯度爆炸问题"><a href="#Exploding-Gradient-Problem-梯度爆炸问题" class="headerlink" title="Exploding Gradient Problem - 梯度爆炸问题"></a>Exploding Gradient Problem - 梯度爆炸问题</h3><p>和梯度消失一样，如果这些激活函数的偏导比较大（大于1），那么梯度很有可能就会exploding。这些大于1的偏导会导致靠近输入层的参数变化比较快，靠近输出层的参数相较而言变化慢，导致梯度爆炸的问题。</p><h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><ol><li>重新设计网络模型，减少网络层数有效解决梯度不稳定问题；</li><li>使用 ReLU 激活函数，ReLU求完微分后不会引起梯度消失或爆炸的问题，而且计算速度快，加速了网络的训练；</li><li>使用LSTM，LSTM单元和相关的门类型神经元结构可以减少梯度消失问题；</li><li>使用梯度截断，自定一个阈值，梯度再大也不能超过这个阈值；</li><li>Batch-Norm，Batch-Norm通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题；</li><li>残差网络结构，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分。</li></ol><h3 id="RNN梯度消失与梯度爆炸"><a href="#RNN梯度消失与梯度爆炸" class="headerlink" title="RNN梯度消失与梯度爆炸"></a>RNN梯度消失与梯度爆炸</h3><p><img src="/科大软院-–-人工智能期末考试复习/RNN梯度消失与梯度爆炸.jpg" alt="RNN梯度消失与梯度爆炸"></p><p>对RNN进行优化需要用到BPTT算法，使用$S_i$来表示RNN的记忆状态，权值$W_x$的偏导如下：</p><p>$$<br>\frac{\partial{L_{t}}}{\partial{W_{x}}}=\sum_{k=0}^{t}{\frac{\partial{L_{t}}}{\partial{O_{t}}}\frac{\partial{O_{t}}}{\partial{S_{t}}}}(\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}})\frac{\partial{S_{k}}}{\partial{W_{x}}}<br>$$</p><p>发现其中$\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}}$是一个累乘，如果每一项都小于1，那么乘多了就变0了，如果每一项都大于1，那么乘多了又会很大，所以RNN存在梯度消失和爆炸的原因。</p><h3 id="为什么LSTM可以解决梯度消失的问题"><a href="#为什么LSTM可以解决梯度消失的问题" class="headerlink" title="为什么LSTM可以解决梯度消失的问题"></a>为什么LSTM可以解决梯度消失的问题</h3><p><img src="/科大软院-–-人工智能期末考试复习/LSTM3-focus-C.png" alt="LSTM3-focus-C"></p><p>在LSTM中，也有和RNN一样的记忆部分，叫做细胞状态(LSTMCell)，用$C_i$来表示。从上图可以看到，LSTM的单元状态$C_i$更新公式如图右侧所示，是一个加法而不是乘法，$f_t\times C_{t-1}$表示以前的记忆需要忘记多少；$i_t\times \tilde{C}_t$表示这一次的输入需要添加多少。因为是加法，所以不容易导致$C_i$接近于0的情况。</p><h2 id="Maxout-Function"><a href="#Maxout-Function" class="headerlink" title="Maxout Function"></a>Maxout Function</h2><p>Maxout Function可以理解成一种分段线性函数来近似任意凸函数，因为任意的凸函数都可由分段线性函数来拟合。它在每处都是局部线性的，而一般的激活函数都有明显的曲率。ReLU是Maxout的一种特殊情况。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout也是一种正则化手段，指暂时丢弃一部分神经元及其连接。随机丢弃神经元可以防止过拟合，同时可以高效地连接不同网络架构。</p><p>如果训练时有p%的概率dropout，那么在测试的时候，所有的权值都要乘以1-p%。</p><p>举个栗子：如果训练时有30%的概率dropout，那么在测试的时候，所有的权值都要乘以0.7。如果其中一个权重为10，则要乘以0.7，权值为7。</p><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><p>CNN和RNN内容这么多我要写啥？？？</p><p>当输入为图像时使用CNN，CNN会自动学习到图像特征。</p><h3 id="与全连接神经网络不同之处："><a href="#与全连接神经网络不同之处：" class="headerlink" title="与全连接神经网络不同之处："></a>与全连接神经网络不同之处：</h3><ol><li>CNN采取稀疏连接的方式；</li><li>权值共享，一个卷积核可以对一张图片很多像素值进行操作。</li></ol><h3 id="CNN特点："><a href="#CNN特点：" class="headerlink" title="CNN特点："></a>CNN特点：</h3><ol><li>一些pattern只和图片局部区域有关；</li><li>图片的不同区域可能会出现的同样的pattern；</li><li>对图片进行降采样处理并不会改变图片的内容。</li></ol><p>卷积层用到1、2两个特点，池化层用到3特点。</p><h3 id="CNN里的超参数："><a href="#CNN里的超参数：" class="headerlink" title="CNN里的超参数："></a>CNN里的超参数：</h3><ul><li>Filter Size - 卷积核的尺寸</li><li>Padding - 边缘填充策略（不知道这个怎么翻才好）</li><li>Stride - 移动步长</li><li>number of filters - 卷积核的个数</li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><h3 id="什么是序列数据？会举例说明。"><a href="#什么是序列数据？会举例说明。" class="headerlink" title="什么是序列数据？会举例说明。"></a>什么是序列数据？会举例说明。</h3><p>有时间维度的数据称为序列数据。例：音乐，语音。</p><h3 id="即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？"><a href="#即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？" class="headerlink" title="即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？"></a>即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？</h3><p>见上面的RNN梯度消失与梯度爆炸部分。</p><h3 id="LSTM与一般的RNN相比，优势在哪？"><a href="#LSTM与一般的RNN相比，优势在哪？" class="headerlink" title="LSTM与一般的RNN相比，优势在哪？"></a>LSTM与一般的RNN相比，优势在哪？</h3><ol><li>见上面为什么LSTM可以解决梯度消失的问题。</li><li>LSTM可以保持长时记忆，LSTM的记忆门可以控制记忆存放多久。不过LSTM可以保持长时间记忆根本原因也是因为LSTM解决了梯度消失的问题吧。</li></ol><h3 id="对于给定问题，能判断出是否该使用RNN模型。"><a href="#对于给定问题，能判断出是否该使用RNN模型。" class="headerlink" title="对于给定问题，能判断出是否该使用RNN模型。"></a>对于给定问题，能判断出是否该使用RNN模型。</h3><p>当输入和输出有一个是序列数据时使用RNN模型。</p><h1 id="References："><a href="#References：" class="headerlink" title="References："></a>References：</h1><ol><li><p><a href="https://zhuanlan.zhihu.com/p/26304729" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26304729</a></p></li><li><p><a href="https://feisky.xyz/machine-learning" target="_blank" rel="noopener">https://feisky.xyz/machine-learning</a></p></li><li><p><a href="https://blog.csdn.net/fjssharpsword/article/details/71157798" target="_blank" rel="noopener">https://blog.csdn.net/fjssharpsword/article/details/71157798</a></p></li><li><p><a href="https://www.jianshu.com/p/b8844a62b04a" target="_blank" rel="noopener">https://www.jianshu.com/p/b8844a62b04a</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/32626442" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32626442</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22810533</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/1013598" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1013598</a></p></li><li><p><a href="https://ziyubiti.github.io/2016/11/06/gradvanish/" target="_blank" rel="noopener">https://ziyubiti.github.io/2016/11/06/gradvanish/</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dnn </tag>
            
            <tag> review for exam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CTC Loss学习笔记</title>
      <link href="/CTC%20Loss%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/CTC%20Loss%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>谨以此文纪念一下我那次晚上11点半查完的实验。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>语音识别时，人说一句“Hello”，尽管发音很标准，但是由于有停顿、换气或是其他原因，音频信息中的“H”音很容易对不上文本信息中的“H”。这就需要预处理对齐问题，但是用人工的方法手动对齐比较音频信息和文本信息，需要耗费大量的人力财力。CTC就是处理这一类对齐问题而生的技术。</p><h2 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h2><p>在语音识别中，CTC会逐帧辨别发的什么音。如果通过发音辨别出了一个字母，那么这一帧就标记为这个字母；如果没声音就标记为blank，表示这是两个单词中间的空格；如果分辨不出来是啥，就标记为ϵ (ctc blank)。所以将“Hello”的音频信息进行辨别后，会得到的结果可能会是：“ϵϵHellϵϵlloϵ”，也可能是“Heeϵϵllϵϵlooϵϵ”，还有其他很多种可能。</p><p>ctc的处理过程分为2步：</p><ol><li>将识别结果中相邻的重复字符删掉，那么“ϵϵHellϵϵlloϵ”就变成了“ϵHelϵloϵ”，“Heeϵϵllϵϵlooϵϵ”就变成了“Heϵlϵloϵ”；</li><li>删去ϵ，“ϵHelϵloϵ”和“Heϵlϵloϵ”就都变成了“Hello”。</li></ol><p>我们的目的是求出P(lable|x)，其中x是输入的数据，lable是标签数据，即正确的输出。P(lable|x)表示的是，如果在输入数据为x的条件下，能够正确输出标签数据的概率有多大，这个概率肯定是越大越好，如果P(lable|x)=1，那么就说明百分之百可以正确输出。</p><p>Loss的定义如下，如果P(lable|x)越接近1，Loss就接近于0；如果P(lable|x)越接近0，Loss就会越大：</p><p>$$<br>Loss=-\ln P(lable|x)<br>$$</p><p>P(lable|x)如何计算呢？从上面ctc的处理过程可以看出来，有很多种序列最终都可以处理为标签数据，“ϵϵHellϵϵlloϵ”就是一条可以被处理为“Hello”的序列，记其为seq，那么产生seq的概率有多少呢？这里$y_c^t$表示在第t时间步生成字符c的概率有多少，那么要生成seq这么长的序列，就要把生成每个字符的概率乘起来。</p><p>$$<br>P(seq|x)=y_\epsilon^1 \times y_\epsilon^2 \times y_H^3 \times y_e^4 \times y_l^5 \times y_l^6 \times y_\epsilon^7 \times y_\epsilon^8 \times y_l^9 \times y_l^{10} \times y_o^{11} \times y_\epsilon^{12}<br>$$</p><p>如果再用π来表示一个序列的集合，这个集合中所有的序列和seq一样，经过ctc处理过后，都可以变为标签数据。那么将这个集合中所有序列的概率加起来，就是P(lable|x)。</p><p>$$<br>P(lable|x)=\sum_{\pi}P(\pi|x)<br>$$</p><p>可是，π集合中有多少序列呢，茫茫多的序列都可以转为lable。如果lable是“cat”的话，那么可以转为“cat”的序列数，可以通过下图可以看出来，白色节点表示识别为字母，黑色节点表示识别不出来是个啥，标记为“ϵ”（图片来自网络）：</p><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_1.png" alt="ctc_loss_1"></p><p>每一条路径都是一种语音识别的结果，而且此结果可以经由ctc处理后转换为标签数据。这才3个字母，就这么条路径，如果成百上千的单词，一一计算每种可能的话，这个计算量很大。</p><h2 id="使用动态规划优化"><a href="#使用动态规划优化" class="headerlink" title="使用动态规划优化"></a>使用动态规划优化</h2><p>动态规划的思想是自顶向下写一个递归式，然后自底向上算出来。</p><p>如上文所述，在t个时间步走到字符“u”的路线有很多条，如果seq是其中一条路线，那么其概率为：</p>$$P(seq|x)=\prod_{i=1}^{t} y^i_{{seq}_i}$$<p>这里定义一个函数α(t,u)，表示在t个时间步走到字符“u”的概率。用π表示所有可以在t个时间步走到字符“u”的路线集合，那么有：</p>$$\alpha(t,u)=\sum_{\pi}\prod_{i=1}^{t} y^i_{\pi_i}$$<p>再看看刚刚开始那个例子“Hello”，如果当前在第t个时间步，判别成了一个字符“l”，那么在第t-1个时间步上，可以是哪些字符呢？</p><p>这要分2种情况讨论：</p><ol><li>如果“Hello”的第一个“l”，也就是”Hello”标红的那个“l”，那么在第t-1个时间步上，可以是“e”、“ϵ”、“l”三种情况；</li></ol><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_2.png" alt="ctc_loss_2"></p><p>所以要求第t个时间步走到“l”的概率，就要先求出在t-1个时间步走到“e”、“ϵ”、“l”的概率，求和之后，再乘以$y_c^t$，即：</p><p>$$<br>\alpha(t,u)=y_u^t[\alpha(t-1,u)+\alpha(t-1,u-1)+\alpha(t-1,u-2)]<br>$$</p><p>这里u-1和u-2表示字符u的上一个字符和上两个字符。</p><ol><li>如果是“Hello”的第二个“l”，也就是“Hello”标红的那个“l”，那么在第t-1个时间步上，只可以是“ϵ”、“l”两种情况。要是算上上面的“l”，那么这两个“l”会合并成1个；如果是在t个时间步上走到了“ϵ”这个字符，那么在t-1个时间步上，也只能是“l”、“ϵ”两种情况。要是也算上上面一个“ϵ”，那么会跨过中间这个“l”，这个字符就不输出了。</li></ol><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_3.png" alt="ctc_loss_3"></p><p>所以第t个时间步走到“l”或”ϵ”，只能横着走过来，或从上一个字符过来，概率即：</p><p>$$<br>\alpha(t,u)=y_u^t[\alpha(t-1,u)+\alpha(t-1,u-1)]<br>$$</p><p>这样递归公式就定义好了，再算一下递归式子的底：</p><p>$$<br>\alpha(1,1)=y_\epsilon^1<br>$$</p><p>$$<br>\alpha(1,2)=y_h^1<br>$$</p><p>$$<br>\alpha(1,u)=0 \quad u&gt;2<br>$$</p><p>从这个底开始，使用递推公式自底向上的计算，直至算出$\alpha(T,lable)+\alpha(T,lable-1)$，T表示总时间步，所有可以经过ctc转化为“Hello”的序列，最后一个字符可以是“ϵ”也可以是“o”。label表示所最后一个字符是“ϵ”，label-1表示最后一个字符是“o”，这样走出的路线都可以经过ctc转化为“Hello”。</p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ctc </tag>
            
            <tag> rnn </tag>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>删数问题的贪心选择策略证明</title>
      <link href="/%E5%88%A0%E6%95%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%B4%AA%E5%BF%83%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5%E8%AF%81%E6%98%8E/"/>
      <url>/%E5%88%A0%E6%95%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%B4%AA%E5%BF%83%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5%E8%AF%81%E6%98%8E/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>题目：在一个n位的正整数A[1…n]中删除其中任意k(k≤n)个数字后，剩下的数字按原次序组成一个新的正整数。对于给定的n位正整数A和k，设计一个贪心算法，使得剩下的数字组成的新数最小。</strong><br><strong>如：A=278693，k=4时最小新数为23，k=3时为263</strong></p></blockquote><a id="more"></a><p>刚刚证明出来贪心选择性质，先写下，之后再慢慢补充全部思路。</p><h2 id="贪心选择策略"><a href="#贪心选择策略" class="headerlink" title="贪心选择策略"></a><strong>贪心选择策略</strong></h2><p>从高位向低位进行搜索：</p><ol><li>如果A[1…n]是一条递增序列，那么就删除最后一个数；</li><li>如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。</li></ol><h2 id="贪心选择性质证明"><a href="#贪心选择性质证明" class="headerlink" title="贪心选择性质证明"></a>贪心选择性质证明</h2><p>假设A中有n个元素，将A中的每个元素进行编号为$a_x(1\leq x\leq n)​$：<br>$$<br>A=[a_1,a_2,…,a_{n-1},a_n]<br>$$</p><ol><li>如果A[1…n]是一条递增序列，那么就删除最后一个数；</li></ol><p>记原本A所代表的数值为$T_A​$，则有：<br>$$<br>T_A=a_1 \times 10^{n-1}+a_2\times10^{n-2}+…+a_{n-1}\times10+a_n<br>$$</p><p>记删除最后一个数后，A变为A’，A’所代表的数值为$T_{A’}$，则有：</p><p>$$<br>T_{A’}=a_1\times10^{n-2}+a_2 \times 10^{n-3}+…+a_{n-2} \times 10+a_{n-1}<br>$$</p><p>如果不删除最后一个数，而是删除另一个数$a_q(1\leq q&lt; n)$，此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有： </p><p>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}+…+a_{n-1} \times 10+a_n<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_q-a_{q+1}) \times 10^{n-q-1}+(a_{q+1}-a_{q+2}) \times 10^{n-q-2}+…+(a_{n-2}-a_{n-1}) \times 10+(a_{n-1}-a_{n})<br>$$</p><p>由于A是一条递增序列，所以有：</p><p>$$<br>a_{q}\leq a_{q+1},a_{q+1}\leq a_{q+2},…,a_{n-2}\leq a_{n-1},a_{n-1}\leq a_{n}<br>$$</p><p>即：</p><p>$$<br>a_{q}-a_{q+1}\leq 0,a_{q+1}-a_{q+2}\leq 0,…,a_{n-2}-a_{n-1}\leq 0,a_{n-1}-a_{n}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’}\leq 0$，那么$T_{A’}$是A删完一个数后，所组成的最小数值。贪心选择是安全的。</p><ol start="2"><li>如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。</li></ol><p>(1) 先考虑一种情况，A中只有两段单调子区间，且高位部分是递增子区间，低位部分是递减子区间。那么A中的数就是先上升再下降，像一座山一样。如果删除第一个递减子序列的首字符是安全的，也就是删除山顶的数是安全的。</p><p>假设在A中，有3个连在一起的数$a_i,a_j,a_k(1\leq i&lt; j&lt; k\leq n)$，其中$(a_1,a_j)$是递增序列，$(a_j,a_n)$是递减序列。</p><p>记原本A所代表的数值为$T_A$，则有：</p><p>$$<br>T_A=a_1 \times 10^{n-1}+a_2 \times 10^{n-2}+…+a_i \times 10^{n-i}+a_j \times 10^{n-j}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_n<br>$$</p><p>删除$a_j$后，A变为A’，A’所代表的数值为$T_{A’}$，则有：</p><p>$$<br>T_{A’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_{n}<br>$$</p><p>如果不删除$a_j$，而是删除另一个数$a_q$。</p><p>a) 当$1\leq q&lt; j$时，此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有：<br>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}+…+a_j \times 10^{n-j}+…+a_{n-1} \times 10+a_{n}<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_q-a_{q+1}) \times 10^{n-q-1}+(a_{q+1}-a_{q+2}) \times 10^{n-q-2}+…+(a_{i}-a_{j}) \times 10^{n-j}<br>$$</p><p>由于$(a_1,a_j)$是一条递增序列，所以有：</p><p>$$<br>a_{q}\leq a_{q+1},a_{q+1}\leq a_{q+2},…,a_{i}\leq a_{j}<br>$$</p><p>即：</p><p>$$<br>a_{q}-a_{q+1}\leq 0,a_{q+1}-a_{q+2}\leq 0,…,a_{i}-a_{j}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’}\leq 0$，那么$T_{A’}$是A删完$a_q(1\leq q &lt; j)$后，所组成的最小数值。贪心选择是安全的。</p><p>(b) 当$j &lt; q\leq n$时，此时A变为A’’’，A’’’所代表的数值为$T_{A’’’}$，则有： </p><p>$$<br>T_{A’’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_j \times 10^{n-j-1}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}…+a_{n-1} \times 10+a_{n}<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_k-a_j) \times 10^{n-k}+…+(a_{q-1}-a_{q-2}) \times 10^{n-q+1}+(a_{q}-a_{q-1}) \times 10^{n-q}<br>$$</p><p>由于$(a_j,a_{n})$是一条递减序列，所以有：</p><p>$$<br>a_{k}\leq a_{j},a_{q-1}\leq a_{q-2},…,a_{q}\leq a_{q-1}<br>$$</p><p>即：</p><p>$$<br>a_{k}-a_{j}\leq 0,a_{q-1}-a_{q-2}\leq 0,a_{q}-a_{q-1}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’’}\leq 0$，那么$T_{A’}$是A删完$a_q(j&lt; q \leq n)$后，所组成的最小数值。贪心选择是安全的。<br>综上(a)(b)所述，若A中只有两段单调子区间，且高位部分是递增子区间，低位部分是递减子区间，那么删去此递减子区间的首字符后，所组成的数是最小数值。贪心选择是安全的。</p><p>(2) 现在已经证明了如果A中只有一个山，那么删除山顶元素是最合适的。如果A中有多个山峰该如何处理？</p><p>换句话说，如果A中不止有2段单调子区间，而是由很多不同的单调子区间所组成，那应该如何处理？</p><p>这个问题看上去有点复杂了，我画个图吧：</p><p><img src="/删数问题的贪心选择策略证明/2_1.jpg" alt="2_1"></p><p>其实把图画出来后，我尝试着删除第一个山的山顶元素，删完后发现，现在的$a_6=a_5$，而$a_7=a_7$：</p><p><img src="/删数问题的贪心选择策略证明/2_2.jpg" alt="2_2"></p><p>但如果删除第一个山之后的任意一个元素的话，删完后发现$a_6=a_5$，而现在的$a_7=a_6$。</p><p><img src="/删数问题的贪心选择策略证明/2_3.jpg" alt="2_3"></p><p>由于$a_6&gt;a_7​$，所以不管后面删哪个元素，都会比删除$a_6​$所得到的数要大。</p><p>数学证明如下：</p><p>假设在A中，有3个连在一起的数$a_i,a_j,a_k(1\leq i&lt; j&lt; k\leq n)​$，还有另一个数$a_r(k \leq r &lt; n)​$，，其中$(a_1,a_j)​$是递增序列，$(a_j,a_r)​$是递减序列。</p><p>记原本A所代表的数值为$T_A$，则有：</p><p>$$<br>T_A=a_1 \times 10^{n-1}+a_2 \times 10^{n-2}+…+a_i \times 10^{n-i}+a_j \times 10^{n-j}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_n<br>$$</p><p>删除$a_j​$后，A变为A’，A’所代表的数值为$T_{A’}​$，则有：</p><p>$$<br>T_{A’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_k \times 10^{n-k}+…<br>$$</p><p>如果不删除$a_j$，而是删除另一个数$a_q(r \leq q \leq n)$。此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有： </p><p>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_j \times 10^{n-j-1}+…<br>$$</p><p>发现$T_{A’}$中的$10^{n-k}$的系数是$a_k$，而$T_{A’’}$中的$10^{n-k}$的系数是$a_j$，由于$a_k&lt; a_j$，那么必有$T_{A’}&lt; T_{A’’}$，所以删除第一个山峰的峰值是安全的。也就是如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。贪心选择是安全的。</p><p>综上，贪心选择性质成立。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> greedy </tag>
            
            <tag> math proof </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划 - Dynamic Programming</title>
      <link href="/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%20-%20Dynamic%20Programming/"/>
      <url>/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%20-%20Dynamic%20Programming/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Those who cannot remember the past are condemned to repeat.</p><p>-Dynamic Programming</p></blockquote><a id="more"></a><p>今天的算法课讲的就是动态规划，虽早已久仰大名，但始终不太懂背后原理。听完课后赶紧把今天学到的东西记下来。</p><p>动态规划和分治法有类似之处，他们都是通过组合子问题的解来求解原问题。不同之处在于，分治法常常用于将原问题划分为几个<strong>互不相交</strong>的子问题，如果划分后，有很多<strong>完全相同</strong>的子问题，那就使用动态规划的思路比较好。例如快速排序，将一段数组以枢纽值为界一分为二，再将这两段分别排序。将分出来的两段数组分别排序就是互不相交的两个子问题，可以使用分治法来解决。再如求斐波那契数列的第n项问题，我们知道斐波那契数列的规律是<br>$$<br>F(0) = 0 \quad F(1) = 1 \quad F(n) = F(n - 1) + F(n - 2) (n &gt;= 2)<br>$$<br>我们发现求F(n)需要计算F(n-1)和F(n-2)，可是当计算F(n-1)和F(n-2)时，都需要计算F(n-3)，这样就造成了重复计算，大幅降低了效率，而动态规划算法的目的就是避免此类重复计算。</p><p>去年暑假，我好好思考了一下上台阶问题：</p><blockquote><p>有100级台阶，每次只允许上1级台阶或上2级台阶，若要求上完全部台阶，共有多少种上法？</p></blockquote><p>第一次看到还以为是组合数学问题，后来也没做出来。再一想，要不直接枚举吧，那这样的数据规模是惊人的，是指数级的时间复杂度。</p><p>emmm~我们考虑一下分治法吧。假设一下，现在已经站在了第100层台阶上，回忆一下刚刚最后一步我是怎么上来着的。我最后一步可以是从98层一步跨上来的，也可以是从99层踩上一级台阶上来。这时让我们时光倒流，我们可以选择最后一步从98层跨两级上来，也可以选择从99层爬一级上来。</p><p>现在我们将这两种可能分开讨论，如果我限定最后一步只能走一步，那么之前我就必须上到99层，而99层到100层只有1条路可走，上到100层的上法就等于上到99层的上法，所以我现在只要得知上到99层有多少种上法，就知道了上到100层有多少种上法；同理，如果我限定最后一步必须跨两步，那么我之前就必须上到98层，所以我现在只要得知上到98层有多少种上法，98层到100层也只有跨两级这一条路，所以上到100层的上法就等于上到98层的上法了。</p><p>现实里，最后一步可以跨两级，也可以只爬一级，所以上到100层的上法，应该等于上到98层的上法加上上到99层的上法。如果用函数F(n)表示上到第n层一共有多少种上法，那么就有F(100) = F(99) + F(98)，接下来也有F(99) = F(98) + F(97)，F(98) = F(97) + F(96)……。</p><p>这样问题就很简单了，只要这样不断递归下去，总会递归到一个终点，这个终点就是F(1)和F(2)，上一层台阶有多少种上法呢，显然除了一步跨上去没别的方法了吧，所以F(1) = 1；上两层台阶呢？一步一步爬，或是一步跨两个，F(2)应该等于2。那么现在的递归式应该是：<br>$$<br>F(1) = 1 \quad F(2) = 2 \quad F(n) = F(n - 1) + F(n - 2) (n &gt; 2)<br>$$<br>将n取值为100，这样我们就可以算出结果了，用c++写出的代码如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> F(n - <span class="number">1</span>) + F(n - <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这个代码跑很久都是跑不出来的，就是因为重复计算的部分太多了，详细讲来，如果我们要算f(100)，那么我们需要先得知f(99)和f(98)，然后想要得知f(99)和f(98)，都需要先算出f(97)，这样f(97)就计算了两遍，造成了重复计算，浪费了大量时间。上面代码中包含大量重复计算的内容，而动态规划算法就很好的解决了重复计算的问题。</p><p>现在我们不再从100层开始逐层向下递归，而是从第一层开始逐层向上走。什么意思呢？正如上面提到的问题，如果从下往上计算的话，我们先算出了f(97)的值，然后在计算f(99)和f(98)的时候，就可以共享一个计算成果，不需要花两倍的时间来计算。</p><p>从底层开始计算时，我们首先得知f(1) = 1以及f(2) = 2，根据F(n) = F(n - 1) + F(n - 2)的公式，可以很轻松算出F(3) = 3，F(4) = 5，然后继续向下推，直到n到100就可以得到结果。用代码实现的话，可以新开一个数组array，array[n]内存储了F(n)的结果。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">long</span>&gt; <span class="built_in">array</span>(n + <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">array</span>[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">array</span>[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="built_in">array</span>[i] = <span class="built_in">array</span>[i - <span class="number">1</span>] + <span class="built_in">array</span>[i - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>[n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这里只需要一个循环就可以得出结果啦~这个代码的时间复杂度是O(n)，但是我们的空间复杂度也达到了O(n)，我们发现很多数据都是只使用了一次就不再使用了，将这个数组改为3个变量循环交替使用可以加大使用效率。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> A, B, C;</span><br><span class="line">    A = <span class="number">1</span>;</span><br><span class="line">    B = <span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="keyword">return</span> A;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) <span class="keyword">return</span> B;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            C = A + B;</span><br><span class="line">            A = B;</span><br><span class="line">            B = C;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> C;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>我们使用3个变量ABC交替使用，只要始终保持ABC的先后顺序就可以啦！这种方法还将空间复杂度降为了O(1)。当然这题我们发现刚好是求斐波那契数列的第101项，如果我们直接使用斐波那契数列求第n项公式可以在常数级的时间内求出结果，但那和动态规划没有关系啦~</p><p>总结一下：线性规划就是先用分治法的方法，从终点往前思考，用递归的方式定义解；然后我们发现其中有很多重复的子问题，这样我们又从起点开始，逐步记录已经求好的子问题的解，然后需要时再拿出来用，以此节约时间。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> dynamic programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode 79. Word Search（非递归算法）</title>
      <link href="/LeetCode%2079.%20Word%20Search%20-%20%E9%9D%9E%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95/"/>
      <url>/LeetCode%2079.%20Word%20Search%20-%20%E9%9D%9E%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>上次算法实验课的时候，助教问我可不可以用非递归的方式实现，回去之后折腾了好一会儿终于在LeetCode上AC了我的非递归版本，在这里分享一下。</p><a id="more"></a><h1 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h1><p>Given a 2D board and a word, find if the word exists in the grid.</p><p>The word can be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once.</p><p><strong>Example:</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">board =</span><br><span class="line">[</span><br><span class="line">  [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;],</span><br><span class="line">  [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;],</span><br><span class="line">  [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">Given word = &quot;ABCCED&quot;, return true.</span><br><span class="line">Given word = &quot;SEE&quot;, return true.</span><br><span class="line">Given word = &quot;ABCB&quot;, return false.</span><br></pre></td></tr></table></figure></div><h1 id="题目大意："><a href="#题目大意：" class="headerlink" title="题目大意："></a>题目大意：</h1><p>给一个char型二维数组和一个word字符串，寻找网格中是否含有word字符串，只能通过相邻（垂直或者水平）的格子连接～</p><h1 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h1><p>此题的要求是在二维的情况下搜索解是否存在，如果使用DFS（深度优先搜索）算法可以很直观明了且迅速得到解。</p><p>DFS算法的基本思路是，当搜索过程遇到分岔路口时，首先选择其中一个路口进入，如果进去之后可以找到解，在此题中就可以直接返回true，声明解是存在的；如果进去之后没有找到解，那么还回到这个分岔路口，选择另外一个路口进入，继续搜解。如果每个路口都进去过了，而且都没有搜到解，那么就可以说解是不存在的。<br>我们将二维数组中的每个点都尝试着作为起点，从这一点试着向上下左右四个方向拓展，一个方向里搜不到解就搜另一个方向，如果四个方向都搜不到解就说明以此点为起点是搜不到解的，换个起点试试~</p><p>递归算法可以看看<a href="https://www.liuchuo.net/archives/3191" target="_blank" rel="noopener">柳神的blog</a>，上次算法实验课的时候，助教问我可不可以用非递归的方式实现，回去之后折腾了好一会儿终于在LeetCode上AC了我的非递归版本，在这里分享一下。</p><p>首先，非递归算法的基本思路和递归算法大致相同，在搜索过程中遇到分岔路口的时候，我们都会选择一个路口进入。当我们使用递归算法时，操作系统会为我们保留这个岔路口的信息，如果我们选错了路，还可以方便的回到这个岔路口；而非递归算法就需要我们自己手动保存分岔路口的信息了。我们使用栈这个数据结构来保存分岔路口的信息，因为栈的先进后出的特性与这种情况十分相符，使用这种特性的目的是为了当做错了某种选择时，我们可以方便的将状态回退。</p><p>举个栗子🌰：比如今天出门要给自己搭配一套衣服，我们要选择衬衫，卫衣以及外套这三件衣服。我们的选择有黑衬衫和白衬衫、红卫衣和绿卫衣、蓝外套和粉外套，然而我的衣品不怎么样，不知道怎么穿才好看，那我只能去尝试一下。我先穿上黑衬衫，再穿上红卫衣，最后穿上蓝外套，对镜子一看，怎么这么丑！肯定是外套的问题，这时我需要换外套，就脱下现在的外套，回到穿着卫衣和衬衫的状态，然后穿上粉外套。对镜子一看，我去，更丑了！肯定是卫衣的问题，那么我要回到选择卫衣的状态，我就需要把外套脱下，再把卫衣脱下，此时只穿着衬衫，这时我就可以重新选择卫衣了。把衣服脱下就是一种状态回退，这种状态回退总是遵循先进后出的顺序，所以使用栈来保存状态回退点再合适不过了。</p><p>接下来直接进入coding时间。我们需要先准备好一个栈<code>stack&lt;unsigned long&gt;</code> 以及访问标记数组<code>vector&lt;vector&lt;bool&gt;&gt; visited;</code></p><p>第一步先把主框架exist函数搭好，之后main函数里就直接调用exist函数得出结果。内部init函数是一个将栈和visited访问标志数组初始化的函数。初始化完成之后，就尝试将board数组中的每个元素设为起点，进行dfs搜索。如果搜索到了就直接返回true；如果搜索完所有点也搜不到就返回false。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">exist</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">  init(board);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; board[<span class="number">0</span>].size(); j++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (DFSTraverse(board, word, i, j))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span> <span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board)</span> </span>&#123;</span><br><span class="line">  visited.resize(board.size(), <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;(board[<span class="number">0</span>].size()));</span><br><span class="line">  dirc.resize(board.size(), <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(board[<span class="number">0</span>].size()));</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>随后就要编写DFSTraverse函数，这样声明函数<code>bool DFSTraverse (vector&lt;vector&lt;char&gt; &gt;  board, string word, int i, int j)</code>， board为需要搜索的二维数组，word是目标字符串，i和j分别是起始元素所在的行和列index（从0开始编号）。递归算法在执行递归程序时，自动保存了两类信息：</p><ol><li>保存分岔口元素的地址，以便于返回到这个分岔路口；</li><li>保存之前所选择的路口，以便于下一次进入另一个路口进行搜索。</li></ol><p>所以，当我们使用dfs的非递归算法时，就需要手动来保存这两类信息。现在我们来看看dfs非递归版本的伪代码模板，但是下面的编码思路不会完全按照这个模板进行，会根据本题情况做适当改动：</p><p><img src="/LeetCode 79. Word Search - 非递归算法/递归模板.png" alt="递归模板"></p><h2 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h2><p>将栈初始化，同时也要将visitded访问标记数据初始化：<br>  <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!s.empty())</span><br><span class="line">    s.pop();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">    fill(visited[i].begin(), visited[i].end(), <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></p><p>  在本题中，还需要设立一个count变量<code>int count = 0;</code> ，表示目前已经搜索到word字符串的第几个字符。</p><h2 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h2><p>访问起始顶点，在本题中，如果起始顶点和word字符串第一个字符对不上，那么可以直接返回false<code>if (board[i][j] != word[count]) return false;</code>；如果起始顶点是word字符串的第一个字符，那么才将起始顶点改为“已访问”标志，将起始顶点进栈，此时还需要将count变量加一。</p>  <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (board[i][j] != word[count]) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    count++;</span><br><span class="line">    visited[i][j] = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>  这里遇到了非递归算法的第一个问题：我该如何保存此元素地址。如果我在线性或非线性链表中进行搜索，入栈我只需要将此节点的地址入栈即可，但我现在是在二维数组中搜索，每个元素都由i和j两个坐标表示，难道我要一次入栈2个元素吗？这样也未尝不可，我也曾试过这个方法，每次入栈2个，出栈的时候也一次出2个元素。可是问题在于，我在只想访问栈顶元素而不要求出栈时，只能访问到一个坐标，另一个坐标需要出栈一个元素才能访问到。这就很麻烦了，真让人头大.jpg。</p><p>  随后我又想到另一个方法，我可以按行顺序对每个元素进行排列，如果每行有10个元素，第一行就是0~9，第二行便是10~19……以此类推。这样的话，我就可以使用一个数字表示出这个元素的坐标，需要用到i和j的时候，也可以很轻松的算出i和j，我可真是太机智了。</p><p>  所以按此思路，入栈时应该是<code>s.push(i * board[0].size() + j);</code> 这条语句，i乘以每行元素个数再加上j，想访问栈顶元素时，便可以使用<code>cur_i = s.top() / board[0].size();cur_j = s.top() % board[0].size();</code>这两条语句。其中cur_i和cur_j是存储目前元素坐标的变量。</p><h2 id="step-3"><a href="#step-3" class="headerlink" title="step 3"></a>step 3</h2><h3 id="step-3-0"><a href="#step-3-0" class="headerlink" title="step 3.0"></a>step 3.0</h3><p>这是伪代码里没有写到的，此题不一定要遍历完全部的元素，只要搜索到目标字符串就可以返回true了，所以while循环体内要这样写：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!s.empty()) &#123;   </span><br><span class="line">    <span class="keyword">if</span> (count == word.length())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">    &lt;code&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="step-3-1"><a href="#step-3-1" class="headerlink" title="step 3.1"></a>step 3.1</h3><p>访问栈顶元素顶点，这里就可以使用上面提到的方法算出栈顶元素的地址了；</p><h3 id="step-3-2"><a href="#step-3-2" class="headerlink" title="step 3.2"></a>step 3.2</h3><p>看看栈顶元素顶点存在未被访问过的邻接点w。这里又面临到非递归算法的第二个问题：如何记录这个顶点已经访问过哪些邻接点，还没有访问过哪些邻接点。因为已经访问过的点如果再访问一遍也是得到一样的结果，很容易陷入死循环。</p><p>对于这个问题，我想过好几个解决方案，最后我在代码中规定每个元素必须按照上下左右的顺序对周边元素进行访问，再使用一个辅助数组dirc<code>vector&lt;vector&lt;int&gt; &gt; dirc;</code>来记录每个元素下一步可以对哪个周边元素进行访问，值可以为1、2、3、4，分别表示可以对上面元素、下面元素、左边元素、右边元素进行访问，若值为5，表示周边元素都访问完了，只能回退到上一个分岔路口，重做选择。我只需要判断dirc中的值，从而决定此元素下一步尝试向哪个方向走。</p><p>当我们决定向一个方向走的时候，我们还需要做3个判断：</p><ol><li>要确定这一步会不会造成数组越界。这里都已向上走为例，向上走时，需要判断<code>cur_i - 1 &gt;= 0</code>；</li><li>要确定下一个访问的元素是否已访问过，如果不判断的话，把路径走成一个环，然后陷入死循环出不来<code>visited[cur_i - 1][cur_j] == false</code>；</li><li>需要判断下一个访问的元素是否与目标字符串里下一个元素相同<code>board[cur_i - 1][cur_j] == word[count]</code>。</li></ol><p>如果通过了上面3个判断，就可以访问邻接点w了，并且将顶点w改为“已访问”标志<code>visited[cur_i - 1][cur_j] = true;</code>，再将顶点w入栈<code>s.push((cur_i - 1) * board[0].size() + cur_j);</code>，count变量也要加一<code>count++;</code>，意味着搜索下一个元素。</p><h3 id="step-3-3"><a href="#step-3-3" class="headerlink" title="step 3.3"></a>step 3.3</h3><p>如果dirc中记录此元素四个方向都访问过了，那么只能回退，需要将此元素出栈<code>s.pop();</code>，访问标记置为“未访问”<code>visited[cur_i][cur_j] = false;</code>，dirc数组内的值重新置为1<code>dirc[cur_i][cur_j] = 1;</code>，count也要减一<code>count--;</code>。</p><h1 id="完整代码："><a href="#完整代码：" class="headerlink" title="完整代码："></a>完整代码：</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">long</span>&gt; s;</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt; visited;</span><br><span class="line">    <span class="comment">// dirc表示行走方向, 1：向上，2：向下，3：向左，4：向右，5：回退</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dirc;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">init</span> <span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board)</span> </span>&#123;</span><br><span class="line">        visited.resize(board.size());</span><br><span class="line">        dirc.resize(board.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; visited.size(); i++) &#123;</span><br><span class="line">            visited[i].resize(board[<span class="number">0</span>].size());</span><br><span class="line">            dirc[i].resize(board[<span class="number">0</span>].size());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">DFSTraverse</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> str, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 栈初始化</span></span><br><span class="line">        <span class="keyword">while</span>(!s.empty()) s.pop();</span><br><span class="line">        <span class="comment">// visited标记数组初始化</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">            fill(visited[i].begin(), visited[i].end(), <span class="literal">false</span>);</span><br><span class="line">            fill(dirc[i].begin(), dirc[i].end(), <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (board[i][j] != str[count]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            count++;</span><br><span class="line">            visited[i][j] = <span class="literal">true</span>;</span><br><span class="line">            s.push(i * board[<span class="number">0</span>].size() + j);</span><br><span class="line">            <span class="keyword">int</span> cur_i = i, cur_j = j;</span><br><span class="line">          </span><br><span class="line">            <span class="keyword">while</span> (!s.empty()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (count == str.length()) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                cur_i = s.top() / board[<span class="number">0</span>].size();</span><br><span class="line">                cur_j = s.top() % board[<span class="number">0</span>].size();</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">switch</span> (dirc[cur_i][cur_j]) &#123;            </span><br><span class="line">                    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                        <span class="comment">// 上</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_i - <span class="number">1</span> &gt;= <span class="number">0</span></span><br><span class="line">                            &amp;&amp; visited[cur_i - <span class="number">1</span>][cur_j] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i - <span class="number">1</span>][cur_j] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i - <span class="number">1</span>][cur_j] = <span class="literal">true</span>;</span><br><span class="line">                            s.push((cur_i - <span class="number">1</span>) * board[<span class="number">0</span>].size() + cur_j);</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                        <span class="comment">// 下</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_i + <span class="number">1</span> &lt; board.size()</span><br><span class="line">                            &amp;&amp; visited[cur_i + <span class="number">1</span>][cur_j] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i + <span class="number">1</span>][cur_j] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i + <span class="number">1</span>][cur_j] = <span class="literal">true</span>;</span><br><span class="line">                            s.push((cur_i + <span class="number">1</span>) * board[<span class="number">0</span>].size() + cur_j);</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">                        <span class="comment">// 左</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_j - <span class="number">1</span> &gt;= <span class="number">0</span></span><br><span class="line">                            &amp;&amp; visited[cur_i][cur_j - <span class="number">1</span>] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i][cur_j - <span class="number">1</span>] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i][cur_j - <span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">                            s.push(cur_i * board[<span class="number">0</span>].size() + (cur_j - <span class="number">1</span>));</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">                        <span class="comment">// 右</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_j + <span class="number">1</span> &lt; board[<span class="number">0</span>].size()</span><br><span class="line">                            &amp;&amp; visited[cur_i][cur_j + <span class="number">1</span>] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i][cur_j + <span class="number">1</span>] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i][cur_j + <span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">                            s.push(cur_i * board[<span class="number">0</span>].size() + (cur_j + <span class="number">1</span>));</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">                        s.pop();</span><br><span class="line">                        visited[cur_i][cur_j] = <span class="literal">false</span>;</span><br><span class="line">                        dirc[cur_i][cur_j] = <span class="number">1</span>;</span><br><span class="line">                        count--;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">default</span>:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">exist</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">        init(board);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; board[<span class="number">0</span>].size(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (DFSTraverse(board, word, i, j))</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">test</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">        init(board);</span><br><span class="line">        <span class="keyword">return</span> DFSTraverse(board, word, <span class="number">1</span>, <span class="number">4</span>) ? <span class="literal">true</span> : <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大软院 - 人工智能期中考试复习</title>
      <link href="/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E4%B8%AD%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/"/>
      <url>/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E4%B8%AD%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰</p><a id="more"></a><h2 id="1、机器学习的定义"><a href="#1、机器学习的定义" class="headerlink" title="1、机器学习的定义"></a><strong>1、机器学习的定义</strong></h2><p><strong>对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，经过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</strong></p><p><strong>任务T：Testing Set - 测试集</strong></p><p><strong>性能度量P：Loss Function - 损失函数</strong></p><p><strong>经验E：Training Set - 训练集</strong></p><h2 id="2、Loss-Function-Cost-Function-损失函数"><a href="#2、Loss-Function-Cost-Function-损失函数" class="headerlink" title="2、Loss Function/Cost Function - 损失函数"></a><strong>2、Loss Function/Cost Function - 损失函数</strong></h2><p><strong>假设函数：</strong><br>$$<br>h_{w,b}(x) = wx+b<br>$$<br><strong>在回归任务中，多使用均方误差作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>在分类任务中，多使用交叉熵作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}[-\hat{y}^{(i)}\log(h_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-h_{w,b}(x^{(i)}))]<br>$$</p><h2 id="3、Sigmoid-Function和Softmax-Function"><a href="#3、Sigmoid-Function和Softmax-Function" class="headerlink" title="3、Sigmoid Function和Softmax Function"></a><strong>3、Sigmoid Function和Softmax Function</strong></h2><p><strong>Sigmoid Function不具体表示哪一个函数，而是表示一类S型函数，常用的有逻辑函数σ(z)：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$<br><strong>Softmax Function，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z “压缩”到另一个K维实向量 σ(z)  中，使得每一个元素的范围都在(0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：</strong><br>$$<br>\sigma(z)_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \quad j = 1,…,K<br>$$</p><h2 id="4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><a href="#4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE" class="headerlink" title="4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE"></a><strong>4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</strong></h2><p><strong>Logistic Regression的假设函数如下：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}} \quad z(x) = wx+b<br>$$</p><p><strong>σ(z)分别对w和b求导，结果为：</strong><br>$$<br>\frac{\partial \sigma(z)}{\partial w} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial w}= \sigma(z)(1-\sigma(z))*x<br>$$</p><p>$$<br>\frac{\partial \sigma(z)}{\partial b} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial b}= \sigma(z)(1-\sigma(z))<br>$$</p><p><strong>如果使用MSE作为损失函数的话，那写出来是这样的：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>当我们使用梯度下降来进行凸优化的时候，分别需要计算L(w,b)对w和b的偏导数：</strong></p><p>$$<br>\frac{\partial L(w,b)}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b)}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))<br>$$</p><p><strong>所以在σ(x)接近于1或者0的时候，也就是预测的结果和真实结果很相近或者很不相近的时候，σ(x)和1-σ(x)中总有一个会特别小，这样会导致梯度很小，从而使得优化速度大大减缓。</strong></p><p><strong>而当使用Cross Entropy作为损失函数时，损失函数为：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}(-\hat{y}^{(i)}\log(\sigma_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-\sigma_{w,b}(x^{(i)})))<br>$$</p><p><strong>L(w,b)分别对w和b求偏导，结果如下：</strong><br>$$<br>\frac{\partial L(w,b) }{\partial w}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b) }{\partial b}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})<br>$$</p><p><strong>这样梯度始终和预测值与真实值之差挂钩，预测值与真实值偏离很大时，梯度也会很大，偏离很小时，梯度会很小。所以我们更倾向于使用Cross Entropy而不使用MSE。函数图如下所示：</strong><br><img src="/科大软院 - 人工智能期中考试复习/1_1.png" alt="Total Loss 函数图"></p><h2 id="5、一堆优化方法"><a href="#5、一堆优化方法" class="headerlink" title="5、一堆优化方法"></a><strong>5、一堆优化方法</strong></h2><p><strong>这堆优化方法只有一个目标：寻求合适的w和b（两个参数情况），使得L(w,b)损失函数的值最小。</strong></p><h3 id="1-Gradient-Descent-梯度下降"><a href="#1-Gradient-Descent-梯度下降" class="headerlink" title="(1) Gradient Descent - 梯度下降"></a><strong>(1) Gradient Descent - 梯度下降</strong></h3><p><strong>如果需要找到一个函数的局部极小值，必须朝着函数上当前点所对应梯度（或者是近似梯度）的反方向，前进规定步长的距离进行迭代搜索。</strong></p><p>$$<br>w’ \leftarrow w - \eta \frac{\partial L(w,b)}{\partial w}<br>$$</p><p>$$<br>b’ \leftarrow b - \eta \frac{\partial L(w,b)}{\partial b}<br>$$</p><p><strong>tips: 这里不可以将更新好的w’代入到L(w,b)中然后计算b’，参数w和b都必须等计算好后一起更新。</strong></p><p><strong>η代表学习率，部分决定了每一次更新的步长大小，如果学习率太低会导致更新十分缓慢，学习率太高又会导致步长太大全场乱跑。</strong></p><h4 id="为什么要以梯度的反方向为更新方向？"><a href="#为什么要以梯度的反方向为更新方向？" class="headerlink" title="为什么要以梯度的反方向为更新方向？"></a><strong>为什么要以梯度的反方向为更新方向？</strong></h4><p><strong>因为梯度方向是函数方向导数最大的方向，所以沿着梯度方向的反方向更新的话，函数下降的变化率最大。（感谢zzj在评论中指正）</strong></p><h3 id="2-Stochastic-Gradient-Descent-随机梯度下降"><a href="#2-Stochastic-Gradient-Descent-随机梯度下降" class="headerlink" title="(2) Stochastic Gradient Descent - 随机梯度下降"></a><strong>(2) Stochastic Gradient Descent - 随机梯度下降</strong></h3><p><strong>随机梯度下降的损失函数要这样定义：</strong></p><p>$$<br>L^{(i)}(w,b) = \frac{1}{2}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p><strong>对比梯度下降的损失函数，发现这里少了求和以及求平均值的过程，因为这里只有一个样本作为参考，这个损失函数也是关于这一个样本的损失函数。</strong></p><p><strong>可以这样来理解：训练集里有20个样本，现在我就当作我只拥有这20个样本的其中一个样本，然后使用这一个样本更新参数，然后再挑另一个样本，更新参数，以此类推，直到所有样本都被用完后，这一轮随机梯度下降就结束了。</strong></p><p><strong>和梯度下降比较一下，随机梯度下降的好处是：梯度下降每次更新都用到了所有的样本，也就是使用所有样本的信息进行一次参数更新。而随机梯度下降每次更新只用到了一个样本，如果这个训练集有m个样本，那么梯度下降更新一次参数，随机梯度下降已经更新了m次参数了。所以随机梯度下降的更新频率是梯度下降的m倍，更新速度更快；</strong></p><p><strong>而随机梯度下降所带来的坏处是：随机梯度下降的更新只参考了一个样本，这种参考有点管中窥豹了，是不可能顾全大局的，所以更新时候的抖动现象很明显，就是参数的前进方向乱七八糟的，但是总体来说还是向着最低点前进。</strong></p><h3 id="3-Mini-batch-Gradient-Descent-Mini-batch梯度下降"><a href="#3-Mini-batch-Gradient-Descent-Mini-batch梯度下降" class="headerlink" title="(3) Mini-batch Gradient Descent - Mini-batch梯度下降"></a><strong>(3) Mini-batch Gradient Descent - Mini-batch梯度下降</strong></h3><p><strong>Mini-batch梯度下降是梯度下降和随机梯度下降的中和版本，随机梯度下降每次更新只考虑一个样本，梯度下降每次更新考虑所有样本，而Mini-batch梯度下降每次更新所考虑的样本是可以被指定的，如果总共有m个样本，那就可以在1~m中任意指定。</strong></p><p><strong>如果每次更新时所参考的样本数合适，那么既兼顾了随机梯度下降更新速度快的特性，又兼顾了梯度下降更新的稳定性。</strong></p><p><strong>可以说梯度下降和随机梯度下降都是Mini-batch梯度下降的一个特例，使用梯度下降时，指定每次更新参考全部样本，而使用随机梯度下降时，每次更新只参考1个样本。</strong></p><p><strong>注意：虽然随机梯度下降和Mini-batch梯度下降都是基于一部分数据进行参数更新，但是更新完后查看损失函数是基于全部训练数据所得出的训练误差。</strong></p><h3 id="4-Adagrad算法"><a href="#4-Adagrad算法" class="headerlink" title="(4) Adagrad算法"></a><strong>(4) Adagrad算法</strong></h3><p><strong>像Adagrad这一类自适应算法都是使用了自适应的学习率，他们都有一个基本思路：在整个训练过程中使用同一个学习率是不合适的，因为在训练开始时，损失值肯定是比较大的，所以需要较大的学习率，而训练快要结束时，越来越接近最低点了，此时需要较小学习率。所以这类算法会依据某些因素，在迭代过程中，逐渐减小学习率。</strong></p><p><strong>比如可以按下面这个公式来设计自适应的学习率，其中t代表了迭代次数：</strong></p><p>$$<br>\eta^t=\frac{\eta}{\sqrt{t+1}}<br>$$</p><p><strong>这样学习率就会根据迭代次数来放缓学习率，从而达到学习率越来越小的目的。但是这样还是不够的，因为还要考虑到具体的函数情况。Adagrad算法不仅要求学习率跟着迭代次数变化，就是按上面的公式算出$\eta^t$，还要再除以之前所有梯度的平方平均数，$g^t$代表了第t次迭代时的梯度。</strong></p><p>$$<br>w^1 \leftarrow w^0 - \frac{\eta^0}{\sigma^0}g^0 \qquad \sigma^0 = \sqrt{(g^0)^2}<br>$$</p><p>$$<br>w^2 \leftarrow w^1 - \frac{\eta^1}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\frac{1}{2}[(g^0)^2+(g^1)^2]}<br>$$</p><p>$$<br>w^3 \leftarrow w^2 - \frac{\eta^2}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\frac{1}{3}[(g^0)^2+(g^1)^2 + (g^2)^2]}<br>$$</p><p>$$<br>……<br>$$</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}<br>$$</p><p><strong>其中有$\eta^t=\frac{\eta}{\sqrt{t+1}}​$代入到最后一个式子就可以得到：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\frac{\eta}{\sqrt{t+1}}}{\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p><p><strong>同时约去$\frac{1}{\sqrt{t+1}}$后就可以得到：</strong></p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p><p><strong>这就是Adagrad算法的公式了，我们可以看到这个式子中，学习率是会一直除以前面所有梯度的平方和再开根号的，这一定是一个大于0的数，所以学习率会越来越小。但是防止一开始的时候梯度就是0，如果让分母变为0会导致错误的，所以后面还要跟一个很小的正数$\epsilon​$，最终的式子是这样的：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2+\epsilon}}g^t<br>$$</p><p><strong>Adagrad算法也有很多不足：</strong></p><p><strong>a) 如果初始的学习率设置过大的话，这个学习率要除以一个较大梯度，那么此算法会对梯度的调节太大；</strong><br><strong>b) 在训练的中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束。</strong></p><h3 id="5-RMSprop算法"><a href="#5-RMSprop算法" class="headerlink" title="(5) *RMSprop算法"></a><strong>(5) *RMSprop算法</strong></h3><p><strong>我感觉最多考到Adagrad算法就行了，RMSprop应该考不到。</strong></p><p><strong>在凸优化问题上，Adagrad算法具有很好的效果，但是在神经网络情况下，很多问题都是非凸优化问题，即损失函数有很多局部最小值，有了Adagrad算法的改进版RMSprop算法，RMSprop算法就像是真实物理世界中一个小球在山坡上向下滑，如果滑落到一个山谷中，小球是不会立刻停在这里的，由于具有惯性的原因，小球会继续向前冲，如果惯性足够的话，可能会再次冲出山头，更有可能会落到另一个更低的山谷中。而传统的梯度下降法只会落在一个山谷中，没有机会冲出来。</strong><br>$$<br>w^1 \leftarrow w^0 - \frac{\eta}{\sigma^0}g^0 \qquad \sigma^0 = g^0<br>$$</p><p>$$<br>w^2 \leftarrow w^1 - \frac{\eta}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\alpha(\sigma^0)^2 + (1-\alpha)(g^1)^2}<br>$$</p><p>$$<br>w^3 \leftarrow w^2 - \frac{\eta}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\alpha(\sigma^1)^2 + (1-\alpha)(g^2)^2}<br>$$</p><p>$$<br>……<br>$$</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1-\alpha)(g^t)^2}<br>$$</p><p><strong>Adagrad和RMSprop算法这两个算法很相近，不同之处在于RMSprop算法增加了一个衰减系数α来控制历史信息的获取多少。</strong></p><h2 id="6、Cross-Validation-交叉验证"><a href="#6、Cross-Validation-交叉验证" class="headerlink" title="6、Cross Validation - 交叉验证"></a><strong>6、Cross Validation - 交叉验证</strong></h2><p><strong>将数据集D划分成k个大小相似的互斥子集，每次用k-1个子集作为训练集，余下的子集做测试集，最终返回k个训练结果的平均值。交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_2.png" alt="cross validation"></p><h3 id="为什么要用交叉验证-Cross-Validation）"><a href="#为什么要用交叉验证-Cross-Validation）" class="headerlink" title="为什么要用交叉验证(Cross-Validation）"></a><strong>为什么要用交叉验证(Cross-Validation）</strong></h3><p><strong>a) 交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合；</strong><br><strong>b) 还可以从有限的数据中获取尽可能多的有效信息。</strong></p><p><strong>注意：交叉验证使用的仅仅是训练集！</strong></p><h2 id="7、偏差、方差、噪声以及泛化误差"><a href="#7、偏差、方差、噪声以及泛化误差" class="headerlink" title="7、偏差、方差、噪声以及泛化误差"></a><strong>7、偏差、方差、噪声以及泛化误差</strong></h2><h3 id="1-偏差"><a href="#1-偏差" class="headerlink" title="(1) 偏差"></a><strong>(1) 偏差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_3.png" alt="bias"></p><p><strong>偏差bias： 期望预测与真实标记的误差，偏差越大偏离理论值越大。</strong></p><p><strong>上面的ppt里，左边的图模型阶数小，所以不可能很好的拟合真实情况，所作出的预测与真实情况偏离程度大，所以偏差就大；而右边的图里模型很复杂，足够模拟出真实情况，所以与真实情况偏离程度小，偏差就小。</strong></p><p><strong>在一个训练集$D$上模型$f$对测试样本$x$预测输出为$f(x;D)$, 那么学习算法$f$对测试样本$x$的期望预测为：</strong></p><p>$$<br>\bar{f}(x)=E_D[f(x;D)]<br>$$<br><strong>这里用偏差的平方来表示偏差的计算公式：</strong></p><p>$$<br>Bias^2(x)=(\bar{f}(x)-\hat{y})^2<br>$$</p><h3 id="2-方差"><a href="#2-方差" class="headerlink" title="(2) 方差"></a><strong>(2) 方差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_4.png" alt="variance"></p><p><strong>方差variance：预测模型的离散程度，方差越大离散程度越大。</strong></p><p><strong>就像上面ppt的左图，因为模型阶数小，所以模型都很集中；而右边的模型阶数大，有时能拟合的很好，有时拟合的不好，不稳定因素太大了。</strong></p><p><strong>使用样本数相同的不同训练集产生的方差为:</strong></p><p>$$<br>var(x)=E_D[(f(x;D)-\bar{f}(x))^2]<br>$$</p><h3 id="3-噪声"><a href="#3-噪声" class="headerlink" title="(3) *噪声"></a><strong>(3) *噪声</strong></h3><p><strong>不可能考</strong></p><p><strong>噪声为真实标记与数据集中的实际标记间的偏差$y_D$表示在数据集中的标记，$\hat{y}$表示真实标记，这两个可能不等）：</strong></p><p>$$<br>\epsilon=E_D[(y_D-\hat{y})^2]<br>$$</p><h3 id="4-泛化误差"><a href="#4-泛化误差" class="headerlink" title="(4) *泛化误差"></a><strong>(4) *泛化误差</strong></h3><p><strong>也不可能考</strong></p><p><strong>学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). 在估计学习算法性能的过程中, 我们主要关注偏差与方差。因为噪声属于不可约减的误差 (irreducible error)。</strong></p><p><strong>下面来用公式推导泛化误差与偏差与方差, 噪声之间的关系。</strong></p><p><strong>以回归任务为例, 学习算法的平方预测误差期望为：</strong></p><p>$$<br>Err(x)=E_D[(y_D-f(x;D))^2]<br>$$</p><p><strong>对算法的期望泛化误差进行分解，就会发现 泛化误差=偏差的平方+方差+噪声:</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_5.png" alt="1_5"></p><p><img src="/科大软院 - 人工智能期中考试复习/1_6.png" alt="1_6"></p><h2 id="8、欠拟合和过拟合"><a href="#8、欠拟合和过拟合" class="headerlink" title="8、欠拟合和过拟合"></a><strong>8、欠拟合和过拟合</strong></h2><p><strong>欠拟合：模型拟合不够，在训练集上拟合情况很差。往往会出现偏差大、方差小的情况；</strong></p><p><strong>过拟合：模型过度拟合，在训练集上拟合情况很好，但是在测试集上拟合情况很差。往往会出现偏差小、方差大的情况。</strong></p><p><strong>出现欠拟合时，解决办法有：</strong></p><p><strong>a) 增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间;</strong><br><strong>b) 尝试非线性模型，比如核SVM 、决策树、DNN等模型;</strong><br><strong>c) 如果有正则项可以减小正则项参数λ；</strong><br><strong>d) Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。</strong></p><p><strong>出现过拟合时，解决办法有：</strong></p><p><strong>a) 交叉检验，通过交叉检验得到较优的模型参数;</strong><br><strong>b) 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</strong><br><strong>c) 正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</strong><br><strong>d) 如果有正则项则可以考虑增大正则项参数 λ;</strong><br><strong>e) 增加训练数据可以有限的避免过拟合;</strong><br><strong>f) Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等。</strong></p><h2 id="9、L1范数和L2范数"><a href="#9、L1范数和L2范数" class="headerlink" title="9、L1范数和L2范数"></a><strong>9、L1范数和L2范数</strong></h2><p><strong>本来以为会很快写完这个部分的，没想到这里的知识点看了一晚上，更没想到这里的知识完完整整拿出来，完全可以写一篇长长的博文。如果想好好学习这里的知识的话，我推荐<a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">这个博客</a>，或者是周老师的西瓜书第252页。这里我就介绍一点最基本的知识了。</strong></p><p><strong>L1范数：向量元素绝对值之和，即：</strong></p><p>$$<br>\left | \theta \right |_1=\sum_{i=1}^{n}\left | \theta_i \right |<br>$$</p><p><strong>L2范数：这里我查了好久的资料，我发现网上所有的资料，包括周老师的西瓜书以及邱锡鹏教授的《神经网络与深度学习》都是说，L2范数是各个元素的平方和再求平方根，即：</strong></p><p>$$<br>\left | \theta \right |_2=\sqrt{\sum_{i=1}^{n}\theta_i^2}<br>$$</p><p><strong>但是吴恩达教授、李宏毅教授课上讲的版本，以及我在学校课上（我们学校的课不就是照搬李宏毅教授的课嘛）所记录的版本，都是写的L2范数为各个元素的平方和，不再求平方根，即：</strong></p><p>$$<br>\left | \theta \right |_2=\sum_{i=1}^{n}\theta_i^2<br>$$</p><p><strong>吴恩达教授：</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_7.png" alt="1_7"></p><p><strong>李宏毅教授：</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_8.png" alt="1_8"></p><p><strong>我的笔记就不拿出来了，和前两位的差不多。在这里我还是使用这个不开根号的版本。</strong></p><h3 id="1-L1范数的作用"><a href="#1-L1范数的作用" class="headerlink" title="(1) L1范数的作用"></a><strong>(1) L1范数的作用</strong></h3><p><strong>L1范数可以产生稀疏权值矩阵，一定程度上也可以防止过拟合。</strong></p><p><strong>稀疏权值矩阵的意思是，这个矩阵中大部分元素都是0，只有很小一部分元素不为0。试想一下当我们在看周杰伦演唱会时，只会把注意力放在周董身上，不太会关注伴舞小姐姐，更不可能去关注旁边又矮又胖的音响（这一句考试别写）。而在计算机视觉领域也一样，传进来一张图片，这个图片有很多很多的像素，但是机器真正要关注的只有其中一部分元素，比如一张田地里的农民，或是草地里的狗狗之类的，其他不重要的信息就直接不看了，所以和主题无关的信息都会乘上0，有意义的信息才会保留。自然语言处理也是一样，有很多无意义的词都会乘上0，只挑选有意义的信息保留下来。关于具体怎么实现的不讲了，去看周老师的西瓜书吧。</strong></p><h3 id="2-L2范数的作用"><a href="#2-L2范数的作用" class="headerlink" title="(2) L2范数的作用"></a><strong>(2) L2范数的作用</strong></h3><p><strong>L2范数主要用于防止模型过拟合。</strong></p><p><strong>L2范数比较重要一点，L2范数的作用是权值衰减，缩小各个权值，使得函数尽可能比不加L2范数平滑很多，增大模型的泛化能力。如果损失函数加上L2范数后，是这样：</strong></p><p>$$<br>L(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]<br>$$</p><p><strong>当使用梯度下降时，得出的式子是这样的：</strong><br>$$<br>\theta_j’ \leftarrow (1-\eta\frac{\lambda}{m})\theta_j - \eta\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})x^{(i)}<br>$$</p><p><strong>每次参数更新时，都要先把原来的参数乘上一个介于0和1的数，所以参数都会先缩小一点点，再进行更新。一般来说$1-\eta\frac{\lambda}{m}$都会是一个很接近1的数，别看一个数打九九折后没什么变化，可是迭代次数多了后，这个权值是会衰减的很严重的。通过这种方法会有效降低权值，使得函数更为平滑，使得模型的泛化能力更强。</strong></p><h2 id="10、优化技巧"><a href="#10、优化技巧" class="headerlink" title="10、优化技巧"></a><strong>10、优化技巧</strong></h2><p><strong>训练集中常常有些特征的范围差别很大，这样会导致在优化时，不同的参数优化速度差别很大。比如房子的大小从10~2000平米都有可能，而房间数只能处在1~10这个范围内，在进行优化时，房间数的参数很快就找到了最低点，而房子大小的参数还离最低点很远，这样的话，房间数的参数就在最低点来回波动，等待房子大小的参数优化好。如果能尽量让所有参数同时优化好，那么会大大提高优化速度。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_优化.png" alt="1_优化"></p><h3 id="1-Feature-Scaling-特征缩放-归一化"><a href="#1-Feature-Scaling-特征缩放-归一化" class="headerlink" title="(1) Feature Scaling - 特征缩放/归一化"></a><strong>(1) Feature Scaling - 特征缩放/归一化</strong></h3><p><strong>归一化的思路是：让输入的值减去样本中最小的值，然后除以样本范围（也就是样本最大值减去样本最小值），这样会使得结果永远在0~1的范围内，所有特征参数差不多一个速度优化到最低点。式子如下：</strong></p><p>$$<br>x’=\frac{x-min}{max-min}<br>$$</p><h3 id="2-Mean-Normalization-均值标准化"><a href="#2-Mean-Normalization-均值标准化" class="headerlink" title="(2) Mean Normalization - 均值标准化"></a><strong>(2) Mean Normalization - 均值标准化</strong></h3><p><strong>均值标准化的思路是：让输入的值减去样本平均数μ，再除以样本标准差σ。经过这样的处理，数据的均值会是0，大小在-1~1之间。均值标准化和归一化一样，也有去除不同特征量纲不同的问题，另外机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布，所以数据以0为中心左右分布会带来很多便利。</strong><br>$$<br>{x}’=\frac{x-\mu}{\sigma}<br>$$</p><h2 id="11、怎么调节学习率"><a href="#11、怎么调节学习率" class="headerlink" title="11、怎么调节学习率"></a><strong>11、怎么调节学习率</strong></h2><p><strong>当我们在训练过程中，发现loss下降的很慢时，可以适当增大学习率；发现loss不降反增的时候，要降低学习率。</strong></p><p><strong>如果我们使用的是Adagrad算法，那么一开始学习率可以设置的相对大一点。</strong></p><h2 id="12、逻辑回归的局限性-深度神经网络的兴起导火索"><a href="#12、逻辑回归的局限性-深度神经网络的兴起导火索" class="headerlink" title="12、逻辑回归的局限性 - 深度神经网络的兴起导火索"></a><strong>12、逻辑回归的局限性 - 深度神经网络的兴起导火索</strong></h2><p><strong>逻辑回归不可以直接处理线性不可分的问题，例如下图所示的异或问题。处理方法是先通过一个函数，将原来的值投影转换，变成线性可分问题，再使用逻辑回归来处理。这一步叫做特征变换，先进行特征变换，再进行逻辑回归，这就是两层神经网络的雏形，也是神经网络的兴起的导火索。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1深度学习导火索.jpg" alt="1深度学习导火索"></p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dnn </tag>
            
            <tag> review for exam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于TensorFlow的CAPTCHA注册码识别实验</title>
      <link href="/%E5%9F%BA%E4%BA%8ETensorFlow%E7%9A%84CAPTCHA%E6%B3%A8%E5%86%8C%E7%A0%81%E8%AF%86%E5%88%AB%E5%AE%9E%E9%AA%8C/"/>
      <url>/%E5%9F%BA%E4%BA%8ETensorFlow%E7%9A%84CAPTCHA%E6%B3%A8%E5%86%8C%E7%A0%81%E8%AF%86%E5%88%AB%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<ul><li>实验任务：使用TensorFlow实现CAPTCHA注册码的识别</li><li>基本思路：采用 Captcha 库生成验证码，将验证码识别问题转化为分类问题，采用 CNN 网络模型进行训练，最终实现对验证码的破解。</li><li>实验步骤：获取验证码训练集 → 构建卷积神经网络和全连接神经网络 → 定义损失函数以及优化方式 → 进行训练并保存训练结果。</li></ul><a id="more"></a><h2 id="获取验证码训练集"><a href="#获取验证码训练集" class="headerlink" title="获取验证码训练集"></a>获取验证码训练集</h2><p>使用captcha库来生成验证码，captcha库可以使用anaconda很轻松下载，captcha库不仅可以用于生成图片验证码，还可以用来生成语音验证码。</p><ol><li>我们将ImageCaptcha类实例化为image对象，构造函数的参数里指定宽度和高度。</li></ol><p>image = ImageCaptcha(width=self.width, height=self.height)</p><ol start="2"><li><p>存储训练数据。</p><p>我们创建两个矩阵X和Y用以表示训练集的输入部分和输出部分，也就是验证码的图片数据存储以及验证码上所写的内容存储。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.zeros([batch_size, self.height, self.width, <span class="number">1</span>])</span><br><span class="line">Y = np.zeros([batch_size, self.char_num, self.classes])</span><br></pre></td></tr></table></figure></div><p>这两个矩阵的第一维参数都是batch_size，表示这一组训练集的个数。</p><p>X的第二和第三位参数分别表示验证码的高度和宽度，最后一个参数表示这是1通道的黑白图片。</p><p>Y的第二位参数表示此验证码上写了几个字符，第三位参数表示每个字符共有多少种选择，这是一条长为self.classes的向量，以独热编码的形式记录信息。</p></li><li><p>生成验证码</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        captcha_str = <span class="string">''</span>.join(random.sample(self.characters, self.char_num))</span><br><span class="line">        img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br><span class="line">        img = np.array(img.getdata())</span><br><span class="line">        X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">            Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br><span class="line">    Y = np.reshape(Y, (batch_size, self.char_num * self.classes))</span><br><span class="line">    <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure></div><p>首先，这个总体的无限循环是一个训练集的生成器，执行此代码后，会在最后的yield语句返回训练集X和Y，然后循环结束。下次再想生成验证码训练集时，会从yield语句（最后一句）开始，回到开头再执行一次循环。</p><p>这样的好处是，以前都是将所有的训练数据全部准备好，然后手动分出很多batch，一个一个的训练，这样的话要求把所有的训练数据全部装入内存，很浪费内存空间。而使用这种生成器语句，如果我训练哪一个batch的数据，就立即生成数据并装入内存，用完再撤出内存，紧接着训练下一个batch的数据时，再立即生成然后将其装入内存。分批装入内存，节省了大量的内存空间。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random.sample(self.characters, self.char_num)</span><br></pre></td></tr></table></figure></div><p>此代码生成一个验证码字符串的随机变量，self.characters为62位的字符串（0~9A~Za~z），self.char_num=4（生成4个字符）。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br></pre></td></tr></table></figure></div><p>这句代码使用的是ImageCaptcha类的内置方法，将字符串变为图片。convert(‘L’)：表示生成的是灰度图片，就是通道数为1的黑白图片。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure></div><p>每个像素值都要除以255，这是为了归一化处理，因为灰度的范围是0~255，这里除以255就让每个像素的值在0~1之间，目的是为了加快收敛速度。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">    Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br></pre></td></tr></table></figure></div><p>这里用以生成对应的测试集Y，j和ch用以遍历刚刚生成的随机字符串，j记录index（0~3，表示第几个字符），ch记录字符串中的字符。找到Y的第i条数据中的第j个字符，然后把62长度的向量和ch相关的那个置为1。</p></li></ol><h2 id="构建卷积神经网络和全连接神经网络"><a href="#构建卷积神经网络和全连接神经网络" class="headerlink" title="构建卷积神经网络和全连接神经网络"></a>构建卷积神经网络和全连接神经网络</h2><ol><li><p>conv2D</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self, x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></div><p>这是2维卷积函数。x表示传入的待处理图片，W表示卷积核，strides=[1, 1, 1, 1]，其中第二个和第三个1分别表示x方向步长和y方向步长，padding=’SAME’表示边界处理策略设为’SAME’，这样卷积处理完图片大小不变。</p></li><li><p>max_pool_2x2</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></div><p>这是2x2最大值池化函数。x表示待被池化处理的图片，ksize=[1, 2, 2, 1]，其中第二个和第三个2分别表示池化窗口高度和池化窗口宽度，strides和padding意义同上。</p></li><li><p>weight_variable</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></div><p>这就是一个很有意思的函数了，用这个函数的目的是从<strong>截断</strong>的正态分布中输出随机值。听着名字就不知道是个什么鬼，我们先看看正太分布的一个性质：</p><p>在正态分布的曲线中，横轴区间（μ-σ，μ+σ）内的面积为68.268949%。</p><p>横轴区间（μ-2σ，μ+2σ）内的面积为95.449974%。</p><p>横轴区间（μ-3σ，μ+3σ）内的面积为99.730020%。</p><p>X落在（μ-3σ，μ+3σ）以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间（μ-3σ，μ+3σ）看作是随机变量X实际可能的取值区间，这称之为正态分布的“3σ”原则。</p><p><img src="/基于TensorFlow的CAPTCHA注册码识别实验/captcha_1.jpg" alt="正态分布"></p><p>而从<strong>截断</strong>的正态分布中输出随机值的意思就是，同样也是随机生成的值，但是生成的值必须服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。换句话说，就是如果随机生成的值如果落在了(μ-2σ,μ+2σ)之外，就重新生成值，这样就保证了随机生成的值都在均值附近。</p><p>扯了这么多，其实就是随机生成一组数，要求不要离中心点μ太远。</p></li><li><p>bias_variable</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></div><p>生成一组全部都是0.1的常量数，没啥好说的。</p></li><li><p>构建第一层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv1 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = self.bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(tf.nn.bias_add(self.conv2d(x_images, w_conv1), b_conv1))</span><br><span class="line">h_pool1 = self.max_pool_2x2(h_conv1)</span><br><span class="line">h_dropout1 = tf.nn.dropout(h_pool1, keep_prob)</span><br><span class="line">conv_width = math.ceil(self.width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(self.height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv1是卷积核，可以理解为一共有32个卷积核，每个卷积核的尺寸是(5, 5, 1)，即长度和宽度都是5，通道是1。每个卷积核对图片处理完就会产生一张特征图，32个卷积核对图片处理完后就会产生32个特征图，将这些特征图叠加排列，那么原本通道数为1的图片现在通道数变为图片的个数，也就是32。图片的尺寸变化为(?, 60, 160, 1) –&gt; (?, 60, 160, 32)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 60, 160, 32) → (?, 30, 80, 32)。</p><p>随后又进行了一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第二层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv2 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout1, w_conv2), b_conv2))</span><br><span class="line">h_pool2 = self.max_pool_2x2(h_conv2)</span><br><span class="line">h_dropout2 = tf.nn.dropout(h_pool2, keep_prob)</span><br><span class="line">conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv2是第二层卷积神经网络的卷积核，共有64个，每个卷积核的尺寸是(5, 5, 32)，处理之后图片的尺寸变化为(?, 30, 80, 32) → (?, 30, 80, 64)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 30, 80, 64) → (?, 15, 40, 64)。</p><p>再进行一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第三层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv3 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b_conv3 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv3 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout2, w_conv3), b_conv3))</span><br><span class="line">h_pool3 = self.max_pool_2x2(h_conv3)</span><br><span class="line">h_dropout3 = tf.nn.dropout(h_pool3, keep_prob)</span><br><span class="line">conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv3是第三层卷积神经网络的卷积核，共有64个，每个卷积核的尺寸是(5, 5, 64)，处理之后图片的尺寸变化为(?, 15, 40, 64) → (?, 15, 40, 64)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 15, 40, 64) → (?, 8, 20, 64)，这里的15 / 2 = 8，是因为边界策略为SAME，那么遇到剩下还有不足4个像素的时候同样采取一次最大值池化处理。</p><p>再进行一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第一层全连接神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv_width = int(conv_width)</span><br><span class="line">conv_height = int(conv_height)</span><br><span class="line">w_fc1 = self.weight_variable([<span class="number">64</span> * conv_width * conv_height, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = self.bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_dropout3_flat = tf.reshape(h_dropout3, [<span class="number">-1</span>, <span class="number">64</span> * conv_width * conv_height])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_dropout3_flat, w_fc1), b_fc1))</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure></div><p>这里就把刚刚卷积神经网络的输出作为传统神经网络的输入了，w_fc1(10240, 1024)和b_fc1(1024)分别是这一层神经网络的参数以及bias。上面代码第五行将卷积神经网络的输出数据由(?, 8, 20, 64)转为了(?, 64 <em> 20 </em> 8)，可以很明显感觉出来把所有的数据拉成了一条一维向量，然后经过矩阵处理，这里的数据变为了(1024, 1)的形状。</p></li><li><p>构建第二层全连接神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w_fc2 = self.weight_variable([<span class="number">1024</span>, self.char_num * self.classes])</span><br><span class="line">b_fc2 = self.bias_variable([self.char_num * self.classes])</span><br><span class="line">y_conv = tf.add(tf.matmul(h_fc1_drop, w_fc2), b_fc2)</span><br></pre></td></tr></table></figure></div><p>再连接一次神经网络，这次不再需要添加激励函数了ReLu了，因为已经到达输出层，线性相加后直接输出就可以了，结果保存在y_conv变量里，最后将y_conv返回给调用函数。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_conv = model.create_model(x, keep_prob)</span><br></pre></td></tr></table></figure></div><p>这是外层函数调用model后所得到的训练结果。</p></li></ol><h2 id="定义损失函数以及优化方式"><a href="#定义损失函数以及优化方式" class="headerlink" title="定义损失函数以及优化方式"></a>定义损失函数以及优化方式</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br></pre></td></tr></table></figure></div><p>由于识别验证码本质上是对验证码中的信息进行分类，所以我们这里使用cross_entropy的方法来衡量损失。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></div><p>优化方式选择的是AdamOptimizer，学习率设置比较小，为1e-4，防止学习的太快而训练不好。</p><h2 id="进行训练并保存训练结果"><a href="#进行训练并保存训练结果" class="headerlink" title="进行训练并保存训练结果"></a>进行训练并保存训练结果</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br><span class="line">        _, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line">        print(<span class="string">'step:%d,loss:%f'</span> % (step, loss))</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">            acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">            <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">                saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        step += <span class="number">1</span></span><br></pre></td></tr></table></figure></div><p>从第二行开始进入训练部分，首先我们需要初始化变量，然后进入一个循环，直到训练的准确度高于99%才会停止训练，否则会一直训练下去。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br></pre></td></tr></table></figure></div><p>用于生成训练集，每次生成64条训练数据，输入部分保存在batch_x里，输出部分保存在batch_y里，每次循环到这里就会再次读入64条训练数据。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure></div><p>开始训练，loss记录损失，_变量不记录任何东西，我debug下来每次_里面都是空，可能只是为了语法的需要，这里必须有个变量接受点什么。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">    acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">    print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">    <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">        saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></div><p>每训练一百次，就会进行一次Test。具体操作是，生成100条测试数据，和之前生成训练数据是一个方法，然后直接进行测试，如果准确度高于99%那么就停止训练，把训练好的模型放在”capcha_model.ckpt”这个文件里面，可以供以后使用。如果没有达到99%的准确度就继续训练。</p><h2 id="六、完整代码"><a href="#六、完整代码" class="headerlink" title="六、完整代码"></a>六、完整代码</h2><h3 id="train-captcha-py"><a href="#train-captcha-py" class="headerlink" title="train_captcha.py"></a>train_captcha.py</h3><p>训练的主代码</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> generate_captcha</span><br><span class="line"><span class="keyword">import</span> captcha_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    captcha = generate_captcha.GenerateCaptcha()  <span class="comment"># 创建一个验证码对象</span></span><br><span class="line">    width, height, char_num, characters, classes = captcha.get_parameter()  <span class="comment"># 获取此验证码对象的各种属性</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x：训练集输入占位符</span></span><br><span class="line">    <span class="comment"># y：训练集输出占位符</span></span><br><span class="line">    <span class="comment"># keep_prob：dropout处理保留此神经元的概率占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, height, width, <span class="number">1</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, char_num * classes])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成训练模型对象</span></span><br><span class="line">    model = captcha_model.CaptchaModel(width, height, char_num, classes)</span><br><span class="line">    <span class="comment"># y_conv: 模型训练完的结果</span></span><br><span class="line">    y_conv = model.create_model(x, keep_prob)</span><br><span class="line">    <span class="comment"># 损失函数，用cross_entropy来描述损失</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line">    <span class="comment"># 优化器，选用AdamOptimizer</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将训练完的结果reshape为(?, 4, 62)的形状</span></span><br><span class="line">    predict = tf.reshape(y_conv, [<span class="number">-1</span>, char_num, classes])</span><br><span class="line">    <span class="comment"># 将真实值也reshape为(?, 4, 62)的形状</span></span><br><span class="line">    real = tf.reshape(y_, [<span class="number">-1</span>, char_num, classes])</span><br><span class="line">    <span class="comment"># 计算预测正确的个数</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(predict, <span class="number">2</span>), tf.argmax(real, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 转型为浮点数格式</span></span><br><span class="line">    correct_prediction = tf.cast(correct_prediction, tf.float32)</span><br><span class="line">    <span class="comment"># 求均值，就是准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(correct_prediction)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 等模型训练好后，可以用saver保存模型，便于以后使用</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># batch_x和batch_y存储64条训练数据的输入和输出值</span></span><br><span class="line">            batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br><span class="line">            <span class="comment"># loss存储本次训练的损失值</span></span><br><span class="line">            _, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line">            print(<span class="string">'step:%d,loss:%f'</span> % (step, loss))</span><br><span class="line">            <span class="comment"># 每进行100次，就进行一次测试</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 获取100条测试数据</span></span><br><span class="line">                batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">                acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">                print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">                <span class="comment"># 如果测试准确度在99%以上就退出，否则继续训练</span></span><br><span class="line">                <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">                    saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            step += <span class="number">1</span></span><br></pre></td></tr></table></figure></div><h3 id="generate-captcha-py"><a href="#generate-captcha-py" class="headerlink" title="generate_captcha.py"></a>generate_captcha.py</h3><p>生成验证码的类</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> captcha.image <span class="keyword">import</span> ImageCaptcha</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenerateCaptcha</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 width=<span class="number">160</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 height=<span class="number">60</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 char_num=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 characters=string.digits + string.ascii_uppercase + string.ascii_lowercase)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        GenerateCaptcha类的构造函数</span></span><br><span class="line"><span class="string">        :param width: 验证码图片的宽</span></span><br><span class="line"><span class="string">        :param height: 验证码图片的高</span></span><br><span class="line"><span class="string">        :param char_num: 验证码字符个数</span></span><br><span class="line"><span class="string">        :param characters: 验证码组成字符串，其中包含总数字串(0~9)+大写字母串(A~Z)+小写字母串(a~z)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">        self.char_num = char_num</span><br><span class="line">        self.characters = characters</span><br><span class="line">        self.classes = len(characters)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_captcha</span><span class="params">(self, batch_size=<span class="number">50</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        生成一组验证码训练数据</span></span><br><span class="line"><span class="string">        :param batch_size:用以指定这一组训练数据的个数</span></span><br><span class="line"><span class="string">        :return:X（这一组训练数据的输入X）, Y（这一组训练数据的输出Y）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 验证码图片对象，这里只限定了尺寸，还没有写内容</span></span><br><span class="line">        image = ImageCaptcha(width=self.width, height=self.height)</span><br><span class="line">        <span class="comment"># X:用以存放训练数据的输入部分</span></span><br><span class="line">        <span class="comment"># batch_size: 这一组训练数据共有batch_size个</span></span><br><span class="line">        <span class="comment"># self.height: 每一个训练数据（验证码）的高度</span></span><br><span class="line">        <span class="comment"># self.width: 每一个训练数据（验证码）的宽度</span></span><br><span class="line">        <span class="comment"># 1: 每一个训练数据（验证码）的通道数，因为是黑白图片所以为1</span></span><br><span class="line">        X = np.zeros([batch_size, self.height, self.width, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Y:用以存放训练数据的输出部分</span></span><br><span class="line">        <span class="comment"># batch_size: 这一组训练数据共有batch_size个</span></span><br><span class="line">        <span class="comment"># self.char_num: 每一个训练数据（验证码）的内容中包含几个字符</span></span><br><span class="line">        <span class="comment"># self.classes: 这个字符的内容是什么（这是一个长度为62的向量，因为10个数字加26个大写字母加26个小写字母长度为62，只有一个值为1，表示这个字符内容，其他都是0）</span></span><br><span class="line">        Y = np.zeros([batch_size, self.char_num, self.classes])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                <span class="comment"># random.sample(self.characters, self.char_num)用于生成一个验证码字符串随机变量</span></span><br><span class="line">                <span class="comment"># self.characters：62位字符串（0~9A~Za~z）</span></span><br><span class="line">                <span class="comment"># self.char_num：4（生成4个字符）</span></span><br><span class="line">                <span class="comment"># captcha_str用以将转好的随机变量转为字符串类型</span></span><br><span class="line">                captcha_str = <span class="string">''</span>.join(random.sample(self.characters, self.char_num))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 用内置方法，将字符串变为图片</span></span><br><span class="line">                <span class="comment"># convert('L')：表示生成的是灰度图片，就是通道数为1的黑白图片</span></span><br><span class="line">                img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br><span class="line">                <span class="comment"># 将生成好的图片数据转成np.array类型</span></span><br><span class="line">                img = np.array(img.getdata())</span><br><span class="line">                <span class="comment"># reshape一下此图片的格式，让他的高度和宽度符合要求</span></span><br><span class="line">                <span class="comment"># 每个像素值都要除以255，这是为了归一化处理，因为灰度的范围是0~255，这里除以255就让每个像素的值在0~1之间，加快收敛速度</span></span><br><span class="line">                <span class="comment"># 将此图片信息放入X的第i个位置，表示这是训练集的第i条数据</span></span><br><span class="line">                X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 这里用以生成对应的测试集</span></span><br><span class="line">                <span class="comment"># j和ch用以遍历刚刚生成的随机字符串，j记录index（0~3，表示第几个字符），ch记录字符串中的字符</span></span><br><span class="line">                <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">                    <span class="comment"># 找到Y的第i条数据中，第j个字符，然后把62长度的向量和ch相关的那个置位1</span></span><br><span class="line">                    Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br><span class="line">            <span class="comment"># 重新整理一下Y，第一维度依然是batch_size，后面把char_num和classes相乘，整合成一条向量</span></span><br><span class="line">            Y = np.reshape(Y, (batch_size, self.char_num * self.classes))</span><br><span class="line">            <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_parameter</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.width, self.height, self.char_num, self.characters, self.classes</span><br></pre></td></tr></table></figure></div><h3 id="captcha-model-py"><a href="#captcha-model-py" class="headerlink" title="captcha_model.py"></a>captcha_model.py</h3><p>训练模型的类</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 width=<span class="number">160</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 height=<span class="number">60</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 char_num=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 classes=<span class="number">62</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        训练模型类的构造函数</span></span><br><span class="line"><span class="string">        :param width: 验证码宽度</span></span><br><span class="line"><span class="string">        :param height: 验证码高度</span></span><br><span class="line"><span class="string">        :param char_num: 验证码内字符数</span></span><br><span class="line"><span class="string">        :param classes: 验证码待选字符个数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">        self.char_num = char_num</span><br><span class="line">        self.classes = classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self, x, W)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        2维卷积函数</span></span><br><span class="line"><span class="string">        strides=[1, 1（x方向步长）, 1（y方向步长）, 1]</span></span><br><span class="line"><span class="string">        padding='SAME'（边界处理策略，设为'SAME'卷积处理完图片大小不变）</span></span><br><span class="line"><span class="string">        :param x: 被卷积处理的图片</span></span><br><span class="line"><span class="string">        :param W: 卷积核</span></span><br><span class="line"><span class="string">        :return: 卷积处理完的图片</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        2x2最大值池化函数</span></span><br><span class="line"><span class="string">        ksize=[1, 2（池化窗口高度）, 2(池化窗口高度), 1]</span></span><br><span class="line"><span class="string">        strides=[1, 1（x方向步长）, 1（y方向步长）, 1]</span></span><br><span class="line"><span class="string">        padding='SAME'</span></span><br><span class="line"><span class="string">        :param x: 待被池化处理的图片</span></span><br><span class="line"><span class="string">        :return: 处理好的图片</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="comment"># 截断式正态分布生成变量</span></span><br><span class="line">        initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="comment"># 生成一组常量</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(self, x_images, keep_prob)</span>:</span></span><br><span class="line">        <span class="comment"># first layer</span></span><br><span class="line">        w_conv1 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">        b_conv1 = self.bias_variable([<span class="number">32</span>])</span><br><span class="line">        <span class="comment"># 第一次卷积 (?, 60, 160, 1) --&gt; (?, 60, 160, 32)</span></span><br><span class="line">        h_conv1 = tf.nn.relu(tf.nn.bias_add(self.conv2d(x_images, w_conv1), b_conv1))</span><br><span class="line">        <span class="comment"># 第一次池化 (?, 60, 160, 32) --&gt; (?, 30, 80, 32)</span></span><br><span class="line">        h_pool1 = self.max_pool_2x2(h_conv1)</span><br><span class="line">        <span class="comment"># 第一层dropout</span></span><br><span class="line">        h_dropout1 = tf.nn.dropout(h_pool1, keep_prob)</span><br><span class="line">        conv_width = math.ceil(self.width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(self.height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># second layer</span></span><br><span class="line">        w_conv2 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">        b_conv2 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">        <span class="comment"># 第二次卷积 (?, 30, 80, 32) --&gt; (?, 30, 80, 64)</span></span><br><span class="line">        h_conv2 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout1, w_conv2), b_conv2))</span><br><span class="line">        <span class="comment"># 第二次池化 (?, 30, 80, 64) --&gt; (?, 15, 40, 64)</span></span><br><span class="line">        h_pool2 = self.max_pool_2x2(h_conv2)</span><br><span class="line">        <span class="comment"># 第二层dropout</span></span><br><span class="line">        h_dropout2 = tf.nn.dropout(h_pool2, keep_prob)</span><br><span class="line">        conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># third layer</span></span><br><span class="line">        w_conv3 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">        b_conv3 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">        <span class="comment"># 第三次卷积 (?, 15, 40, 64) --&gt; (?, 15, 40, 64)</span></span><br><span class="line">        h_conv3 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout2, w_conv3), b_conv3))</span><br><span class="line">        <span class="comment"># 第三次池化 (?, 15, 40, 64) --&gt; (?, 8, 20, 64)</span></span><br><span class="line">        h_pool3 = self.max_pool_2x2(h_conv3)</span><br><span class="line">        <span class="comment"># 第三层dropout</span></span><br><span class="line">        h_dropout3 = tf.nn.dropout(h_pool3, keep_prob)</span><br><span class="line">        conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层全连接神经网络</span></span><br><span class="line">        conv_width = int(conv_width)</span><br><span class="line">        conv_height = int(conv_height)</span><br><span class="line">        <span class="comment"># 第一层权值 (64 * 20 * 8, 1024)</span></span><br><span class="line">        w_fc1 = self.weight_variable([<span class="number">64</span> * conv_width * conv_height, <span class="number">1024</span>])</span><br><span class="line">        <span class="comment"># 第一层bias (1, 1024)</span></span><br><span class="line">        b_fc1 = self.bias_variable([<span class="number">1024</span>])</span><br><span class="line">        <span class="comment"># 将CNN的输出结果展开成一条线，shape为(64 * 20 * 8, 1)</span></span><br><span class="line">        h_dropout3_flat = tf.reshape(h_dropout3, [<span class="number">-1</span>, <span class="number">64</span> * conv_width * conv_height])</span><br><span class="line">        <span class="comment"># 神经网络处理，激发函数选择relu (64 * 20 * 8, 1) --&gt; (1024, 1)</span></span><br><span class="line">        h_fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_dropout3_flat, w_fc1), b_fc1))</span><br><span class="line">        <span class="comment"># dropout处理</span></span><br><span class="line">        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># second fully layer</span></span><br><span class="line">        w_fc2 = self.weight_variable([<span class="number">1024</span>, self.char_num * self.classes])</span><br><span class="line">        b_fc2 = self.bias_variable([self.char_num * self.classes])</span><br><span class="line">        <span class="comment"># 神经网络处理, (1024, 1) --&gt; (4 * 62, 1)</span></span><br><span class="line">        y_conv = tf.add(tf.matmul(h_fc1_drop, w_fc2), b_fc2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_conv</span><br></pre></td></tr></table></figure></div><h3 id="predict-captcha-py"><a href="#predict-captcha-py" class="headerlink" title="predict_captcha.py"></a>predict_captcha.py</h3><p>我们把训练好的模型保存起来，之后可以使用此代码去识别任意的验证码，只要图片输入的要求符合高60宽160且1通道黑白图片即可。这个类上面我没讲解，我也没机会试，因为训练时间太久了我还没训出来呢，据说GPU训练需要4~5个小时，CPU训练的话需要20小时，大家训练好了可以拿去试试。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> generate_captcha</span><br><span class="line"><span class="keyword">import</span> captcha_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    captcha = generate_captcha.GenerateCaptcha()</span><br><span class="line">    width, height, char_num, characters, classes = captcha.get_parameter()</span><br><span class="line"></span><br><span class="line">    gray_image = Image.open(sys.argv[<span class="number">1</span>]).convert(<span class="string">'L'</span>)</span><br><span class="line">    img = np.array(gray_image.getdata())</span><br><span class="line">    test_x = np.reshape(img, [height, width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, height, width, <span class="number">1</span>])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">    model = captcha_model.CaptchaModel(width, height, char_num, classes)</span><br><span class="line">    y_conv = model.create_model(x,keep_prob)</span><br><span class="line">    predict = tf.argmax(tf.reshape(y_conv, [<span class="number">-1</span>,char_num, classes]),<span class="number">2</span>)</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.95</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">False</span>, gpu_options=gpu_options)) <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        saver.restore(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">        pre_list = sess.run(predict, feed_dict=&#123;x: [test_x], keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pre_list:</span><br><span class="line">            s = <span class="string">''</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">                s += characters[j]</span><br><span class="line">            print(s)</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lab </tag>
            
            <tag> dnn </tag>
            
            <tag> tensorflow </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

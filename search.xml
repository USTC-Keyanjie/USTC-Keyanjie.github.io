<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>分布式与云计算复习笔记(updating)</title>
      <link href="/studynote-of-distributed-system/"/>
      <url>/studynote-of-distributed-system/</url>
      
        <content type="html"><![CDATA[<p>分布式与云计算2019春季课程笔记</p><blockquote><p>这个东西吧，太简单了，我们这里不讲了</p><p>这个东西吧，比较难，后续我们在讲吧(后续后续。。。)</p><p>这里吧，(喝口水，ppt翻页)，直接跳过了</p><p>天文地理鲁迅balabala，石竹老师先退票再买票，最后没抢到票我感到非常高兴233333</p></blockquote><p>end</p><a id="more"></a><iframe frameborder="no" border="0" marginwidth="50" marginheight="0" width="250" height="86" src="//music.163.com/outchain/player?type=3&id=902907641&auto=1&height=66"></iframe><hr><p><strong>瞎划的重点：</strong></p><p><a href="#⚡️虚拟化">⚡️虚拟化</a></p><p><a href="#⚡️远程过程调用-Remote-Procedure-Call">⚡️远程过程调用 - Remote Procedure Call</a></p><p><a href="#⚡️面向流的通信-Stream-oriented-Communication">⚡️面向流的通信 - Stream-oriented Communication</a></p><p><a href="#⚡️⚡️DHT">⚡️⚡️DHT</a></p><p><a href="#⚡️⚡️HLS">⚡️⚡️HLS</a></p><p><a href="#⚡️⚡️逻辑时钟-Logical-clocks">⚡⚡️逻辑时钟 - Logical clocks</a></p><p><a href="#⚡️⚡️以数据为中心的一致性模型">⚡️⚡️以数据为中心的一致性模型</a></p><p><a href="#⚡️⚡️以客户为中心的一致性模型">⚡️⚡️以客户为中心的一致性模型</a></p><h1 id="Chapter-1-概述"><a href="#Chapter-1-概述" class="headerlink" title="Chapter 1 概述"></a>Chapter 1 概述</h1><h2 id="分布式系统的定义"><a href="#分布式系统的定义" class="headerlink" title="分布式系统的定义"></a>分布式系统的定义</h2><blockquote><p>分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关的系统。</p></blockquote><p>为了使种类各异的计算机和网络都呈现为单个的系统，分布式系统常常通过一个”软件层”组织起来，该”软件层”在逻辑上位于有用户和应用程序组成的高层与有操作系统组成的低层之间。这样的分布式系统又称为<strong>中间件(middleware)</strong>。</p><p><img src="/studynote-of-distributed-system/中间件.png" alt="中间件"></p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><h3 id="使资源可访问"><a href="#使资源可访问" class="headerlink" title="使资源可访问"></a>使资源可访问</h3><p>分布式系统最<strong>主要</strong>的目标是使用户能够方便地访问远程资源，并且以一种受控的方式与其他用户共享这些资源。</p><h3 id="透明性"><a href="#透明性" class="headerlink" title="透明性"></a>透明性</h3><p>分布式系统的<strong>重要</strong>目标之一是将它的进程和资源实际上在多台计算机上分布这样一个事实隐藏起来。如果一个分布式系统在用户和应用程序面前呈现为单个计算机系统，这样的分布式系统就称为是<strong>透明的</strong>。</p><h4 id="透明的类型"><a href="#透明的类型" class="headerlink" title="透明的类型"></a>透明的类型</h4><ul><li>访问透明性：对不同数据表示形式以及资源访问方式的隐藏；</li><li>位置透明性：用户无法判别资源在系统中的物理位置；</li><li>迁移透明性：如果分布式系统中的资源移动不会影响该资源的访问方式，就可以说这种分布式系统能提供；</li><li>重定位透明性：如果资源可以在接受访问的同时进行重新定位，而不引起用户和应用程序的注意，拥有这种资源的系统能提供重定位透明性。</li><li>复制透明性：对同一个资源存在多个副本这样一个事实的隐藏；</li><li>并发透明性：隐藏资源是否由若干相互竞争的用户共享这一事实；</li><li>故障透明性：隐藏资源的故障与恢复。</li></ul><h4 id="透明度"><a href="#透明度" class="headerlink" title="透明度"></a>透明度</h4><p>在设计并实现分布式系统时，把实现分布的透明性作为目标是正确的，但是应该将它和其他方面的问题（比如性能)结合起来考虑。</p><h3 id="开放性"><a href="#开放性" class="headerlink" title="开放性"></a>开放性</h3><p>开放式的分布式系统：根据一系列准则来提供服务，这些准则描述了所提供服务的语法和语义。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>系统的可扩展性至少可以通过三个方面来度量：</p><ol><li>规模可扩展：可方便向系统里加入更多用户和资源；</li><li>地域可扩展：可使系统中的用户与资源相隔十分遥远；</li><li>管理可扩展：即使分布式系统跨越多个独立的管理机构，仍然可以方便的对其进行管理。</li></ol><h4 id="可扩展性问题"><a href="#可扩展性问题" class="headerlink" title="可扩展性问题"></a>可扩展性问题</h4><p>分布式算法：</p><ol><li>没有任何计算机拥有整个系统的全局信息；</li><li>计算机只根据本地信息做出决策；</li><li>某台计算机的故障不会使算法崩溃；</li><li>不存在<strong>全局时钟</strong>。</li></ol><h4 id="扩展技术"><a href="#扩展技术" class="headerlink" title="扩展技术"></a>扩展技术</h4><h5 id="隐藏通信等待时间"><a href="#隐藏通信等待时间" class="headerlink" title="隐藏通信等待时间"></a>隐藏通信等待时间</h5><ol><li>使用<strong>异步通信</strong>，通常用于批处理系统；</li><li>启动一个新的控制线程来执行请求。</li></ol><p><strong>问题</strong>：有许多应用程序不适用此方式。</p><h5 id="分布技术"><a href="#分布技术" class="headerlink" title="分布技术"></a>分布技术</h5><p>把某个组件分割成多个部分，然后再把他们分散到系统中去。</p><p>例：DNS、万维网WWW</p><h5 id="复制技术"><a href="#复制技术" class="headerlink" title="复制技术"></a>复制技术</h5><p>对组件进行复制并将副本分部到系统各处。</p><p><strong>缓存</strong>是复制的一种特殊形式。缓存一般是在访问资源的客户<strong>附近</strong>制作该资源的副本。</p><h2 id="分布式系统的类型"><a href="#分布式系统的类型" class="headerlink" title="分布式系统的类型"></a>分布式系统的类型</h2><h3 id="分布式计算系统"><a href="#分布式计算系统" class="headerlink" title="分布式计算系统"></a>分布式计算系统</h3><blockquote><p>用于高性能计算任务的系统。</p></blockquote><h4 id="集群计算系统"><a href="#集群计算系统" class="headerlink" title="集群计算系统"></a>集群计算系统</h4><p>底层硬件是由类似的工作站或PC集组成，通过高速的局域网紧密连接起来的。而且，每个节点运行的是相同的操作系统。</p><p>集群计算系统的特点是同构性，大多数情况下，集群中的计算机都是相同的：</p><ul><li>有相同的操作系统</li><li>通过同一网络连接</li></ul><p><img src="/studynote-of-distributed-system/集群计算系统一个示例.png" alt="集群计算系统一个示例"></p><h4 id="网格计算系统"><a href="#网格计算系统" class="headerlink" title="网格计算系统"></a>网格计算系统</h4><p>组成分布式系统的子分组通常构建成一个计算机系统联盟，每个系统归属于不同的管理域。</p><p>网格计算系统具有高度异构性：</p><ul><li>硬件</li><li>操作系统</li><li>网络</li><li>管理域</li><li>安全策略</li></ul><p>都不尽相同。</p><p>以<strong>虚拟组织</strong>的方式，把来自不同计算机组织的资源集中起来，是一组人或机构协调工作。属于同一虚拟组织的人，具有访问提供给该组织的资源的权限。</p><p><img src="/studynote-of-distributed-system/网格计算系统的分层体系结构.png" alt="网格计算系统的分层体系结构"></p><p><strong>光纤层</strong>：在特定站点提供对局部资源的接口。这些接口都进行了定制，以允许在某个虚拟组织中实现资源共享。</p><p><strong>连接层</strong>：由通信协议组成，用于支持网格事务处理，延伸多个资源的使用。例如，用于在资源之间传输数据或从远程地点访问资源的协议。另外，连接层还有安全协议，用于进行用户和资源的认证。</p><p><strong>资源层</strong>：负责管理单个资源。它使用由连接层提供的功能，直接调用对光线层可用的接口。</p><p><strong>汇集层</strong>：负责处理对多个资源的访问，通常由资源分派、把任务分配和调度到多资源以及数据复制等服务组成。连接层和资源层由相对较小、较标准的协议集组成，而汇集层由很多用于不同目的的不同协议组成。</p><p><strong>应用层</strong>：由应用程序组成。</p><h3 id="分布式信息系统"><a href="#分布式信息系统" class="headerlink" title="分布式信息系统"></a>分布式信息系统</h3><h3 id="分布式普适系统"><a href="#分布式普适系统" class="headerlink" title="分布式普适系统"></a>分布式普适系统</h3><hr><h1 id="Chapter-2-体系结构"><a href="#Chapter-2-体系结构" class="headerlink" title="Chapter 2 体系结构"></a>Chapter 2 体系结构</h1><h2 id="体系结构的样式"><a href="#体系结构的样式" class="headerlink" title="体系结构的样式"></a>体系结构的样式</h2><p>根据组件、组件之间相互的连接方式、组件之间的数据交换以及这些元素如何集成到一个系统中来定义。</p><ul><li><strong>组件Component</strong>：一个模块单元，可以提供良好定义接口，在其环境中是可替换的。</li><li><strong>链接器Connector</strong>：在组件之间传递通信、使组件相互协调和协作。</li></ul><p>根据组件和连接器的使用，划分成<strong>不同体系结构</strong>：</p><ol><li><p>分层体系结构</p><p><img src="/studynote-of-distributed-system/分层体系结构样式.png" alt="分层体系结构样式"></p><p>组件组成了不同的层，其中$L_i$层中的组员可以调用下面的层$L_{i-1}$。</p></li><li><p>基于对象的体系结构</p><p><img src="/studynote-of-distributed-system/对象体系结构样式.png" alt="对象体系结构样式"></p><p>每个对象都对应一个组件，这些组件是通过(远程)过程调用机制来连接的。</p></li><li><p>以数据为中心的体系结构</p><p>通过一个公用(被动或主动的)仓库进行通信。</p></li><li><p>基于事件的体系结构</p><p><img src="/studynote-of-distributed-system/基于事件的体系结构样式.png" alt="基于事件的体系结构样式"></p><p><img src="/studynote-of-distributed-system/基于共享数据空间的体系结构样式.png" alt="基于共享数据空间的体系结构样式"></p><p>通过事件的传播来通信。</p></li></ol><h2 id="系统体系结构"><a href="#系统体系结构" class="headerlink" title="系统体系结构"></a>系统体系结构</h2><h3 id="集中式体系结构"><a href="#集中式体系结构" class="headerlink" title="集中式体系结构"></a>集中式体系结构</h3><p><strong>客户-服务器</strong>的交互方式，又被称为<strong>请求-回复</strong>行为。</p><p><strong>幂等操作</strong>：如果某个操作可以重复多次而无害处，那么称它是<strong>幂等</strong>的。</p><h4 id="应用分层"><a href="#应用分层" class="headerlink" title="应用分层"></a>应用分层</h4><p><strong>客户-服务器</strong>模型分为三层：</p><ol><li>用户接口层，用于与用户交互；</li><li>处理层，包含应用程序；</li><li>数据层，管理要使用的实际数据。</li></ol><h4 id="多层体系结构"><a href="#多层体系结构" class="headerlink" title="多层体系结构"></a>多层体系结构</h4><p><img src="/studynote-of-distributed-system/各种客户-服务器组织结构.png" alt="各种客户-服务器组织结构"></p><h3 id="非集中式体系结构"><a href="#非集中式体系结构" class="headerlink" title="非集中式体系结构"></a>非集中式体系结构</h3><h4 id="结构化的点对点体系结构"><a href="#结构化的点对点体系结构" class="headerlink" title="结构化的点对点体系结构"></a>结构化的点对点体系结构</h4><p><strong>点对点</strong>体系结构：在结构化的点对点体系机构中，覆盖网络是一个确定性的过程来构成的。这个使用最多的进程是通过一个<strong>分布式哈希表</strong>来组织进程的。</p><p><img src="/studynote-of-distributed-system/Chord系统中，从数据项到结点的映射.png" alt="Chord系统中，从数据项到结点的映射"></p><h4 id="非结构化的点对点体系结构"><a href="#非结构化的点对点体系结构" class="headerlink" title="非结构化的点对点体系结构"></a>非结构化的点对点体系结构</h4><p>很多非结构化的点对点系统的一个目标就是构造一个类似于<strong>随机图</strong>的覆盖网络。</p><h4 id="超级对等体"><a href="#超级对等体" class="headerlink" title="超级对等体"></a>超级对等体</h4><p><img src="/studynote-of-distributed-system/超级对等体网络中结点的分层组织结构.png" alt="超级对等体网络中结点的分层组织结构"></p><p>能维护一个索引或者充当一个代理程序的结点称为<strong>超级对等体</strong>，控制管理多个常规对等体。每个常规对等体作为一个客户连接到超级对等体。</p><h3 id="混合体系结构"><a href="#混合体系结构" class="headerlink" title="混合体系结构"></a>混合体系结构</h3><h4 id="边界服务器系统"><a href="#边界服务器系统" class="headerlink" title="边界服务器系统"></a>边界服务器系统</h4><p>这种系统部署在因特网中，服务器放置在网络的“边界”。这种边界是由企业网络和实际的因特网之间的分界线形成的。例如，因特网服务提供商。家庭终端用户通过ISP连接到因特网，该ISP就可以认为是因特网的边界。</p><p><img src="/studynote-of-distributed-system/把因特网看做是由一系列便捷服务器组成的.png" alt="把因特网看做是由一系列便捷服务器组成的"></p><h4 id="协作分布式系统"><a href="#协作分布式系统" class="headerlink" title="协作分布式系统"></a>协作分布式系统</h4><p><strong>混合结构</strong>主要部署在协作式分布式系统中。在很多重要的系统中，主要问题是先启动起来，因为经常会部署一个传递的客户-服务器结构。一旦某个结点要加入系统，就可以使用完全非集中式的组织结构，用于协作。</p><p><strong>BitTorrent</strong>是一个点对点的文件下载系统。基本思想是，当一个终端用户要查找某个文件时，他可以从其他用户那里下载文件块，直到所下载的文件块能够组装成完整的文件为止。</p><p><img src="/studynote-of-distributed-system/BitTorrent的工作原理.png" alt></p><h2 id="分布式系统的自我管理"><a href="#分布式系统的自我管理" class="headerlink" title="分布式系统的自我管理"></a>分布式系统的自我管理</h2><p><strong>自治计算</strong>：以高级反馈控制系统的形式来组织分布式系统，允许自动自适应变换。也叫做<strong>自主</strong>系统。</p><p>自适应的多样性：</p><ul><li>自我管理</li><li>自我恢复</li><li>自我配置</li><li>自我优化</li><li>等</li></ul><h3 id="反馈控制模型"><a href="#反馈控制模型" class="headerlink" title="反馈控制模型"></a>反馈控制模型</h3><p><img src="/studynote-of-distributed-system/反馈控制系统的逻辑组织结构.png" alt></p><p>反馈控制系统的核心由需要管理的组件形成。这些组件能通过可控输入参数驱动，受干扰或噪声输入的影响。</p><p>系统本身需要被监视，因此需要对系统各个方面进行测量。但是实际测量很难做到，就需要一个逻辑<strong>尺度预测组件</strong>。</p><p>控制循环的核心部分是反馈分析组件，分析上述测量值，并把它们与参考值进行比较。包含了决定自适应的各种算法。</p><hr><h1 id="Chapter-3-进程"><a href="#Chapter-3-进程" class="headerlink" title="Chapter 3 进程"></a>Chapter 3 进程</h1><h2 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h2><p><strong>为什么</strong>要使用线程？</p><p>虽然进程构成了分布式系统中的基本组成单元，但是实践表明，操作系统提供的用于构建分布式系统的进程在粒度上还是太大了。而就粒度而言，将每个进程细分为若干控制线程的形式则更加合适，可以使构建分布式应用程序变得更加方便，获得更好的性能。</p><p>线程与进程<strong>不同</strong>之处？</p><ul><li>线程不会以性能降低为代价，换取高度的并发透明性。</li><li>线程上下文一般只包含存储在寄存器和存储器中的尽可能少的信息，仅仅用于执行一系列指令。</li></ul><h3 id="分布式系统中的线程"><a href="#分布式系统中的线程" class="headerlink" title="分布式系统中的线程"></a>分布式系统中的线程</h3><h4 id="多线程客户端"><a href="#多线程客户端" class="headerlink" title="多线程客户端"></a>多线程客户端</h4><p>多线程客户端最大的问题是如何隐藏网络延时。常规方法是启动网络通信线程后，立即进行其他工作。</p><h4 id="多线程服务器"><a href="#多线程服务器" class="headerlink" title="多线程服务器"></a>多线程服务器</h4><p>在服务器上使用多线程技术有很多好处：</p><ol><li><p>可以提升性能</p><ul><li>启动一个线程处理请求比启动进程开销更小</li><li>可以隐藏网络延时。当有请求到来时，也可以做其他工作。</li></ul></li><li><p>结构更优</p><p>多线程编程会更简单。</p><p><img src="/studynote-of-distributed-system/以分发器 工作者模型组织起来的多线程服务器.png" alt="以分发器/工作者模型组织起来的多线程服务器"></p><p>上图中，<strong>分发器线程</strong>读取文件操作请求，随后选择一个空闲的<strong>工作者线程</strong>来处理请求。</p></li></ol><h2 id="⚡️虚拟化"><a href="#⚡️虚拟化" class="headerlink" title="⚡️虚拟化"></a>⚡️虚拟化</h2><blockquote><p>虚拟化：把一样东西看成另一个东西</p></blockquote><ul><li>多虚一，多个资源虚拟整合成一个资源；</li><li>一虚多，把一个资源虚拟成多个，供多用户使用，可弥补运营成本；</li><li>做隔离，底层和上层之间不可见，<strong>便于移植</strong>；早期用于程序迁移</li></ul><h3 id="虚拟机体系结构"><a href="#虚拟机体系结构" class="headerlink" title="虚拟机体系结构"></a>虚拟机体系结构</h3><p>计算机系统通常在4个不同层次提供4个不同的界面：</p><ol><li><p>由<strong>机器指令</strong>组成，任何程序都可激起的，介于硬件软件之间的界面；</p><p>Interface between hardware and software, non-privileged.</p></li><li><p>由<strong>机器指令</strong>组成，只有特权程序(e.g 操作系统)才可激起的界面；</p><p>Interface between hardware and software, privileged.</p></li><li><p>有操作系统提供的<strong>系统调用</strong>组成的界面；</p><p>System calls.</p></li><li><p>由库调用组成的界面，通常形成了所谓的<strong>应用程序编程接口(API)</strong>。</p><p>Libraries functions.</p></li></ol><p><strong>而虚拟化的实质就是模仿这些界面的行为。</strong></p><ul><li>指令集虚拟化的代价很高；</li><li>硬件虚拟化，性能损失不大，是虚拟化的主流。</li></ul><p><img src="/studynote-of-distributed-system/4个不同层次的界面.png" alt="4个不同层次的界面"></p><h3 id="虚拟机的两种方式"><a href="#虚拟机的两种方式" class="headerlink" title="虚拟机的两种方式"></a>虚拟机的两种方式</h3><ol><li><p>构建一个运行时（runtime）系统，实质上提供了一套抽象指令集来执行程序。例如Java虚拟机，将指令进行翻译，使用库虚拟化；还有使用Windows操作系统模拟Linux，是OS虚拟化。</p><p><img src="/studynote-of-distributed-system/进程虚拟机.png" alt></p></li><li><p>使用<strong>虚拟机监视器（VMM）</strong>这一层，虚拟机监视器完全屏蔽硬件，但提供一个同样指令集（或其他硬件）的界面。这个界面可以同时提供给多个程序，可以有多个不同的操作系统独立并发地运行在同一平台。</p><p><img src="/studynote-of-distributed-system/虚拟机监视器.png" alt="虚拟机监视器"></p></li></ol><h3 id="虚拟化-v-s-容器"><a href="#虚拟化-v-s-容器" class="headerlink" title="虚拟化 v.s. 容器"></a>虚拟化 v.s. 容器</h3><h4 id="虚拟化：OpenStack"><a href="#虚拟化：OpenStack" class="headerlink" title="虚拟化：OpenStack"></a>虚拟化：OpenStack</h4><p>使用OpenStack做虚拟化时，它可以将操作系统都一并打包好，各个虚拟机之间完全隔离，互不干涉。</p><p>层次结构：</p><ul><li>最顶层应用</li><li>操作系统(任意的操作系统)</li><li>OpenStack</li><li>底层硬件</li></ul><h4 id="容器：Docker"><a href="#容器：Docker" class="headerlink" title="容器：Docker"></a>容器：Docker</h4><p>Docker是基于<strong>进程容器(Process container)</strong>的轻量级VM解决方案，用比虚拟机技术少很多的资源消耗实现了类似于虚拟机的对CPU、磁盘、网络的隔离。</p><p>层次结构：</p><ul><li>最顶层应用</li><li>Docker</li><li>操作系统(仅限Linux)</li><li>底层硬件</li></ul><h4 id="OpenStack与Docker的区别"><a href="#OpenStack与Docker的区别" class="headerlink" title="OpenStack与Docker的区别"></a>OpenStack与Docker的区别</h4><p><img src="/studynote-of-distributed-system/Docker与Openstark.png" alt></p><h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><h3 id="客户端软件与分布透明性"><a href="#客户端软件与分布透明性" class="headerlink" title="客户端软件与分布透明性"></a>客户端软件与分布透明性</h3><p>一个带有副本服务器的分布式系统，可以通过将调用请求转发给每一个服务器的副本来达到复制透明性。客户代理将会透明地偶素即所有对象的响应，并且只向客户应用程序送回一个返回值。</p><p><img src="/studynote-of-distributed-system/复制透明性.png" alt></p><h2 id="服务器"><a href="#服务器" class="headerlink" title="服务器"></a>服务器</h2><ul><li><p><strong>迭代服务器</strong></p><p>自己处理请求，并且在必要的情况下将响应返回给发出请求的客户。</p><p>一次只能处理<strong>一个</strong>客户端请求，而并发服务器不同。</p></li><li><p><strong>并发服务器</strong></p><p>并不自己处理请求，而是将请求传递给某个独立线程或者其他进程来处理，自身立即返回并等待下一个输入的请求。</p></li><li><p><strong>超级服务器</strong></p><p>负责监听所有与这些服务关联的端点，当收到请求的时候，它派生出一个进程已对该请求进行进一步处理，这个派生出的进程在处理完毕后将会自动退出运行。</p><p><img src="/studynote-of-distributed-system/使用超级服务器来进行客户-服务器绑定.png" alt></p></li><li><p><strong>状态无关服务器</strong></p><p>不保存器客户的状态信息，而且也不将自身的状态变化告知任何客户。</p><ul><li>不记录文件是否已经打开(只在访问后再次关闭文件)；</li><li>不一定会使得客户的缓存变无效；</li><li>不跟踪客户行为。</li></ul><p>后果：</p><ul><li>客户端与服务器端完全独立；</li><li>非<strong>无状态服务器</strong>的情况下，由于客户或者服务器端某一方发生崩溃，从而导致双方状态不一致。而在无状态服务器中，因为是无状态的，这种状态不一致的情况大大减小了；</li><li>因为某些原因性能可能会有所降低，例如服务器无法预测客户的行为(比如预取文件块)。</li></ul></li><li><p>状态相关服务器</p><p>跟踪客户的状态，一直保留客户端的信息直到被显式删除，包括：</p><ul><li>记录文件已被打开，以便可以进行预取；</li><li>知道客户端缓存了哪些数据，并允许客户端保留共享数据的本地副本</li></ul><p>如果允许客户端保留本地副本，则可以提升读写操作的性能。</p><p>缺陷：</p><ul><li>如果服务器崩溃，那么必须将自身的整个状态恢复到崩溃之前。但是如果采用状态无关设计，就不需要采取任何特殊措施来使崩溃的服务器恢复。</li></ul></li><li><p>Cookies</p><p>Cookies是一小段数据，其中包含有对服务器有用的针对特定客户的信息。浏览器本身永远不会执行cookie，它只对cookie进行存储。</p><p>Cookies可用于将当前客户端操作与先前操作相关联，也可用于状态的存储。</p></li></ul><h3 id="服务器集群"><a href="#服务器集群" class="headerlink" title="服务器集群"></a>服务器集群</h3><h4 id="常见的组织"><a href="#常见的组织" class="headerlink" title="常见的组织"></a>常见的组织</h4><p><img src="/studynote-of-distributed-system/三层服务器集群通常的结构.png" alt></p><p>服务器集群逻辑上由三层组成。</p><ul><li>第一层：（逻辑上的）交换机，由它分配客户请求给服务器；</li><li>第二层：应用计算服务器，是专用于提供计算能力的服务器；</li><li>第三层：文件和数据库服务器，是分布式文件系统或分布式数据库系统。</li></ul><p>交换机形成了集群入口，提供了唯一的网络地址。</p><p>一种标准的存取服务器集群的方式是建立一个TCP连接，在这之上应用级别的请求可作为一个会话的一部分来发送，撤除连接可结束会话。在传输层交换机的情况下，交换机接受到来的TCP连接请求，转发这些请求给一台服务器。</p><p><img src="/studynote-of-distributed-system/TCP转发的原理.png" alt></p><p>当交换机收到一个TCP连接请求时，他就找到处理这个请求的最佳服务器，并转发这个请求包给这个服务器。服务器反过来会发送一个应答给请求的客户，但把交换机的IP地址插入到承载TCP数据段的IP包头的原地址域。(看上去是服务器欺骗客户自己是交换机，但是客户等待的是交换机的应答，而不是某个不知名的服务器，所以这种欺骗是必须的。从而可以看出，实现TCP转发需要操作系统级别的修改)</p><h4 id="分布式服务器"><a href="#分布式服务器" class="headerlink" title="分布式服务器"></a>分布式服务器</h4><blockquote><p>分布式服务器指可动态变化的一个机器群，它的访问点也可以变化，但对外却表现为一强有力的单台机器。</p></blockquote><p>如何在这样的系统中实现一个稳定访问点？</p><p>主要思想是利用可用的网络服务，比如IP版本6的移动支持(MIPv6)。在MIPv6中，一个移动结点假设有一个<strong>宿主网络</strong>，通常待在这个网络中，并有稳定的地址，称为<strong>宿主地址</strong>。</p><p>宿主网络有一个特别的路由器，称为<strong>宿主代理</strong>。当移动节点附着到一个外部网络时，它会收到一临时<strong>需要的地址</strong>，其他结点可以发送网络包给这个地址。这个临时地址通告给宿主代理，它随后就可以转发送给移动节点的网络包到这个临时地址。</p><p>在分布式服务器的情况下，给集群分配一个唯一的联系地址（服务器生命周期内和外界通信的地址），任何时候分布式服务器都有一节点作为联系地址的访问点，这个角色可以容易被另一结点取代。访问点在分布式服务器所在网络的宿主代理注册自己的地址为临时地址。这是所有的网络包都会导向访问点，他然后分配请求给当前参与分布式服务器的结点。若访问点失效，一个简单容错机制启动，另一个访问点会选出并注册一个新的临时地址。</p><p>这个配置会使得宿主代理和访问点称为瓶颈，因为这样所有的流量都要流经这两台机器，可以使用MIPv6的<strong>路径优化</strong>（见下图）：</p><ol><li>客户知道服务器的地址是HA，把请求传给HA；</li><li>服务器的宿主代理把请求转发给当前临时地址CA；</li><li>宿主代理转发CA给客户；</li><li>客户把(HA,CA)存储在本地，之后通信直接送给CA。</li></ol><p><img src="/studynote-of-distributed-system/分布式服务器路径优化.png" alt></p><h2 id="代码迁移"><a href="#代码迁移" class="headerlink" title="代码迁移"></a>代码迁移</h2><p>代码迁移会带来与本地资源使用相关的问题，因为迁移时要求资源同时迁移，并且在目标机器上重新绑定到本地资源，或者使用系统范围的网络引用。</p><p>另一个问题是，在迁移代码时要考虑异构性。可以使用虚拟机来处理异构性。</p><h3 id="代码迁移模型"><a href="#代码迁移模型" class="headerlink" title="代码迁移模型"></a>代码迁移模型</h3><p>进程包含如下三段：</p><ul><li>代码段，包含构成正在运行的程序的所有指令；</li><li>资源段，包含指向进程需要的外部资源（文件、打印机、设备等）的指针；</li><li>执行段，用来存储进程的当期那执行状态量（包括私有数据、栈、程序计数器等）。</li></ul><p><strong>弱可移动性</strong>：</p><p>只传输代码段以及某些初始化数据。传输过来的程序总是从预先定义的几个位置之一开始执行。</p><p><strong>强可移动性</strong>：</p><p>可以先停止运行中的进程，然后将它移到另一台机器上去，再从刚才中断的位置继续执行。</p><p><img src="/studynote-of-distributed-system/代码迁移的各种不同方法.png" alt></p><hr><h1 id="Chapter-4-通信-Communication"><a href="#Chapter-4-通信-Communication" class="headerlink" title="Chapter 4 通信 - Communication"></a>Chapter 4 通信 - Communication</h1><p><strong>进程间通信 - Interprocess Communication</strong></p><blockquote><p>进程间通信是每个分布式系统的核心。</p></blockquote><p>但是，不同机器的进程想相互交流是很困难的。 😭</p><p><strong>解决方案：</strong></p><p>目前主要有四种方法用于进程间通信 - IPC models:</p><ul><li>RPC</li><li>RMI</li><li>MOM</li><li>Streams</li></ul><h2 id="分层协议-Layered-Protocols"><a href="#分层协议-Layered-Protocols" class="headerlink" title="分层协议 - Layered Protocols"></a>分层协议 - Layered Protocols</h2><p>分层协议可以解决通信的问题</p><h3 id="两大类协议"><a href="#两大类协议" class="headerlink" title="两大类协议"></a>两大类协议</h3><ul><li><p><strong>面向连接</strong></p><p>消息发送与接收双方需要先建立连接，再传输数据；</p></li><li><p><strong>面向无连接</strong></p><p>直接传数据</p></li></ul><h3 id="中间件协议"><a href="#中间件协议" class="headerlink" title="中间件协议"></a>中间件协议</h3><p>中间件是一种应用程序，逻辑上位于应用层中。</p><ul><li>中间件通信协议支持高层通信服务；</li><li>对实时数据传输进行设定并使其保持同步的协议；</li><li>还提供可靠的多播服务。</li></ul><h2 id="⚡️远程过程调用-Remote-Procedure-Call"><a href="#⚡️远程过程调用-Remote-Procedure-Call" class="headerlink" title="⚡️远程过程调用 - Remote Procedure Call"></a>⚡️远程过程调用 - Remote Procedure Call</h2><p>RPC就是一个机器调用位于其他机器上的进程。</p><p>A机器的进程调用B机器的进程：</p><ol><li>A将本地调用进程挂起；</li><li>A通过参数将信息传递给B；</li><li>B执行被调用进程；</li><li>B执行结束将信息传回给A。</li></ol><p>一般说的RPC就是同步RPC，如上面的流程所示，机器A的调用进程：</p><ul><li>无需保护现场；</li><li>在结果返回之前会阻塞的。</li></ul><h3 id="基本的RPC操作-Basic-RPC-Operation"><a href="#基本的RPC操作-Basic-RPC-Operation" class="headerlink" title="基本的RPC操作 - Basic RPC Operation"></a>基本的RPC操作 - Basic RPC Operation</h3><p>RPC操作背后隐含的思想是尽量是<strong>远程过程调用具有与本地调用相同的形式</strong>。</p><p>将客户过程对客户存根发出的本地调用转换成对服务器过程的本地调用，而客户和服务器都不会意识到有中间步骤的存在。</p><p><img src="/studynote-of-distributed-system/客户与服务器之间的RPC原理.png" alt></p><p>使用socket进行通信，把socket通信隐藏到底层，从而实现透明性。</p><p><strong>RPC的步骤</strong>：</p><p><strong>总结一下就是</strong>：</p><ol><li><p>发送：</p><p>客户过程 -&gt; 客户存根 -&gt; 客户操作系统 -&gt; 服务器操作系统 -&gt; 服务器存根 -&gt; 服务器进程</p></li><li><p>返回：从右向左就是了。</p></li></ol><p><strong>具体</strong>：</p><ol><li>客户过程以正常的方式调用客户存根；</li><li>客户存根生成一个消息，然后调用本地操作系统；</li><li>客户端操作系统将消息发送给远程操作系统；</li><li>远程操作系统将消息交给服务器存根；</li><li>服务器存根将参数提取出来，然后调用服务器；</li><li>服务器执行要求的操作，操作完成后将结果返回给服务器存根；</li><li>服务器存根将结果打包成一个消息，然后调用本地操作系统；</li><li>服务器操作系统将含有结果的消息发送回客户端操作系统；</li><li>客户端操作系统将消息交给客户存根；</li><li>客户存根将结果从消息中提取出来，返回给调用他的客户过程。</li></ol><p><img src="/studynote-of-distributed-system/通过RPC进行远程计算.png" alt></p><h3 id="参数传递-Parameter-Passing"><a href="#参数传递-Parameter-Passing" class="headerlink" title="参数传递 - Parameter Passing"></a>参数传递 - Parameter Passing</h3><p>完整定义了RPC协议后，需要实现客户存根和服务器存根，不同点仅仅在于面向应用程序的接口。</p><p>接口通常使用<strong>IDL</strong>(接口定义语言)，用IDL说明的接口可以与适当的编译时接口或者运行时接口一起编译到客户存根过程和服务器存根中。</p><h3 id="异步RPC-Variations"><a href="#异步RPC-Variations" class="headerlink" title="异步RPC - Variations"></a>异步RPC - Variations</h3><p><strong>异步RPC中：</strong></p><ul><li>客户发出RPC请求，接受到服务器的确认信息后，不会阻塞，继续向下执行；</li><li>服务器在接受到RPC请求后立即向客户送回应答，之后再调用客户请求的过程。</li></ul><p><img src="/studynote-of-distributed-system/传统RPC和异步RPC.png" alt></p><p>异步RPC效率高，但是难编程。</p><p><strong>延迟的同步RPC</strong>：</p><p>就是两个异步RPC结合起来。</p><p>远程服务器在处理请求时，客户同时做一些其他的事情。当服务器处理好请求时，返回结果去中断客户，服务器一端变成发送端，实现第二个异步RPC。</p><p><img src="/studynote-of-distributed-system/延迟的同步RPC.png" alt></p><h2 id="面向消息的通信-Message-oriented-Communication"><a href="#面向消息的通信-Message-oriented-Communication" class="headerlink" title="面向消息的通信 - Message-oriented Communication"></a>面向消息的通信 - Message-oriented Communication</h2><h3 id="套接字"><a href="#套接字" class="headerlink" title="套接字"></a>套接字</h3><p>一种通信端点。如果应用程序要通过底层网络发送某些数据，可以把这些数据写入套接字，然后从套接字读出数据。</p><p><img src="/studynote-of-distributed-system/TCP IP.png" alt="pastedGraphic.png"></p><p>服务器一般执行前4个原语，一般按照图中顺序执行。调用套接字原语的时候，调用者创建一个新的通信端点，用于某种特定的传输协议的。</p><p><img src="/studynote-of-distributed-system/使用套接字的面向连接通信模式.png" alt></p><h3 id="消息传递接口-MPI"><a href="#消息传递接口-MPI" class="headerlink" title="消息传递接口 MPI"></a>消息传递接口 MPI</h3><p><strong>MPI</strong>的先进之处：</p><p>程序的硬件独立性需要导致MPI的出台。MPI是为并行应用程序设计的，是为瞬时通信量身定做的。它直接使用的是底层网络。</p><p>MPI是并行计算，一般运行于集群中。</p><p><img src="/studynote-of-distributed-system/MPI源语.png" alt></p><h3 id="消息队列系统"><a href="#消息队列系统" class="headerlink" title="消息队列系统"></a>消息队列系统</h3><p>面向消息的中间件服务。为持久异步通信提供多种支持。<strong>本质</strong>是，提供消息的中介存储能力，这样就不需要消息发送方和接收方在消息传输中都保持激活状态。与套接字和MPI的重要<strong>区别</strong>在于，它的设计目标一般是支持那些时间要求较为宽松的消息传输，不适合几秒甚至几微秒内要完成的传输。</p><h2 id="⚡️面向流的通信-Stream-oriented-Communication"><a href="#⚡️面向流的通信-Stream-oriented-Communication" class="headerlink" title="⚡️面向流的通信 - Stream-oriented Communication"></a>⚡️面向流的通信 - Stream-oriented Communication</h2><h3 id="流的定义"><a href="#流的定义" class="headerlink" title="流的定义"></a>流的定义</h3><blockquote><p>A (continuous) data stream is a connection oriented communication facility that supports isochronous data transmission</p></blockquote><ul><li><p><strong>简单流</strong>：</p><p>只包含单个数据序列；</p></li><li><p><strong>复杂流</strong>：</p><p>简单流的组合</p></li></ul><h3 id="传输模式"><a href="#传输模式" class="headerlink" title="传输模式"></a>传输模式</h3><ul><li><p><strong>异步传输模式</strong>：</p><p>流中的数据项是逐个传输的，但是对某一项在何时进行传输并没有进一步的限制。这是采用离散数据流时常见的情况。比如文件的传输。</p></li><li><p><strong>同步传输模式</strong>：</p><p>数据流中每一个单元都定义了一个端到端的最大延迟时间，容许延迟。数据单元的传输时间是否远远小于最大允许延迟并不重要。</p></li><li><p><strong>等时传输模式</strong>：</p><p>数据单元必须按时传输，端到端的延迟时间同时又上限和下限。这个上、下限也称为边界延迟抖动。在视频和音频方面很常用，比如音视频的同步(对口型)。</p><p>流媒体就属于这个类，既有最大端到端延迟限制，又有最小的端到端延迟限制。</p></li></ul><h3 id="流与服务质量"><a href="#流与服务质量" class="headerlink" title="流与服务质量"></a>流与服务质量</h3><h4 id="QoS服务质量："><a href="#QoS服务质量：" class="headerlink" title="QoS服务质量："></a>QoS服务质量：</h4><ol><li>数据传输所要求的比特率；</li><li>创建会话的最大延时(比如应用程序何时可以开始发送数据)；</li><li>端到端的最大延时；</li><li>最大延时抖动；</li><li>最大往返延时。</li></ol><h4 id="QoS保证："><a href="#QoS保证：" class="headerlink" title="QoS保证："></a>QoS保证：</h4><ul><li><p>使用 buffer 来减少 jitter</p><p>当数据包有不同的延时，接收方先把它们存储在缓冲区，当总是有足够的数据包进入缓冲区时，接收方就可以以固定的速率把数据包传递给应用程序。</p><p><img src="/studynote-of-distributed-system/使用缓冲区减少抖动.png" alt></p></li><li><p>针对丢包情况，interleaving，若没有交错机制，丢包时会把连续的几帧一起丢掉，视觉影响较大；而使用交错机制，会交叉丢包。</p><p>如在传送音视频时，采用交错传输，丢失的帧分布较广，这样丢失的就不是一大段而是零散的帧，对于音视频的播放影响就较小。但是这样需要更大的缓冲区，因此程序的开始延时更高。</p><p><img src="/studynote-of-distributed-system/交错传输.png" alt></p></li></ul><h3 id="流同步"><a href="#流同步" class="headerlink" title="流同步"></a>流同步</h3><p><strong>音视频流同步</strong></p><p>采用音视频的编解码协议：<strong>H264协议</strong></p><h2 id="多播通信-Communication"><a href="#多播通信-Communication" class="headerlink" title="多播通信 -  Communication"></a>多播通信 -  Communication</h2><h3 id="Gossip-Based-Data-Dissemination"><a href="#Gossip-Based-Data-Dissemination" class="headerlink" title="Gossip-Based Data Dissemination"></a>Gossip-Based Data Dissemination</h3><p>属于p2p系统的多播。</p><p>应用在自主网或是p2p，就是没有服务器的那一类网。</p><h3 id="几种多播"><a href="#几种多播" class="headerlink" title="几种多播"></a>几种多播</h3><ul><li><p>全序多播 Total order</p></li><li><p>FIFO多播</p><p>先出去的应该先被受到，后出去的应该后被收到</p><p>但是结果是不确定的。你不知道哪个先到哪个后到。也可以在应用层做保证，但是很难做。</p></li><li><p>Causal 因果序多播</p></li></ul><hr><h1 id="Chapter-5-命名系统-Naming"><a href="#Chapter-5-命名系统-Naming" class="headerlink" title="Chapter 5 命名系统 - Naming"></a>Chapter 5 命名系统 - Naming</h1><blockquote><p>命名是在分布式中表示这个实体，且要访问到这个实体。</p></blockquote><ul><li>访问点：用来实体的一种特殊实体。</li><li>地址：访问点的名称。</li></ul><p>所以<strong>访问点</strong>就是实体的<strong>地址</strong>。</p><h2 id="⚡️⚡️DHT"><a href="#⚡️⚡️DHT" class="headerlink" title="⚡️⚡️DHT"></a>⚡️⚡️DHT</h2><p>全称是Distributed Hash Tables，是P2P环境下最经典的解决方案</p><h3 id="Chord"><a href="#Chord" class="headerlink" title="Chord"></a>Chord</h3><p>使用一个m位的标识符空间，把<strong>随机选择</strong>的标识符赋给结点，并把键值赋值给特定实体（任意的东西，比如文件、进程）。</p><p><strong>构造Finger table算法：</strong></p><p>每个Chord结点维护一个最多有m个实体的指状表(Finger table)，如果用$FT_p$表示结点$p$的指状表，那么有：<br>$$<br>FT_p[i]=succ(p+2^{i-1})<br>$$</p><ul><li>$p$是当前结点</li><li>$i​$是指状表的index</li><li>$succ(k)$表示k（若结点k存在）或k的下一个<strong>存在</strong>的结点，即$succ(k) \geq k$</li></ul><p>例：</p><p><img src="/studynote-of-distributed-system/Chord.png" alt></p><p>根据$FT_p[i]=succ(p+2^{i-1})$公式，构造结点p=4的Finger table：</p><table><thead><tr><th style="text-align:center">i</th><th style="text-align:center">$FT_p[i]$</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">$succ(4+1)=9$</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">$succ(4+2)=9$</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">$succ(4+4)=9$</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">$succ(4+8)=14$</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">$succ(4+16)=20$</td></tr></tbody></table><p><strong>解析算法：</strong></p><p>目标：从节点p开始解析key=k的结点</p><p>搜索节点p的Finger table，从上依次向下搜索，如果一个结点q满足：<br>$$<br>q=FT_p[j] \leq k &lt; FT_p[j+1]<br>$$<br>那么就将该请求转发给结点q；</p><p>如果p的Finger table第一个结点就比k还大，即：<br>$$<br>p &lt; k &lt; FT_p[1]<br>$$<br>那么就转发给$FT_p[1]$结点，此节点负责结点k，将k的地址返回给结点p。</p><p><strong>例：</strong></p><p>还是上面那个图</p><p>从结点1开始解析k=26：</p><ol><li>结点1的指状表里，$FT_1[5]=18 \leq 26​$，将请求转发给18；</li><li>结点18的指状表里，$FT_{18}[2]=20 \leq 26 &lt; FT_{18}[3]=28$，将请求转发给20；</li><li>结点20的指状表里，$FT_{20}[1]=21 \leq 26 &lt; FT_{20}[2]=28$，将请求转发给21；</li><li>结点21的指状表里，$21 &lt; 26 &lt; FT_{21}[1]=28​$，将请求转发给28，该结点负责解析k=26；</li></ol><p>从结点28开始解析k=12：</p><ol><li>结点28的指状表里，$FT_{28}[4]=4 \leq 12 &lt; FT_{28}[5]=14$，将请求转发给4；</li><li>结点4的指状表里，$FT_{4}[3]=9 \leq 12 &lt; FT_{4}[4]=14$，将请求转发给9；</li><li>结点9的指状表里，$FT_{9}[2]=11 \leq 12 &lt; FT_{9}[3]=14$，将请求转发给11；</li><li>结点11的指状表里，$11 &lt; 12 &lt; FT_{11}[1]=14$，将请求转发给14，该结点负责解析k=12；</li></ol><h2 id="⚡️⚡️HLS"><a href="#⚡️⚡️HLS" class="headerlink" title="⚡️⚡️HLS"></a>⚡️⚡️HLS</h2><p>网络被划分为一组域。每个域D都有关联的目录节点dir(D)，dir(D)会跟踪域中的实体，形成一颗目录结点树。</p><h3 id="HLS结构"><a href="#HLS结构" class="headerlink" title="HLS结构"></a>HLS结构</h3><p>看下面这个图来解释一下HLS吧： </p><p><img src="/studynote-of-distributed-system/image-20190417105530318.png" alt="image-20190417105530318"></p><p>为了跟踪实体E的位置，实体E位于域S中，所以域S的目录结点N含有E在该域中的位置信息。</p><p>而在比域S更高一级的域T中，域T的目录结点N’也有实体E的位置信息，但是这个位置信息只有N的指针，也就是要找实体E，就先去找到其子域的目录结点N，然后通过目录节点N找到E。</p><p>同理，在比域T更大的域中，那个域的目录节点也有实体E的位置信息，不过这个位置信息只有N’的指针，要找实体E，就要先找N’，然后找到N，最后找到E。</p><p>所以顶级域的目录结点，即根（目录）节点，包括全部实体位置信息。</p><h3 id="如果有多个实体"><a href="#如果有多个实体" class="headerlink" title="如果有多个实体"></a>如果有多个实体</h3><p>实体可以拥有多个地址，比如被复制了，实体在域D1和域D2中都有地址，那么同时包含D1和D2的最小域目录结点将有两个指针，每个指针都指向一个包含地址的子域。</p><p><img src="/studynote-of-distributed-system/image-20190417110841084.png" alt="image-20190417110841084"></p><h3 id="HLS查询操作"><a href="#HLS查询操作" class="headerlink" title="HLS查询操作"></a>HLS查询操作</h3><p><img src="/studynote-of-distributed-system/image-20190417111032790.png" alt="image-20190417111032790"></p><p>现在希望能定位实体E的位置信息，那么就向当前域的目录结点发送查找请求：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if 目录结点找到了实体E的位置信息:</span><br><span class="line">if 找到的是子域目录结点的地址</span><br><span class="line">把查找请求转发给子域的目录结点</span><br><span class="line">else </span><br><span class="line">找到了叶节点，把地址返回给请求的客户</span><br><span class="line">else</span><br><span class="line">把查找请求转发给父节点</span><br></pre></td></tr></table></figure></div><p>最差情况是一直找不到，向上转发直到根节点。</p><h3 id="插入操作"><a href="#插入操作" class="headerlink" title="插入操作"></a>插入操作</h3><p><strong>这块不要看中文教材！因为教材把图用错了！(真的坑)</strong></p><p><img src="/studynote-of-distributed-system/image-20190417113202813.png" alt="image-20190417113202813"></p><p><img src="/studynote-of-distributed-system/image-20190417113211529.png" alt="image-20190417113211529"></p><p>如果要插入实体E，将其所在域的目录节点加入实体E的地址，然后一路向上转发。</p><p>如果这个目录结点不知道E，就存储一下子域地址，直到一个节点知道E的位置或者到根节点就终止。</p><p><strong>为啥刚刚插入的实体E，可能有的结点已经知道E的位置了呢？</strong></p><p>因为E可能是复制过来的，正如上面讲的有多个实体，包含两个E的最小域的目录节点会有E的两个地址，而再往上一层，就只有E的一个地址了(指向两个E的最小域的目录节点)，到这里就停止向上传递。</p><p><img src="/studynote-of-distributed-system/image-20190417113836041.png" alt="image-20190417113836041"></p><p>这是中文教材的配图，怎么都看不懂这两条线啥意思，原来中文教材用成查询的图了。</p><p>_(:3」∠)_</p><h3 id="HLS思想"><a href="#HLS思想" class="headerlink" title="HLS思想"></a>HLS思想</h3><p>若用root node实现扁平化直接管理，根节点负载过大，且一旦崩溃，整个集群系统瘫痪。因此，使用分治思想，不同dom内实现自治。</p><h2 id="名称空间的实现-Name-Space-Implementation"><a href="#名称空间的实现-Name-Space-Implementation" class="headerlink" title="名称空间的实现 - Name Space Implementation"></a>名称空间的实现 - Name Space Implementation</h2><blockquote><p><strong><em>Basic issue</em></strong></p><p>Distribute the name resolution process as well as name space management across multiple machines, by distributing nodes of the naming graph.  </p></blockquote><h3 id="Consider-a-hierarchical-naming-graph-and-distinguish-three-levels"><a href="#Consider-a-hierarchical-naming-graph-and-distinguish-three-levels" class="headerlink" title="Consider a hierarchical naming graph and distinguish three levels:"></a>Consider a hierarchical naming graph and distinguish three levels:</h3><ul><li><strong>Global level:</strong><br>Consists of the high-level directory nodes. Main aspect is that these directory nodes have to be jointly managed by different administrations</li><li><strong>Administrational level:</strong><br>Contains mid-level directory nodes that can be grouped in such a way that each group can be assigned to a separate administration.</li><li><strong>Managerial level:</strong><br>Consists of low-level directory nodes within a single administration. Main issue is effectively mapping directory nodes to local name servers.  </li></ul><p><img src="/studynote-of-distributed-system/image-20190417153950223.png" alt="Globalimage-20190417153950223"></p><p>从上至下依次分为了Global level, Administrational level and Managerial level。</p><p>Global层几乎不怎么变化，Administrational层变化多些，Managerial层变化最多。</p><p>具体区别看下面这个表：</p><table><thead><tr><th style="text-align:left">Item</th><th style="text-align:left">Global</th><th>Administrational</th><th>Managerial</th></tr></thead><tbody><tr><td style="text-align:left">Geographical scale of network</td><td style="text-align:left">Worldwide</td><td>Organization</td><td>Department</td></tr><tr><td style="text-align:left">Total number of nodes</td><td style="text-align:left">Few</td><td>Many</td><td>Vast numbers</td></tr><tr><td style="text-align:left">Responsiveness to lookups</td><td style="text-align:left">Seconds</td><td>Milliseconds</td><td>Immediate</td></tr><tr><td style="text-align:left">Update propagation</td><td style="text-align:left">Lazy</td><td>Immediate</td><td>Immediate</td></tr><tr><td style="text-align:left">Number of replicas</td><td style="text-align:left">Many</td><td>None or few</td><td>None</td></tr><tr><td style="text-align:left">Is client-side caching applied?</td><td style="text-align:left">Yes</td><td>Yes</td><td>Sometimes</td></tr></tbody></table><h2 id="名称解析-Name-Resolution"><a href="#名称解析-Name-Resolution" class="headerlink" title="名称解析 - Name Resolution"></a>名称解析 - Name Resolution</h2><h3 id="迭代名称解析-Iterative-Name-Resolution"><a href="#迭代名称解析-Iterative-Name-Resolution" class="headerlink" title="迭代名称解析 - Iterative Name Resolution"></a>迭代名称解析 - Iterative Name Resolution</h3><ul><li>resolve(dir,[name1,…,nameK]) is sent to Server0 responsible for dir</li><li>Server0 resolves resolve(dir,name1) → dir1, returning the identification (address) of Server1, which stores dir1.</li><li>Client sends resolve(dir1,[name2,…,nameK]) to Server1, etc.</li></ul><p><img src="/studynote-of-distributed-system/image-20190417162116644.png" alt="image-20190417162116644"></p><h3 id="递归名称解析-Recursive-Name-Resolution"><a href="#递归名称解析-Recursive-Name-Resolution" class="headerlink" title="递归名称解析 - Recursive Name Resolution"></a>递归名称解析 - Recursive Name Resolution</h3><ul><li>resolve(dir,[name1,…,nameK]) is sent to Server0 responsible for dir</li><li>Server0 resolves resolve(dir,name1) →dir1, and sends resolve(dir1,[name2,…,nameK]) to Server1, which stores dir1.</li><li>Server0 waits for the result from Server1, and returns it to the client.</li></ul><p><img src="/studynote-of-distributed-system/image-20190417162203036.png" alt="image-20190417162203036"></p><h3 id="Scalability-Issues"><a href="#Scalability-Issues" class="headerlink" title="Scalability Issues"></a>Scalability Issues</h3><p><strong><em>issue 1</em></strong></p><p>因为名称解析都需要先解析高级域名，所以高层服务器的需要每秒处理大量处理请求。</p><p><strong><em>Solution:</em></strong></p><p>因为在global层和administrational层的结点内容几乎不会变，所有我们可以把这些服务器的内容广泛地复制到多个服务器中，这样名称解析可以就近处理，加快速度，如果找不到再发请求到高层服务器。</p><p><strong><em>issue 2</em></strong></p><p><strong><em>Geographical scalability</em></strong> - 地域可扩展性，即名称解析服务要可以在很大的地理距离内进行扩展。</p><p>如果客户和服务器端离的比较远，那么最好采用递归名称解析，因为客户端只和服务器通信一次即可取得结果，而迭代名称解析需要通信多次。</p><p><img src="/studynote-of-distributed-system/image-20190417163715938.png" alt="image-20190417163715938"></p><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>不考了不考了！寒窗三年不如回家种田！</p><h2 id="LDAP"><a href="#LDAP" class="headerlink" title="LDAP"></a>LDAP</h2><hr><h1 id="Chapter-6-同步化-Synchronization"><a href="#Chapter-6-同步化-Synchronization" class="headerlink" title="Chapter 6 同步化 - Synchronization"></a>Chapter 6 同步化 - Synchronization</h1><h2 id="时钟同步-Clock-Synchronization"><a href="#时钟同步-Clock-Synchronization" class="headerlink" title="时钟同步 - Clock Synchronization"></a>时钟同步 - Clock Synchronization</h2><h3 id="Berkeley算法"><a href="#Berkeley算法" class="headerlink" title="Berkeley算法"></a>Berkeley算法</h3><p>Berkeley UNIX系统的时间服务器(实际上是时间守护程序)是主动的，定期询问每台机器的时间。然后算出一个平均时间，并告诉所有其他机器将它们的时钟拨快/拨慢一个新的时间。</p><p><img src="/studynote-of-distributed-system/image-20190420021051196.png" alt="image-20190420021051196"></p><p><strong>感谢王海杰同学发现并提醒我中文书这里时间写错了。</strong></p><p><img src="/studynote-of-distributed-system/image-20190417195809532.png" alt="image-20190417195809532"></p><h2 id="⚡️⚡️逻辑时钟-Logical-clocks"><a href="#⚡️⚡️逻辑时钟-Logical-clocks" class="headerlink" title="⚡️⚡️逻辑时钟 - Logical clocks"></a>⚡️⚡️逻辑时钟 - Logical clocks</h2><h3 id="⚡️⚡️Lamport逻辑时钟"><a href="#⚡️⚡️Lamport逻辑时钟" class="headerlink" title="⚡️⚡️Lamport逻辑时钟"></a>⚡️⚡️Lamport逻辑时钟</h3><h4 id="先发生关系-Happen-Before-HB-Relation"><a href="#先发生关系-Happen-Before-HB-Relation" class="headerlink" title="先发生关系 - Happen-Before (HB) Relation"></a>先发生关系 - Happen-Before (HB) Relation</h4><p>表达式：a→b读作”a在b之前发生”，意思是所有进程一直认为事件a先发生，然后事件b才发生。</p><p>HB关系准则：</p><ol><li><p>If $\exists process   p_i$ : <strong><em>a comes before b</em>, then <em>a</em>→<em>b</em></strong></p><p>如果a和b是同一进程中的两个事件，且a在b之前发生，则a<em>→</em>b为真</p></li><li><p>For any message m, <strong><em>send(m)</em> → <em>receive(m)</em></strong></p><p>如果a是一个进程中<strong>发送</strong>消息的事件，b是另一个进程中<strong>接受</strong>这个消息的事件，那么a<em>→</em>b为真</p></li><li><p>IF a, b and c are events such that, <strong><em>a</em> → <em>b</em> and <em>b</em> → <em>c</em>, then <em>a</em> → <em>c</em></strong></p><p>如果a→b，b→c，那么a→c</p></li></ol><p><strong><em>Problem</em></strong>：</p><p>How do we maintain a global view on the system’s behavior that is consistent with the  happened before relation?</p><p>如何使用HB Relation来维持系统时间一致？</p><p><strong><em>Solution:</em></strong><br>attach a timestamp C(e) to each event e, satisfying the following properties:</p><p>对于每个事件a，我们都能为它分配一个所有进程都认可的时间值C(a)</p><ol><li><p>If a and b are two events in the same process, and a→b, then we demand that C(a) &lt; C(b).</p><p>如果a比b先，那么应该是C(a) &lt; C(b).</p></li><li><p>If a corresponds to sending a message m, and b to the receipt of that message, then also C(a) &lt; C(b).</p><p>若a是发送的，b是接受的，那么应该是C(a) &lt; C(b).</p></li></ol><p>若上述两条最后出现了C(a) ≥ C(b)，那么执行操作C(b) = C(a) +1，使得C(a) &lt; C(b).</p><p><img src="/studynote-of-distributed-system/image-20190417211008993.png" alt="image-20190417211008993"></p><p>m3是从P3发送给P2，发送时间是P3的60，接受时间是P2的56，这不符合发送时间&lt;接收时间，所以把接受时间改为发送时间+1，即61。m4同理。</p><p><img src="/studynote-of-distributed-system/image-20190417211446492.png" alt="image-20190417211446492"></p><p><strong>注意：调整时钟操作发生在中间件层。</strong></p><h3 id="全序多播"><a href="#全序多播" class="headerlink" title="全序多播"></a>全序多播</h3><p><strong>所谓的全序，就是所有进程统一顺序</strong>，只要最后所有进程的事件执行顺序相同就行。</p><ol><li>进程先给消息打上一个当前逻辑时间作为时间戳，然后把这个消息放进自己的本地队列中，再将这个消息发送给其他进程；</li><li>进程收到来自其他进程的消息后，也把这个消息放进本地队列中，按时间戳排序。</li></ol><p>例：</p><p><img src="/studynote-of-distributed-system/IMG_20190417_214308.jpg" alt="IMG_20190417_214308"></p><p>A和B的时间不同，A在本地时间3发出消息，B在本地时间6发出消息。</p><p>最后双方的本地队列的顺序都是先执行A的消息，再执行B的消息。</p><p>但是这样永远是时间快的进程先执行，时间慢的后执行，所以B发送给A的时候，要使用Lamport逻辑时钟准则调整A的时钟。</p><p><strong>全序多播开销很大！</strong></p><p>字丑勿见怪</p><h3 id="向量时钟"><a href="#向量时钟" class="headerlink" title="向量时钟"></a>向量时钟</h3><p>但是向量时钟可以捕获<strong>因果关系</strong>。</p><p>每个进程都维护一个向量$VC$，比如进程$P_i$维护的向量就是$VC_i$，有两个性质：</p><ol><li>$VC_i[i]​$表示目前为止，本进程$P_i​$发生了多少事件；</li><li>$VC_i[j]=k​$表示进程$P_i​$知道$P_j​$已经发生了k个事件。</li></ol><p>执行步骤：</p><ol><li>$P_i​$进程执行一个事件之前，先把自身的向量里第i个分量(也就是$VC_i[i]​$)自增1；</li><li>$P_i​$进程发送一个消息m给$P_j​$时，把m的时间戳ts(m)设置为等于$VC_i​$；</li><li>接受消息m时，进程$P_j$通过为每个k设置$VC_j[k] = \max(VC_j[k], ts(m)[k]$)来调整自己的向量。然后，$P_j$要执行这个事件，如步骤1所示，把$VC_j[i]​$自增1.</li></ol><h4 id="强制因果序多播"><a href="#强制因果序多播" class="headerlink" title="强制因果序多播"></a>强制因果序多播</h4><p><strong>设计思想：</strong></p><p>我懂的不能比你少，不然万一有其他消息还没来，你的消息就插队了，所以我不能收你的消息。</p><p><strong>具体操作：</strong></p><p>进程$P_i$发给进程$P_j$一个时间戳为$ts(m)$的消息m。如果不满足以下两个条件，这个消息就不交付给应用层，直到满足条件为止：</p><ol><li>$ts(m)[i] = VC_j[i]+1$</li><li>$ts(m)[k] \leq VC_j[k] \quad for   all   k \neq i$</li></ol><p>关于你刚刚发的消息，可以比我多知道一条消息。毕竟他发的，肯定比我多了解一条；</p><p>但是关于其他消息，你知道的不可以比我多，不然我就是还有其他消息没收到，等我收到其他消息后再处理这个消息。</p><p><strong>这本中文书你还能再坑一点吗？</strong></p><p><img src="/studynote-of-distributed-system/image-20190418102502861.png" alt="image-20190418102502861"></p><p><img src="/studynote-of-distributed-system/image-20190418104322665.png" alt="image-20190418104322665"></p><p><strong>例：</strong></p><p><img src="/studynote-of-distributed-system/image-20190418104501554.png" alt="image-20190418104501554"></p><ol><li>$P_0​$先发送消息m，自身先把$VC_0[0]​$自增1，消息的时间戳为(1, 0, 0)；</li><li>$P_1$收到消息m，当时$P_1$的向量是(0, 0, 0)，只有第一维比自己多1个，其他都和自己相同，所以满足上述条件，消息交付应用层；</li><li>$P_1$发送消息m*，自身先把$VC_1[1]$自增1，消息的时间戳为(1, 1, 0)， $P_0$接收此消息；</li><li>$P_2$不接收$P_1$发送的消息，因为此时$P_2$的向量是(0, 0, 0)，$P_0$的消息还没收到，所以延迟接受$P_1​$的消息；</li><li>等到$P_0$的消息到来，$P_2$调整过自身的向量后，才可以接收$P_1$的消息。</li></ol><p>例2：</p><p><img src="/studynote-of-distributed-system/image-20190418105824022.png" alt="image-20190418105824022"></p><h2 id="互斥-Mutual-Exclusion"><a href="#互斥-Mutual-Exclusion" class="headerlink" title="互斥 - Mutual Exclusion"></a>互斥 - Mutual Exclusion</h2><p>分布式系统情况下，进程将需要同时访问相同的资源，互斥算法就是保证进程之间能够互斥访问资源。</p><h3 id="集中式算法"><a href="#集中式算法" class="headerlink" title="集中式算法"></a>集中式算法</h3><p>选举一个进程作为协调者。其他进程想访问资源都要问他。</p><p><img src="/studynote-of-distributed-system/image-20190418203632964.png" alt="image-20190418203632964"></p><p>进程1是协调者，进程3想访问资源，问一下进程1，进程1说ok。</p><p><img src="/studynote-of-distributed-system/image-20190418203738517.png" alt="image-20190418203738517"></p><p>进程2也想访问资源，进程1说有人了，你到队列里等一下吧。</p><p><img src="/studynote-of-distributed-system/image-20190418204013915.png" alt="image-20190418204013915"></p><p>进程3用完资源后，向等待队列的队首进程说一声，可以用资源了。</p><h3 id="非集中式算法"><a href="#非集中式算法" class="headerlink" title="非集中式算法"></a>非集中式算法</h3><blockquote><p>这个Decentralized一点意义都没有！开销大！神经病！书上举这个例子就是说不要什么事都尝试Decentralized。Google云计算的也只是一个协调者，协调者挂了就选举。多协调者没好处，只会扯皮！</p><p>——丁箐</p></blockquote><h3 id="分布式算法"><a href="#分布式算法" class="headerlink" title="分布式算法"></a>分布式算法</h3><p>当一个进程想要某个资源，就向所有其他进程发送消息询问。比如进程A想要一个资源，就向其他进程发消息询问，进程B就收到了A的消息。</p><ul><li><p>若B不想用这个资源，就给A回一个OK。</p><p>“Yes, you can have it. I don’t want it, so what do I care?”</p></li><li><p>若B已经在用这个资源了，就不理A，但是把A的请求消息放进等待队列中。</p><p>   “Sorry, I am using it. I will save your request, and give you an OK when I am done with it.”   </p></li><li><p>若B也想用这个资源，但是还没开始用，那么就把A发送来的消息时间戳和自己将要广播消息的时间戳进行比较，谁发的早谁获得资源。<strong>这里的一致性是由Lamport逻辑时钟保证的。</strong></p><p>竞争失败的进程会给获得资源的进程发个OK，然后把自己放进等待队列里。</p><ul><li><p>If the incoming message has a lower timestamp, <strong>the receiver sends back an OK.</strong> </p><p>“I want it also, but you were first.”</p></li><li><p>If it’s own message has a lower timestamp, <strong>it queues it up.</strong> </p><p>“Sorry, I want it also, and I was first.”</p></li></ul></li></ul><p><img src="/studynote-of-distributed-system/image-20190418214710376.png" alt="image-20190418214710376"></p><h3 id="令牌环算法-A-Token-Ring-Algorithm"><a href="#令牌环算法-A-Token-Ring-Algorithm" class="headerlink" title="令牌环算法 - A Token Ring Algorithm"></a>令牌环算法 - A Token Ring Algorithm</h3><p><img src="/studynote-of-distributed-system/image-20190418220302174.png" alt="image-20190418220302174"></p><p>上图是两种组件令牌环的方式，左边是无序的，右边是有序的。</p><p>令牌(token)在进程间相互传递，拥有令牌的进程可以访问共享资源，用完就向下传。</p><p>如果某个进程收到了令牌但不用访问资源就传下去。</p><p><strong>不允许某一个进程用完资源后，使用同一令牌继续访问该资源。</strong></p><p><strong>整个分布式系统中只有一个令牌</strong>，如果令牌毁了，比如拥有令牌的进程挂掉了，那么就需要重启一个复杂的分布式进程创建新令牌。</p><h2 id="选举算法-Election-Algorithms"><a href="#选举算法-Election-Algorithms" class="headerlink" title="选举算法 - Election Algorithms"></a>选举算法 - Election Algorithms</h2><p>很多分布式的机子，要选举出一个协调者。</p><p>或者协调者崩了，要重选一个协调者。</p><p><strong>选举算法就是选一个协调者的算法。</strong></p><h3 id="欺负算法-Bully-Algorithm"><a href="#欺负算法-Bully-Algorithm" class="headerlink" title="欺负算法 - Bully Algorithm"></a>欺负算法 - Bully Algorithm</h3><p>当任何一个进程发现协调者不再响应请求时，它就发起一次选举</p><ol><li>P向所有编号比它大的进程发送一个ELECTION消息</li><li>如果无人响应，P获胜并称为协调者</li><li>如果有编号比它大的进程响应，则由响应者接管选举工作。P的工作完成。</li></ol><p><img src="/studynote-of-distributed-system/image-20190418222917752.png" alt="image-20190418222917752"></p><p>当有ELECTION消息到达时，接收者回送一个OK消息给发送者，表明自己仍在运行，并接管选举工作。最终除了一个进程外，其他进程都将放弃，这个进程就是新的协调者。它将获胜的消息发送给所有进程。</p><p>当以前的一个崩溃了的进程恢复过来时，它将主持一次选举。</p><p><strong>总结一下：</strong></p><p>这个算法就是哪个进程的编号大且没崩盘哪个就是协调者，因为编号小的都要问编号大的，编号大的只要没崩都说接管选举任务。而且崩盘的进程一旦恢复了，就会广播一个COORDINATOR消息，表示大哥回来了，又接管协调者的任务。</p><h3 id="环算法-Election-in-a-Ring"><a href="#环算法-Election-in-a-Ring" class="headerlink" title="环算法 - Election in a Ring"></a>环算法 - Election in a Ring</h3><p>会有2个消息，各绕环一圈。</p><p><strong>step 1 发送ELECTION message：</strong></p><p>当任何一个进程发现协调者不工作时，<strong>它就构造一个带有它自己进程号的ELECTION消息</strong>，发送给后继者。如果后继者崩溃，就跳过崩溃的进程，继续往下走，直到找到一个正在运行的进程。每一步，发送者都将自己的进程号加入到消息中，使自己也成为协调者的候选人。</p><p>最终消息返回到发起选举的进程。当发起者接收到包含自己号的消息，识别出这个事件，<strong>选出进程号最大的作为协调者</strong>。</p><p><strong>step 2 发送COORDINATOR message：</strong></p><p>第一则的消息转变为COORDINATOR，并再一次绕环运行，<strong>通知大家谁是新的协调者以及新环中的成员</strong>。消息循环一周后被删除，然后每个进程恢复正常工作。</p><p><img src="/studynote-of-distributed-system/image-20190418224020806.png" alt="image-20190418224020806"></p><p>这个图是说当进程2和5同时发现协调者7崩盘了，使用环选举算法选出新的协调者。</p><p>这个图在演示Election message是怎么循环的，现在还没有4到5的箭头，是因为还没循环到那里。</p><h3 id="无线系统下的选举算法"><a href="#无线系统下的选举算法" class="headerlink" title="无线系统下的选举算法"></a>无线系统下的选举算法</h3><p>选定一个源节点，在网络拓扑结构中构造树形结构，源节点就是树的根结点。</p><p>例：</p><ol><li>选定a是源节点；</li><li>a向相邻结点(b和c)发生ELECTION消息；</li><li>b和c第一次接受到ELECTION消息，认为a是他们的父结点；</li><li>b和c再次向相邻结点发送ELECTION消息，此时g结点一定都会受到b和c的ELECTION消息，谁的消息先到谁就是g的父结点；</li><li>如此递给构造树形结构，直到叶节点；</li><li>从叶节点开始，逐层向上发送自身情况，每个非叶子节点收到子节点情况汇报后，会将子结点情况与自身情况相比较，选择出最优结点，再向上汇报。这样根结点(源节点)就知道哪个结点是最优的。从而选出这个结点是协调者。</li></ol><p><img src="/studynote-of-distributed-system/image-20190419105744830.png" alt="image-20190419105744830"></p><p><strong>总结：</strong></p><ol><li>父结点向子节点广播ELECTION消息，直到最底层。</li><li>从最底层开始每个结点向父结点报告自己的容量和节点号，又父结点选择容量大的作为最佳结点，继续再向自己的父结点报告。最终到根节点选出一个容量最大的最佳结点。</li></ol><hr><h1 id="Chpater-7-一致性和复制-Consistency-amp-Replication-第7章和第8章部分内容参考李博强同学笔记，在此感谢辛苦整理！"><a href="#Chpater-7-一致性和复制-Consistency-amp-Replication-第7章和第8章部分内容参考李博强同学笔记，在此感谢辛苦整理！" class="headerlink" title="Chpater 7 一致性和复制 -  Consistency &amp; Replication (第7章和第8章部分内容参考李博强同学笔记，在此感谢辛苦整理！)"></a>Chpater 7 一致性和复制 -  Consistency &amp; Replication (第7章和第8章部分内容参考李博强同学笔记，在此感谢辛苦整理！)</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="为什么要进行数据的复制？"><a href="#为什么要进行数据的复制？" class="headerlink" title="为什么要进行数据的复制？"></a>为什么要进行数据的复制？</h3><p>进行复制是为了增强系统的<strong>可靠性</strong>和<strong>性能</strong>。</p><ul><li>可靠性：更好的防止数据被破坏。</li><li>性能：当分布式系统需要在服务器数量和地理区域上进行扩展时，就需要复制来提高性能。</li></ul><h3 id="复制带来的问题-？"><a href="#复制带来的问题-？" class="headerlink" title="复制带来的问题 ？"></a>复制带来的问题 ？</h3><p>一致性问题</p><h3 id="两个术语-W-i-x-a-和-R-i-x-b"><a href="#两个术语-W-i-x-a-和-R-i-x-b" class="headerlink" title="两个术语$W_i(x)a$和$R_i(x)b$"></a>两个术语$W_i(x)a$和$R_i(x)b$</h3><ul><li>$W_i(x)a$: 进程$P_i​$把数值a写入到数据项x。 </li><li>$R_i(x)b$: 进程$P_i$从数据项x读取数据后返回数值b。 </li></ul><h3 id="严格一致性"><a href="#严格一致性" class="headerlink" title="严格一致性"></a>严格一致性</h3><p><strong>就是写操作必须在读操作的前面执行 </strong></p><p><strong>定义:</strong></p><p>在一个副本上执行更新操作时，无论这一操作是在哪个副本上启动或执行的，这一更新操作都应该在后序操作发生前传播到所有副本。</p><p>从这个定义里可以发现：</p><ul><li><p>隐含的假设存在绝对的全局时间。 </p></li><li><p>在单处理器中(或者在有单一控制总线环境下-&gt;单一时钟)可以实现，在分布式系统中<strong>不可能</strong>实现。 </p></li></ul><p><strong>例:</strong> 下图中，(a)是具有严格一致性的，(b)是没有的，因为P2出现了R(x)NIL，P1的写操作没有在P2的读操作之前传播过来。 (不要问我这根线啥意思，没看懂，懂的老哥求在底下评论赐教，不胜感激！)</p><p><img src="/studynote-of-distributed-system/image-20190419143114051.png" alt="image-20190419143114051"></p><h2 id="⚡️⚡️以数据为中心的一致性模型"><a href="#⚡️⚡️以数据为中心的一致性模型" class="headerlink" title="⚡️⚡️以数据为中心的一致性模型"></a>⚡️⚡️以数据为中心的一致性模型</h2><p><strong>要求知道定义，能判断！</strong></p><h3 id="持续一致性"><a href="#持续一致性" class="headerlink" title="持续一致性"></a>持续一致性</h3><p>这块没讲，老师大概提了一下，大概意思是什么是一致，怎么判断一致，以一个什么样的标准来判断一致。 </p><h3 id="以数据为中心的一致性模型和以客户为中心的一致性模型的区别"><a href="#以数据为中心的一致性模型和以客户为中心的一致性模型的区别" class="headerlink" title="以数据为中心的一致性模型和以客户为中心的一致性模型的区别"></a>以数据为中心的一致性模型和以客户为中心的一致性模型的区别</h3><h4 id="所谓数据为中心的一致性模型"><a href="#所谓数据为中心的一致性模型" class="headerlink" title="所谓数据为中心的一致性模型:"></a>所谓数据为中心的一致性模型:</h4><p>不区分客户，所以叫以数据为中心的一致性模型。</p><ol><li><strong>针对所有的用户，是没有区别的。所有用户看到的都是同样的东西。对所有的用户一视同仁。</strong></li><li>以数据为中心的一致性模型，从大的范围来看，<strong>都是比较严格的一致性模型。</strong></li><li><strong>成本比较高。</strong></li><li>常常用在大型的数据中心中，系统一般来讲不是很大，<strong>以局域网为主。</strong></li></ol><h4 id="以用户为中心的一致性模型"><a href="#以用户为中心的一致性模型" class="headerlink" title="以用户为中心的一致性模型:"></a>以用户为中心的一致性模型:</h4><ol><li><strong>用户不同，看到的结果可能不一样。</strong></li><li>以客户为中心的一致性模型，<strong>都是比较弱的一致性。</strong></li><li>因为是针对某一个客户的需求来保持一致。<strong>相对来说成本比较低。</strong></li><li>范围广，甚至可以在Internet上使用，相对来说是在一个广域网上，<strong>一般用在企业应用上。</strong></li></ol><p><strong>后面的一系列模型，就是在对严格一致性的条件进行放松，在一个双方都可以接受的约束，认为它是一致的 (类似于睁一只眼闭一只眼)</strong> </p><h3 id="顺序一致性"><a href="#顺序一致性" class="headerlink" title="顺序一致性"></a>顺序一致性</h3><p><strong>定义</strong></p><p>数据存储满足以下条件时，称为是顺序一致的。</p><blockquote><p>任何执行结果都是相同的，就好像所有进程对数据存储的读、写操作是按某种序列顺序执行的，并且每个进程的操作按照程序所制定的顺序出现在这个序列中。 </p></blockquote><p><strong>和严格一致不同之处</strong></p><p>不再要求写必须在读的前面。 </p><p><strong>思想</strong></p><p>允许由于种种因素出错，但是出的错得是一样的。</p><p><strong>例：</strong></p><p><img src="/studynote-of-distributed-system/image-20190419151031060.png" alt="image-20190419151031060"></p><p>左边这个图（先不看红色的字）是满足<strong>顺序一致性</strong>的。P1的写a操作由于网络延迟等因素，在P2的写b操作之后发生，所以P3和P4都是先读到b，再读到a。如果要满足严格一致性的话（看红色字），那么在P2写b操作之后，所有进程应该只能读出b来，这才满足严格一致性。</p><p>右边的图不满足顺序一致性，P3先读到了b再读到a，P4先读到a再读到b。进程对数据存储的读写没有按某种序列顺序进行。</p><p><strong>顺序一致性也是比较难做出来的。但是是一个强一致性，顺序一致性做到了 100%的分布式透明性。</strong> </p><h3 id="因果一致性"><a href="#因果一致性" class="headerlink" title="因果一致性"></a>因果一致性</h3><p><strong>基本思想</strong></p><p>从数据的产生和消费的角度去定义因果。数据总是产生在前，消费在后。 </p><p><strong>因果性</strong></p><p>如果事件B是由事件A引起的，或受到事件A的影响，那么因果关系必然要求其他每个人先看到事件A再看到事件B。</p><p>考虑一个分布式数据库的示例：</p><p>假设进程P1对数据项x执行了写操作。然后进程P2先读取x，然后对y执行写操作。这里，对x的读操作和对y的写操作具有潜在的因果关系，因为y的计算可能依赖于P2所读取的x的值(也即，P1写入的值)。 </p><p><strong>潜在因果关系</strong> </p><ol><li><strong>同一进程，先读再写有因果关系。</strong>很可能是读取了这个数据才能执行操作，然后写入结果。</li><li><strong>不同进程，先写再读有因果关系。</strong>不同的进程间，一个把结果写进去，另一个才能够读出来。</li><li>两个读之间没有因果关系。 </li><li>两个写之间没有因果关系。 </li><li>因果关系具有传递性，e.g. If write1-&gt; read, and read -&gt; write2, then write1 -&gt; write2.</li></ol><p><strong>定义</strong></p><p>如果数据库是因果一致的，那么它必须服从以下条件：</p><blockquote><p>所有进程必须以相同的顺序看到具有<strong>潜在因果关系</strong>的写操作。</p><p>不同进程上可以以不同的顺序看到并发的写操作。</p></blockquote><p>我看到这个定义也是一脸懵逼，看几个例子就懂了。</p><p><strong>例子1</strong></p><p><img src="/studynote-of-distributed-system/image-20190419153834211.png" alt="image-20190419153834211"></p><p>由潜在因果关系第一条，P1和P2之间，先发生$W_1(x)a​$，再发生$R_2(x)a​$，先写再读，具有潜在因果关系，所以读出来的结果必须是一致的，所有进程都必须读出a。这就是定义第一句话的意思。</p><p>由潜在因果关系第四条，两个写之间没有因果关系，所以$W_2(x)b$和$W_1(x)c$没有因果关系，那么读出什么结果都满足因果一致性，这就是定义第二句话的意思。但是这里不满足严格一致性和顺序一致性。</p><p>因果关系具有传递性，在P2内部发生了先读再写，由潜在因果关系第二条，满足因果关系，再由因果关系的传递性可知，$W_1(x)a$与$W_2(x)b$构成因果关系。图上原谅色箭头所示。</p><p><strong>例子2：</strong></p><p><img src="/studynote-of-distributed-system/image-20190419155014568.png" alt="image-20190419155014568"></p><p>刚刚才提到的，由因果关系的传递性可知，$W_1(x)a$与$W_2(x)b$构成因果关系，所以后面所有进程读出来的结果都必须一直，但是这里不一致，所以不满足因果一致性。</p><p><img src="/studynote-of-distributed-system/image-20190419155138555.png" alt="image-20190419155138555"></p><p>这个就满足了，因为两个并发写没有因果关系。</p><p><strong>应用</strong></p><p>因果一致性是首先应用在大量集群中的一致性模型。</p><p>实现方式：</p><ul><li>矢量时钟</li><li>向量时间戳捕获因果关系</li></ul><h3 id="分组操作"><a href="#分组操作" class="headerlink" title="分组操作"></a>分组操作</h3><p>分组操作是一个弱一致性模型。实质是依靠<strong>加锁（这个锁就是同步化变量）</strong>，将一些非原子操作组合为原子操作，从而实现弱一致性。</p><p>由于引入了锁的概念，分布式系统就变得不是那么透明了。 </p><p><strong>分组操作需要满足以下标准:</strong> </p><blockquote><ol><li>在一个进程对被保护的共享数据的所有更新操作执行完之前，不允许另一个进程执行对同步化变量的获取访问。 </li><li>如果一个进程对某个同步化变量正进行互斥模式访问，那么其他进程就不能拥有该同步化变量，即使是非互斥模式也不行。 </li><li>某个进程对某个同步化变量的互斥模式访问完成后，除非该变量的拥有者执行完操作，否则任何其他进程对该变量的下一个非互斥模式访问也是不允许的。 </li></ol></blockquote><p><strong>解析：</strong></p><ol><li>第一个条件表示，当一个进程获得拥有权后，这种拥有权直到所有被保护的数据都已更新为止。换句话说，对被保护数据的所有远程修改都是可见的。 (这个远程修改又是个啥？？？)</li><li>第二个条件表示，在更新一个共享数据项之前，进程必须以互斥模式进入临界区，以确保不会有其他进程试图同时更新该共享数据。 </li><li>如果一个进程要以非互斥模式进入临界区，必须首先与该同步化变量的拥有者进行协商，确保临界区获得了访问被保护共享数据的最新副本。 </li></ol><p>我感觉上面废话了这么多，就是想说：</p><ol><li>一个进程获得一个数据的控制权后，其他进程不能再获得这个数据的控制权；</li><li>一个进程在修改一个数据时，必须以互斥模式进入临界区；</li><li>进程以非互斥模式进入临界区，要先获得共享数据的最新副本。</li></ol><p><strong>例：入口一致性问题</strong></p><p>入口一致性的编程问题: 正确地把数据与同步化变量相关联</p><p><img src="/studynote-of-distributed-system/image-20190419175505219.png" alt="image-20190419175505219"></p><ol><li>P1获得了x的同步变量，修改x一次，然后再获得y的同步变量，修改y，然后释放x的同步变量，再释放y的同步变量。 </li><li>进程P2获得了x的同步变量，但是没有获得y，因此他可以从x读取值a，但如果读取y则为NIL，因为进程P3首先获得了y的同步变量，所以当P1释放y的同步变量时，P3可以读得值b。 </li></ol><h3 id="Weak-consistency-properties"><a href="#Weak-consistency-properties" class="headerlink" title="Weak consistency properties"></a>Weak consistency properties</h3><ul><li>Accesses to <em>synchronization</em> <em>variables</em> associated with a data store are sequentially consistent.</li><li>No operation on a synchronization variable is allowed to be performed until <em>all</em> <em>previous writes have been completed</em> everywhere. </li><li>No read or write operation on data items are allowed to be performed until <em>all previous operations to synchronization variables have been performed</em>.</li></ul><p>例：</p><p><img src="/studynote-of-distributed-system/image-20190419180527374.png" alt="image-20190419180527374"></p><ol><li>S表示同步，由于P2，P3没有进行S操作，所以是不保证读取结果一样的。 </li><li>对于第二个(b)，P1进行了S操作，P2也在R之前进行了S操作，P2应该读取R(x)b。所以(b)这个例子是不满足弱一致性模型的。如果S在R后面，则不违反弱一致性模型。 </li></ol><h2 id="⚡️⚡️以客户为中心的一致性模型"><a href="#⚡️⚡️以客户为中心的一致性模型" class="headerlink" title="⚡️⚡️以客户为中心的一致性模型"></a>⚡️⚡️以客户为中心的一致性模型</h2><p>以客户为中心的一致性模型，底层上是更新的传播。 </p><h3 id="最终一致性-事件一致性"><a href="#最终一致性-事件一致性" class="headerlink" title="最终一致性/事件一致性"></a>最终一致性/事件一致性</h3><blockquote><p>没有更新操作时，所有副本逐渐成为相互完全相同的副本。</p></blockquote><p><strong>最终一致性实际上只要求更新操作被保证传播到所有副本。</strong> </p><p>常用在广域网，分布式数据库上。 </p><h3 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h3><ul><li><p>$X_i[t]$: Version of data item <em>x</em> at time <em>t</em> at local copy $L_i$</p><p>t时刻本地副本$L_i$中数据项x的版本</p></li><li><p>$WS(x_i[t])$: all write operations at $L_i$ since init</p><p>对$L_i$中的x进行一系列写操作得到$X_i[t]$，$WS(x_i[t])$表示这一系列写操作的集合</p></li><li><p>$WS(x_i[t1], x_j[t2])$: If operations  in $WS(x_i[t1])$ have also been performed at local copy $L_j$ at a later time <em>t</em>2, it is known that $WS(x_i[t1])$ is part of $WS(x_j[t2])$ </p><p>t2时刻， $WS(x_i[t1])$中的操作也已经在本地副本$L_i$上执行完毕，表示更新传播了，记为 $WS(x_i[t1], x_j[t2])$</p></li></ul><h3 id="⚡️单调读"><a href="#⚡️单调读" class="headerlink" title="⚡️单调读"></a>⚡️单调读</h3><p><strong>定义</strong></p><blockquote><p><em>If a process reads the value of a data item x, any successive read operation on x by that process will always return that same or a more recent value.</em></p><p>如果一个进程读取数据项x的值，那么该进程对x执行的任何后续读操作将总是得到第一次读取的那个值或更新的值。 </p></blockquote><p><strong>解释</strong></p><p>Client “sees” only same or newer version of data.</p><p>接下来的读操作要么和之前的读操作值一样，要么值比他还新。</p><p><strong>例</strong></p><p><img src="/studynote-of-distributed-system/image-20190420005037923.png" alt="image-20190420005037923"></p><p>进程先在L1上对x执行了一批写操作，又在L2上再执行一批写操作。</p><p>图(a)满足单调读一致性。在执行对L2写x2的时候，会先把在L1上的写操作更新到L2的副本上，即先写入x1，再写入x2。这样，在L2读到x2时，这里面的x2是包含x1写操作的内容的。</p><p>而图(b)中就不满足了，直接对L2写x2，那么在后续读到x2的时候，只有WS(x2)的内容，不包含x1带来的内容。</p><h3 id="⚡️单调写"><a href="#⚡️单调写" class="headerlink" title="⚡️单调写"></a>⚡️单调写</h3><p><strong>定义</strong></p><blockquote><p><em>A write operation by a process on a data item x is completed before any successive write operation on x by the same process.</em></p><p>一个进程对数据项x执行的写操作必须在该进程对x执行任何后续写操作之前完成。</p></blockquote><p><strong>解释</strong></p><p>Write happens on a copy only if it’s brought up to date with preceding write operations on same data (but possibly at different copies).</p><p>两个写操作，前面那个写操作写完，后面那个写操作才写</p><p>每个写操作完全覆盖<em>x</em>的当前值，但是没有必要更新副本。如果现在副本要更新，必须先完成之前的写操作。</p><p><strong>例</strong></p><p><img src="/studynote-of-distributed-system/image-20190420010641362.png" alt="image-20190420010641362"></p><p>图(a)中，在W(x2)之前，先把W(x1)的操作更新到L2(就是WS(x1))，满足单调写一致性。</p><p>图(b)中，b中没有WS(x1)，所以不满足单调写一致性。 </p><h3 id="⚡️读写一致性-read-your-writes"><a href="#⚡️读写一致性-read-your-writes" class="headerlink" title="⚡️读写一致性 read-your-writes"></a>⚡️读写一致性 read-your-writes</h3><p><strong>定义</strong></p><blockquote><p><em>The effect of a write operation by a process on data item x, will always be seen by a successive read operation on x by the same process.</em></p><p>一个进程对数据项x执行一次写操作的结果总是会被该进程对x执行的后续读操作看见。</p></blockquote><p><strong>解释</strong></p><p>All previous writes are always completed before any successive read.</p><p>任何读操作发生之前，应该完成全部的写操作。</p><p><img src="/studynote-of-distributed-system/image-20190420011205118.png" alt="image-20190420011205118"></p><p>进程在L1中写了一次x1，在L2中进行读操作，根据读写一致性，此时应该能把在L1中写入的x1读出来。</p><p>图(a)中，在R(x2)发生之前，有WS(x1;x2)操作，所以W(x1)在R(x2)之前被传播过来了。</p><p>图(b)中，没有把W(x1)的操作更新到L2中，所以R(x2)中没有体现x1的更新，不满足读写一致性。</p><h3 id="⚡️写读一致性-writes-follow-reads"><a href="#⚡️写读一致性-writes-follow-reads" class="headerlink" title="⚡️写读一致性  writes-follow-reads"></a>⚡️写读一致性  writes-follow-reads</h3><p><strong>定义</strong></p><blockquote><p><em>A write operation by a process on a data item x following a previous read operation on x by the same process, is guaranteed to take place on the same or a more recent value of x that was read.</em></p><p>同一个进程，对数据项x执行的读操作之后的写操作，保证发生在于x读值相同或比值更新的值上。</p></blockquote><p><strong>解释</strong></p><p>Any successive write operation on x will be performed on a copy of x that is same or more recent than the last read.</p><p>在x上所有接下来的写操作，应该是从上次读到的地方开始写，或者是比上次读到的结果还要新的地方开始写。</p><p><strong>例</strong></p><p><img src="/studynote-of-distributed-system/image-20190420012142100.png" alt="image-20190420012142100"></p><p>在WS(x2)的时候，要在上次发生读操作R(x1)的地方，或者更新的地方进行写操作，这样需要把WS(x1)传播到L2上。</p><p>图(a)中，有WS(x1,x2)，先传播了WS(x1)，再写入了x2，接下来的W(x2)就在正确的地方写入。</p><p>但是在b中没有传播WS(x1)，只有WS(x2)，不满足写读一致性。因为后续写操作没有发生在于先前x读取值相同或比之更新的值上。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>官方总结</strong></p><p>We can <strong>avoid system-wide consistency</strong>, by concentrating on what specific <strong>clients</strong> want, instead of what should be maintained by servers. Relax consistency requirements even further.</p><p>根据客户端的需求来维护内容，进一步放宽了一致性。</p><p><strong>个人总结</strong></p><p>无论是单调读单调写，还是读写一致写读一致，都是先在一个地方发生写操作，此时其他副本发生了操作，而先前的写操作没有传播过来，就是不一致的，传播过来就一致。</p><h2 id="复制管理"><a href="#复制管理" class="headerlink" title="复制管理"></a>复制管理</h2><h3 id="内容分发"><a href="#内容分发" class="headerlink" title="内容分发"></a>内容分发</h3><p>一个重要的设计问题是将要实际传播哪些信息，分三种:</p><ol><li>只传播更新的通知 </li><li>把数据从一个副本传送到另一个副本</li><li>把更新操作传播到其他副本。 </li></ol><h3 id="拉协议与推协议"><a href="#拉协议与推协议" class="headerlink" title="拉协议与推协议"></a>拉协议与推协议</h3><ol><li><p>基于推式的方法又称为基于服务器的协议，不需要其他副本请求更新，这些更新就被传播到那些副本哪里。 </p></li><li><p>基于拉式的方法，一台服务器或客户请求其他服务器向他发送该服务器此时持有的任何更新，基于拉的协议又称为基于客户的协议 </p></li><li><p>比较: </p><p>| Issue                   | Push-based                               | Pull-based        |<br>| ———————– | —————————————- | —————– |<br>| State of server         | List of client replicas and caches       | None              |<br>| Messages sent           | Update (and possibly fetch update later) | Poll and update   |<br>| Response time at client | Immediate (or fetch-update time)         | Fetch-update time |</p><p><img src="/studynote-of-distributed-system/image-20190420013804698.png" alt="image-20190420013804698"></p></li></ol><h2 id="一致性协议"><a href="#一致性协议" class="headerlink" title="一致性协议"></a>一致性协议</h2><p>讲具体如何实现前面的模型。主要讲Primary-Based Protocols基于主备份的协议和Replicated-Write </p><p>Protocols复制的写协议，持续一致性(喝口水)这个不讲了。 </p><h3 id="Primary-Based-Protocols-基于主备份的协议"><a href="#Primary-Based-Protocols-基于主备份的协议" class="headerlink" title="Primary-Based Protocols 基于主备份的协议"></a>Primary-Based Protocols 基于主备份的协议</h3><p>一个集中式的协议，是主从式的架构。 </p><h4 id="远程写协议"><a href="#远程写协议" class="headerlink" title="远程写协议"></a>远程写协议</h4><p><strong>原理图</strong> </p><p><img src="/studynote-of-distributed-system/image-20190420013926379.png" alt="image-20190420013926379"></p><p>所有的更新操作全都由primary来完成。当主服务器收到W2的时候，向所有从服务器发送W3，要 对大家一起对，要错大家一起错。 </p><ul><li><p>这个广播的写操作具有原子性，所以又叫原子多播。 </p></li><li><p>这个不是以数据为中心的一致性模型。比较符合强一致性模型的理念的。 </p></li></ul><p><strong>潜在的问题</strong> </p><ol><li>发送W1更新请求之后，需要经过W2，原子多播W3，等所有的W4都受到，才返回W5，这 一个过程中，启动更新的进程会被阻塞，要等待很长时间。 </li><li>另外一种一个非阻塞的方法，就是当前已经更新了x个副本，就返回一个确认消息，然后再通知备份服务器执行这个更新。 </li><li>牺牲了客户的时间来保证性能。</li></ol><h4 id="本地写协议"><a href="#本地写协议" class="headerlink" title="本地写协议"></a>本地写协议</h4><p><strong>原理图</strong></p><p><img src="/studynote-of-distributed-system/image-20190420014332847.png" alt="image-20190420014332847"></p><p>也是基于主备份的协议，当某个进程要更新数据项x是，先定位x的主副本，然后把它移动到当前访问的副本上，客户标记访问的服务器为new primary，之前的是old primary。先给客户做应答， 然后由new告诉各个备份服务器来更新、接受他们发来的更新确认。 </p><p><strong>潜在的问题</strong></p><p>如果客户发起写请求之后，收到确认更新消息了，立即从别的备份服务器再去读，有可能读到一个错误的结果，牺牲了严格的准确性来提升性能。 </p><h3 id="Replicated-Write-Protocols-复制的写协议"><a href="#Replicated-Write-Protocols-复制的写协议" class="headerlink" title="Replicated-Write Protocols 复制的写协议"></a>Replicated-Write Protocols 复制的写协议</h3><p>一种分布式的解决方案。</p><p><strong>规定</strong></p><p>一个服务器集群有N个机器，每个机器上都有这个文件。</p><p>若要更新这个文件，客户必须先联系至少$\frac{N}{2}+1$个服务器，并得到他们同意后自行更新。一旦他们更新，该文件将被修改，这个新文件也将与一个新版本号关联，该版本号用于识别文件的版本，对于所有新更新的文件，他们的版本号是相同的。</p><p>在读取的时候，有两种版本 </p><p>a) 联系至少N+1个服务器，请求他们返回该文件关联的版本号。 </p><p>b) 根据$N_R$和$N_W$来计算，$N_W$越大，$N_R$就可以越小。(在下面详细说) </p><ol><li><p>一个客户要读取一个具有N个副本的文件，必须组织一个读团体(read quorum)该读团体是任意$N_R$个以上服务器的集合。</p></li><li><p>要求改一个文件，客户必须组织一个至少有$N_W​$个服务器的写团体。 </p></li><li><p>$N_R$和$N_W$满足以下条件 </p><ul><li>$N_R+N_W&gt;N​$：防止读写操作冲突 </li><li>$N_W&gt;N/2$：防止写写操作冲突 </li></ul><p><img src="/studynote-of-distributed-system/image-20190420020558986.png" alt="image-20190420020558986"></p><p><img src="/studynote-of-distributed-system/image-20190420020615554.png" alt="image-20190420020615554"></p><p><strong>唉~~一声长叹，复习不易啊。</strong></p></li></ol><p><strong>举例</strong>: </p><p><img src="/studynote-of-distributed-system/image-20190420015837821.png" alt="image-20190420015837821"></p><p>图(a)和(c)是可行的，满足上述两个条件。只要满足$N_R+N_W&gt;N$，那么再怎样都会读到最新版本的文件，根据文件的版本号选择最新的文件即可。</p><p>图(b)不行，因为不满足$N_W&gt;N/2$，如果一个进程更新{A, B, C, D, E, F}，令一个进程更新{G, H, I, J, K, L}，这样两边都是最新的版本号，但是文件是内容是不同的，发生写写冲突。</p><h3 id="剩下的内容喝了口水，不讲了"><a href="#剩下的内容喝了口水，不讲了" class="headerlink" title="剩下的内容喝了口水，不讲了"></a>剩下的内容喝了口水，不讲了</h3><hr><p>我真的整理不动了😭</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.chinastor.com/s/openstack/10093K202017.html" target="_blank" rel="noopener">http://www.chinastor.com/s/openstack/10093K202017.html</a></li><li><a href="https://blog.csdn.net/controllerha/article/details/78766840" target="_blank" rel="noopener">https://blog.csdn.net/controllerha/article/details/78766840</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed system </tag>
            
            <tag> review for exam </tag>
            
            <tag> study note </tag>
            
            <tag> cloud computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab3：词频统计</title>
      <link href="/hadoop-demo-wordcount/"/>
      <url>/hadoop-demo-wordcount/</url>
      
        <content type="html"><![CDATA[<p>使用hadoop写一次词频统计的demo。</p><a id="more"></a><p>具体的操作细节有大佬已经写好了<a href="https://github.com/wangyu-/files/wiki/ds_lab3" target="_blank" rel="noopener">wiki</a>，有需要请移步，我这里只分析java代码细节。</p><h1 id="全部代码"><a href="#全部代码" class="headerlink" title="全部代码"></a>全部代码</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">      <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">        word.set(itr.nextToken());</span><br><span class="line">        context.write(word, one);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">  sum += val.get();</span><br><span class="line">  &#125;</span><br><span class="line">  result.set(sum);</span><br><span class="line">  context.write(key, result);</span><br><span class="line">  &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h1 id="Map操作"><a href="#Map操作" class="headerlink" title="Map操作"></a>Map操作</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">      word.set(itr.nextToken());</span><br><span class="line">      context.write(word, one);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>自己定义一个类，这里取名为TokenizerMapper。这个类继承自Mapper类，继承时要指定4个泛型，分别表示：</p><ul><li>输入键key的参数类型</li><li>输入值value的参数类型</li><li>输出键key的参数类型</li><li>输出值value的参数类型</li></ul><p>比如这里，输入的是文本信息，输入的value是文本中的一行文字，类型为Text，而输入的key表示该行首字母相对于文本文件首地址的偏移量，类型为java最大的类Object。 而输出的信息是词频信息，key表示一个单词，类型为Text，value表示其词频，类型为IntWritabe。</p><p>第5行定义了一个常量对象one，就表示数量1，后面出现一个单词就记录出现1次。</p><p>第6行开始实现了map方法，参数有输入信息的key和value，还有上下文context。</p><p>第7行new了一个StringTokenizer类的实例itr，在构造对象时，就把value的字符创按分隔符分成了一个个单词。</p><ul><li><code>itr.hasMoreTokens()</code>表示是否后面还有单词</li><li><code>itr.nextToken()</code>表示下一个单词</li></ul><p><code>context.write(word, one);</code>表示将这个单词的词频记为1，写入context用以记录。</p><h1 id="Reduce操作"><a href="#Reduce操作" class="headerlink" title="Reduce操作"></a>Reduce操作</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">sum += val.get();</span><br><span class="line">&#125;</span><br><span class="line">result.set(sum);</span><br><span class="line">context.write(key, result);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>自己定义一个类，取名为IntSumReducer，继承自Reduce类，同样指定4个泛型，分别表示：</p><ul><li>输入键key的参数类型</li><li>输入值value的参数类型</li><li>输出键key的参数类型</li><li>输出值value的参数类型</li></ul><p>在这里输入是Map操作的输出，即词频信息，而输出是整合好的词频信息。</p><p>第4行实现reduce方法，参数key是单词，<code>Iterable&lt;IntWritable&gt; values</code>是一个可迭代的集合，表示这个单词所有的词频信息，context是上下文。</p><p>5到8行遍历统计词频，用sum来计数，最后在第10行写入到这个key的value。</p><h1 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="JAVA"><figure class="iseeu highlight /java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">  job.setJarByClass(WordCount.class);</span><br><span class="line">  job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">  job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">  job.setReducerClass(IntSumReducer.class);</span><br><span class="line">  job.setOutputKeyClass(Text.class);</span><br><span class="line">  job.setOutputValueClass(IntWritable.class);</span><br><span class="line">  FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><ul><li><code>Configuration conf = new Configuration();</code>从Hadoop的配置文件里读取参数；</li><li><code>job.setJarByClass(WordCount.class);</code>根据WordCount类的位置设置Jar文件；</li><li><code>job.setMapperClass(TokenizerMapper.class);</code> 设置Mapper ；</li><li><code>job.setCombinerClass(IntSumReducer.class);</code>这句代码要提一下，因为如果所有的slave都分别做map操作，然后把信息全部返回给master节点，导致master节点负载很大，也会加大网络通信量。所以这个combiner操作相当于slave节点上自己先做一次reduce操作，再把信息传给master节点reduce，有助于提高性能；</li><li><code>job.setReducerClass(IntSumReducer.class);</code>设置Reduce；</li><li><code>job.setOutputKeyClass(Text.class);</code>和<code>job.setOutputValueClass(IntWritable.class);</code> 分别设置输出key的类型和value的类型；</li><li><code>FileInputFormat.addInputPath(job, new Path(args[0]));</code>设置输入文件，它是args第一个参数 ；</li><li><code>FileOutputFormat.setOutputPath(job, new Path(args[1]));</code>设置输出文件，将输出结果写入这个文件里，它是args第二个参数 ;</li><li><code>System.exit(job.waitForCompletion(true) ? 0 : 1);</code>等待执行结果，成功执行就退出码设置为0，否则为1。</li></ul>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed system </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab2：Hadoop Distributed File System(HDFS)上的基本操作</title>
      <link href="/Basic-operations-on-HDFS/"/>
      <url>/Basic-operations-on-HDFS/</url>
      
        <content type="html"><![CDATA[<p>分布式第二次实验。</p><a id="more"></a><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>使用master节点进行操作，另外两个节点也要开机。</p><h2 id="先进入root账户"><a href="#先进入root账户" class="headerlink" title="先进入root账户"></a>先进入root账户</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div><h2 id="将bin目录和sbin目录里的命令配入环境变量"><a href="#将bin目录和sbin目录里的命令配入环境变量" class="headerlink" title="将bin目录和sbin目录里的命令配入环境变量"></a>将bin目录和sbin目录里的命令配入环境变量</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>按下<code>i</code>，在最后添加一行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/hadoop-2.8.5/sbin:/usr/local/hadoop/hadoop-2.8.5/bin</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p><h2 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure></div><h2 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h2><h3 id="查看HDFS上当前目录下所有文件"><a href="#查看HDFS上当前目录下所有文件" class="headerlink" title="查看HDFS上当前目录下所有文件"></a>查看HDFS上当前目录下所有文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure></div><h3 id="递归查看HDFS上当前目录下所有文件"><a href="#递归查看HDFS上当前目录下所有文件" class="headerlink" title="递归查看HDFS上当前目录下所有文件"></a>递归查看HDFS上当前目录下所有文件</h3><p>这条命令会递归进入每个文件夹，展示出所有文件。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls -R /</span><br></pre></td></tr></table></figure></div><h3 id="在HDFS上新建文件夹"><a href="#在HDFS上新建文件夹" class="headerlink" title="在HDFS上新建文件夹"></a>在HDFS上新建文件夹</h3><h4 id="方式1-逐个建立文件夹"><a href="#方式1-逐个建立文件夹" class="headerlink" title="方式1 逐个建立文件夹"></a>方式1 逐个建立文件夹</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /user</span><br><span class="line">hadoop fs -mkdir /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h4 id="方式2-递归建立文件夹"><a href="#方式2-递归建立文件夹" class="headerlink" title="方式2 递归建立文件夹"></a>方式2 递归建立文件夹</h4><p>这种方式下，如果要建立的文件夹父目录不存在则同时建立父目录的文件夹。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h3 id="本地准备一份文件"><a href="#本地准备一份文件" class="headerlink" title="本地准备一份文件"></a>本地准备一份文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /tmp/</span><br><span class="line">mkdir charles1</span><br><span class="line">mkdir charles2   # 为后续操作做准备</span><br><span class="line">cd charles1</span><br><span class="line">vim ds2019.txt</span><br></pre></td></tr></table></figure></div><p>写入：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Distributed System 2019Spring</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p><h3 id="向HDFS上传文件"><a href="#向HDFS上传文件" class="headerlink" title="向HDFS上传文件"></a>向HDFS上传文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put /tmp/charles1/ds2019.txt /user/hadoop-user/</span><br></pre></td></tr></table></figure></div><h3 id="查看是否上传成功"><a href="#查看是否上传成功" class="headerlink" title="查看是否上传成功"></a>查看是否上传成功</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls -R /</span><br></pre></td></tr></table></figure></div><h3 id="在Terminal显示文件内容"><a href="#在Terminal显示文件内容" class="headerlink" title="在Terminal显示文件内容"></a>在Terminal显示文件内容</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /user/hadoop-user/ds2019.txt</span><br></pre></td></tr></table></figure></div><h3 id="下载HDFS上的文件"><a href="#下载HDFS上的文件" class="headerlink" title="下载HDFS上的文件"></a>下载HDFS上的文件</h3><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /user/hadoop-user/ds2019.txt /tmp/charles2</span><br></pre></td></tr></table></figure></div><h3 id="删除HDFS上的文件"><a href="#删除HDFS上的文件" class="headerlink" title="删除HDFS上的文件"></a>删除HDFS上的文件</h3><h4 id="方式1-删除某个文件"><a href="#方式1-删除某个文件" class="headerlink" title="方式1 删除某个文件"></a>方式1 删除某个文件</h4><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm /user/hadoop-user/ds2019.txt</span><br></pre></td></tr></table></figure></div><h4 id="方式2-递归删除文件夹"><a href="#方式2-递归删除文件夹" class="headerlink" title="方式2 递归删除文件夹"></a>方式2 递归删除文件夹</h4><p>若被删除的文件夹下还有文件，则一同删除</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r /user/</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> distributed system </tag>
            
            <tag> lab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式lab1：Ubuntu16.04上Hadoop环境安装</title>
      <link href="/Hadoop-installed-in-Ubuntu1604/"/>
      <url>/Hadoop-installed-in-Ubuntu1604/</url>
      
        <content type="html"><![CDATA[<p>分布式第一次实验，怎么坑这么多！🤯</p><a id="more"></a><h2 id="首先"><a href="#首先" class="headerlink" title="首先"></a>首先</h2><p>你需要开3台虚拟机，一台作为master主节点，两个作为slave从节点，分别叫做slave1和slave2。</p><h2 id="VMware虚拟机桥接模式设置"><a href="#VMware虚拟机桥接模式设置" class="headerlink" title="VMware虚拟机桥接模式设置"></a>VMware虚拟机桥接模式设置</h2><p>参考这个<a href="https://blog.csdn.net/ccyhummer/article/details/80714430" target="_blank" rel="noopener">文章</a>，（没用过VMware23333）</p><h2 id="Parallels虚拟机桥接模式设置"><a href="#Parallels虚拟机桥接模式设置" class="headerlink" title="Parallels虚拟机桥接模式设置"></a>Parallels虚拟机桥接模式设置</h2><p>参考这个<a href="https://blog.csdn.net/wuxiangmujingli/article/details/52671448" target="_blank" rel="noopener">文章</a>，IP的起始地址设为192.168.1.11，结束地址设为192.168.1.31</p><h2 id="hostname的配置操作"><a href="#hostname的配置操作" class="headerlink" title="hostname的配置操作"></a>hostname的配置操作</h2><ol><li><p>修改hosts文件，如下（master节点、slave1节点、slave2节点都做）：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br><span class="line">gedit /etc/hosts</span><br></pre></td></tr></table></figure></div><p>修改内容如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.1.11master</span><br><span class="line">192.168.1.12slave1</span><br><span class="line">192.168.1.13slave2</span><br></pre></td></tr></table></figure></div></li><li><p>修改hostname</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> master节点上修改为master，slave1节点和slave2节点上分别修改为slave1、slave2</span><br><span class="line">hostname master/slave1/slave2</span><br></pre></td></tr></table></figure></div><p>这样不能彻底修改hostname（主机名），重启后还会还原到默认的Ubuntu，要彻底修改要修改/etc/hostname，namenode和datanode各自修改为自己的hostname</p><p>直接用编辑器打开<strong>/etc/hostname</strong>这个文件<strong>，把原来的名称</strong>删掉<strong>，</strong>不要用#注释，直接删掉，因为#没用，<strong>修改内容</strong>：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> master节点上修改为master，slave1节点和slave2节点上分别修改为slave1、slave2</span><br><span class="line">master/slave1/slave2</span><br></pre></td></tr></table></figure></div><p>退出shell客户端，重新进入，并且换成root操作(这是教程上的一步，没看懂这是什么操作)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div></li><li><p>这些工作都做好了，互相ping一下看看能不能ping通，ping 节点名称</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping master/slave1/slave2</span><br></pre></td></tr></table></figure></div></li></ol><h2 id="安装jdk-所有节点都做"><a href="#安装jdk-所有节点都做" class="headerlink" title="安装jdk(所有节点都做)"></a>安装jdk(所有节点都做)</h2><ol><li><p>前往<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">oracle Java官网</a>下载JDK</p></li><li><p>确保当前是系统账户</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root</span><br></pre></td></tr></table></figure></div></li><li><p>解压缩到指定目录（以jdk-8u201-linux-x64.tar.gz为例）</p><p>创建目录:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/lib/jvm</span><br></pre></td></tr></table></figure></div><p>解压缩到该目录:</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u201-linux-x64.tar.gz -C /usr/lib/jvm</span><br></pre></td></tr></table></figure></div></li><li><p>修改环境变量，如果提示没有装vim就使用<code>apt install vim</code>装一个:　　</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>在文件末尾追加下面内容：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>set oracle jdk environment</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201  # 这里要注意目录要换成自己解压的jdk 目录</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre  </span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib  </span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure></div></li><li><p>使环境变量马上生效：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></div></li><li><p>系统注册此jdk</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_201/bin/java 300</span><br></pre></td></tr></table></figure></div></li><li><p>查看java版本，看看是否安装成功：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></div></li></ol><h2 id="SSH无密码验证配置"><a href="#SSH无密码验证配置" class="headerlink" title="SSH无密码验证配置"></a>SSH无密码验证配置</h2><h3 id="在master节点上："><a href="#在master节点上：" class="headerlink" title="在master节点上："></a>在master节点上：</h3><ol><li><p>先安装ssh</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure></div></li><li><p>先创建本地公钥：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.ssh</span><br><span class="line">cd ~/.ssh               </span><br><span class="line">rm ./id_rsa*            # 删除之前生成的公匙（如果有）</span><br><span class="line">ssh-keygen -t rsa       # 一直按回车就可以</span><br></pre></td></tr></table></figure></div></li><li><p>让 Master 节点需能无密码 SSH 本机，在 Master 节点上执行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys</span><br></pre></td></tr></table></figure></div></li><li><p>完成后可执行 <code>ssh Master</code> 验证一下（可能需要输入 yes，成功后执行 <code>exit</code> 返回原来的终端）。接着在 master 节点将上公匙传输到 slave1 节点和slave2节点：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp ./id_rsa.pub parallels@slave1:/home/parallels/  # 向slave1节点传</span><br><span class="line">scp ./id_rsa.pub parallels@slave2:/home/parallels/  # 向slave2节点传</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在slave1节点上（slave2同理）："><a href="#在slave1节点上（slave2同理）：" class="headerlink" title="在slave1节点上（slave2同理）："></a>在slave1节点上（slave2同理）：</h3><ol><li><p>将 ssh 公匙加入授权：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略</span><br><span class="line">cat /home/parallels/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在master节点上测试："><a href="#在master节点上测试：" class="headerlink" title="在master节点上测试："></a>在master节点上测试：</h3><p>执行<code>ssh slave1</code>，会有这样的结果：</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_1.png" alt="ssh slave1"></p><p>那么说明成功了，再执行<code>ssh slave2</code>看看。</p><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><p><strong>所有节点上</strong>，执行<code>ufw disable</code>就好。</p><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><h3 id="在master节点上：-1"><a href="#在master节点上：-1" class="headerlink" title="在master节点上："></a>在master节点上：</h3><p>[注]全程在root账户下运行</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br></pre></td></tr></table></figure></div><ol><li><p>到Hadoop<a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener">官网</a>下载binary的hadoop，我下载的是<strong>hadoop-2.8.5.tar.gz</strong>文件</p></li><li><p>在/usr/local目录下建立hadoop目录</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/local/hadoop</span><br></pre></td></tr></table></figure></div></li><li><p>把<strong>hadoop-2.8.5.tar.gz</strong>拷贝到/usr/local/<strong>hadoop</strong>目录下，然后解压，这里的<code>&lt;path&gt;</code>写自己的下载位置</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &lt;path&gt;/hadoop-2.8.5.tar.gz /usr/local/hadoop</span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">tar –zxvf hadoop-2.8.5.tar.gz</span><br><span class="line">cd hadoop-2.8.5</span><br></pre></td></tr></table></figure></div></li><li><p>在/usr/local/<strong>hadoop</strong>目录下新建tmp文件夹和hdfs文件夹</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">mkdir tmp</span><br><span class="line">mkdir hdfs</span><br></pre></td></tr></table></figure></div></li><li><p>编辑（可用<code>vim hadoop-env.sh</code>也可双击打开编辑）hadoop-2.8.5/etc/hadoop/<strong>hadoop-env.sh</strong>文件，把JAVA_HOME设置成Java安装根路径，如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/hadoop-2.8.5/etc/hadoop</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_201</span><br></pre></td></tr></table></figure></div></li><li><p>新建<strong>slaves</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi slaves</span><br></pre></td></tr></table></figure></div><p>添加这两条</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></div></li><li><p>编辑<strong>master</strong>文件，全文改为<code>master</code></p></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>core-site.xml</strong>文件(我在文件系统里使用的sublime修改的文件，也可以直接双击使用gedit打开)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>hdfs-site.xml</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>mapred-site.xml</strong>文件(可能需要先重命名，默认文件名为 mapred-site.xml.template)</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>修改hadoop-2.8.5/etc/hadoop/<strong>yarn-site.xml</strong>文件</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="XML"><figure class="iseeu highlight /xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></div></li><li><p>将配置好的文件复制到<strong>各个slave</strong>节点上：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo rm -r ./hadoop/tmp     # 删除 Hadoop 临时文件</span><br><span class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制</span><br><span class="line">cd ~</span><br><span class="line">scp ./hadoop.master.tar.gz slave1:/home/parallels</span><br><span class="line">scp ./hadoop.master.tar.gz slave2:/home/parallels</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="在slave1节点上："><a href="#在slave1节点上：" class="headerlink" title="在slave1节点上："></a>在slave1节点上：</h3><ol><li><p>解压文件，并修改<strong>owner</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo su root # 若当前不是root就进入root账户</span><br><span class="line">rm -r /usr/local/hadoop    # 删掉旧的（如果存在）</span><br><span class="line">tar -zxf /home/parallels/hadoop.master.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure></div></li></ol><h3 id="回到master节点"><a href="#回到master节点" class="headerlink" title="回到master节点"></a>回到master节点</h3><ol><li><p>将bin目录和sbin目录里的命令配入环境变量</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure></div><p>按下<code>i</code>，在最后添加一行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hadoop/hadoop-2.8.5/sbin:/usr/local/hadoop/hadoop-2.8.5/bin</span><br></pre></td></tr></table></figure></div><p>然后按下<code>esc</code>，输入<code>:wq</code>写入文件。</p></li><li><p>使环境变量马上生效：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></div></li><li><p>在 Master 节点执行 NameNode 的格式化</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure></div></li><li><p>接着可以启动 hadoop 了，启动需要在 <strong>Master</strong> 节点的<strong>sbin</strong>文件夹中进行：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></div><p><img src="/Hadoop-installed-in-Ubuntu1604/3_5.png" alt="hadoop启动"></p><p>通过命令 <code>jps</code> 可以查看各个节点所启动的进程。正确的话，在 Master 节点上可以看到 NameNode、ResourceManager、SecondrryNameNode、JobHistoryServer 进程。</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_2.png" alt="Master节点上的进程"></p></li><li><p>分别进入slave1节点和slave2节点，使用<code>jps</code>命令查看运行情况</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_6.png" alt="slave1运行情况"></p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_8.png" alt="slave2运行情况"></p></li><li><p>在namenode上查看集群状态</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -report</span><br></pre></td></tr></table></figure></div><p>此时可以通过在浏览器中打开<a href="http://master:50070" target="_blank" rel="noopener">http://master:50070</a>查看。</p><p><img src="/Hadoop-installed-in-Ubuntu1604/3_3.png" alt="浏览器查看结果"></p></li></ol><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>如各位同学发现问题欢迎评论指正，评论功能终于打开了233333！</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢苏璐岩、<a href="wanghj.net">wanghj</a>、汪宇同学为本文指正错误！</p>]]></content>
      
      
      <categories>
          
          <category> distributed system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> distributed system </tag>
            
            <tag> lab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>round(number[, ndigits])</title>
      <link href="/round(number%5B,%20ndigits%5D)/"/>
      <url>/round(number%5B,%20ndigits%5D)/</url>
      
        <content type="html"><![CDATA[<p><strong>round函数是Python中常用的四舍五入的函数，但是今天试用了一下发现有点小坑</strong>。</p><a id="more"></a><p><strong>对0.5进行四舍五入，结果应该为1，可是Python解释器给的结果是：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(0.5)</span><br><span class="line">0</span><br></pre></td></tr></table></figure></div><p><strong>紧接着我又试了几个数，发现：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(1.5)</span><br><span class="line">2</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(2.5)</span><br><span class="line">2</span><br></pre></td></tr></table></figure></div><p><strong>round(1.5)还算正常，round(2.5)又很奇怪了，然后看了看Python文档，里面这样解释round函数：</strong></p><blockquote><p><strong>For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both round(0.5) and round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits (positive, zero, or negative). The return value is an integer if ndigits is omitted or None. Otherwise the return value has the same type as number.</strong></p></blockquote><p><strong>意思是会把这个输入的数保留到小数点后指定的ndigit位数，并且按照一定规则圆整。规则就是看看这个数更靠近左右两端中哪一端，然后就将这个数圆整为这一端，比如0.4就圆整为0，0.6就圆整为1。那如果是像0.5这样的到左右两端距离一样的情况，就圆整到偶数的一端，比如0.5就圆整到0，1.5就圆整到2。</strong></p><p><strong>文档里还写了这么一句话：</strong></p><blockquote><p><strong>Note: The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67instead of the expected 2.68. This is not a bug: it’s a result of the fact that most decimal fractions can’t be represented exactly as a float. See Floating Point Arithmetic: Issues and Limitations for more information.</strong></p></blockquote><p><strong>就是按照上述规则，round(2.675, 2)的结果应该是2.68，可是实际结果是：</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="SHELL"><figure class="iseeu highlight /shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; round(2.675, 2)</span><br><span class="line">2.67</span><br></pre></td></tr></table></figure></div><p><strong>这不是我们写了个bug，这是因为机器在存储浮点数2.675的时候，不会存储到这么精确，存储的数比2.675小一点点，应该差不多是2.674999999这样，所以才会被圆整到2.67，这个问题没法解决。</strong></p>]]></content>
      
      
      <categories>
          
          <category> note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> round </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大软院 - 人工智能期末考试复习</title>
      <link href="/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/"/>
      <url>/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>但愿看我博客复习的同学不会被8%的概率dropout吧~</p><a id="more"></a><h1 id="AI的四大主流流派"><a href="#AI的四大主流流派" class="headerlink" title="AI的四大主流流派"></a>AI的四大主流流派</h1><ol><li>符号主义（Symbolism）例：知识图谱</li><li>连接主义（Connectionism）例：深度神经网络</li><li>行为主义（Behaviourism）例：机器人</li><li>统计主义（Statisticsism）例：机器学习</li></ol><h1 id="AI、机器学习、深度学习三者之间的异同和关联"><a href="#AI、机器学习、深度学习三者之间的异同和关联" class="headerlink" title="AI、机器学习、深度学习三者之间的异同和关联"></a>AI、机器学习、深度学习三者之间的异同和关联</h1><ol><li>三者属于包含关系：AI包含机器学习，机器学习包含深度学习，深度学习主要指机器学习中的深度神经网络；</li></ol><p><img src="/科大软院 - 人工智能期末考试复习/三者关系.png" alt="三者关系"></p><ol start="2"><li><p>在机器学习中，需要先进行人工特征提取，再定义Model；而大部分深度学习只构建一个端对端的模型，没有人工特征提取；</p></li><li><p>在深度学习中，有时需要先进行数据清洗、格式转换、特征提取（卷积）等操作，再将数据喂给全连接神经网络。</p></li></ol><h1 id="机器学习的两个阶段"><a href="#机器学习的两个阶段" class="headerlink" title="机器学习的两个阶段"></a>机器学习的两个阶段</h1><ol><li><p>训练：“三步曲” on training set</p><ul><li><p>定义Model；</p></li><li><p>定义Loss/cost/error/objective  function；</p></li><li><p>如何找到Model中的最佳function：利用梯度下降来迭代调整参数，以使得损失函数达到最小值。深度神经网络通过反向传播来降低损失，优化模型。</p></li></ul></li><li><p>预测：on Devset and testing set</p><ul><li>前向传播：预测输出，计算Loss；</li><li>通过开发集（Devset）可以用于超参数调优，模型经过训练集训练，和开发集调优，然后交给测试集测试性能。</li></ul></li></ol><h2 id="几个基本概念："><a href="#几个基本概念：" class="headerlink" title="几个基本概念："></a>几个基本概念：</h2><ul><li>Epoch：对整个数据集进行一次forward和backward过程，被称为一个Epoch；</li><li>Batch_size：一次forward/backward过程中的训练样例数，被称为Batch_size；</li><li>Batch：使用训练集中一小部分样本对模型权重进行一次反向传播的参数更新，这一小部分样本被称为一个Batch；</li><li>Iteration：1个Iteration等于使用batchsize个样本训练一次。</li></ul><p>举个栗子🌰：假设我的训练集中总样本数为2048、Batch_size=128，那么我需要迭代(Iterations)16次才能完成一个Epoch。</p><h1 id="机器学习分类的定义及差异"><a href="#机器学习分类的定义及差异" class="headerlink" title="机器学习分类的定义及差异"></a>机器学习分类的定义及差异</h1><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习的目标是建立一个学习过程，将预测结果与“训练数据”（即输入数据）的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。</p><p>例：手写字符识别、肿瘤分类、预测天气、支持向量机、线性判别。</p><p>特点：训练样本数据和待分类的类别已知，且训练样本数据皆为标签数据。</p><h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><p>非监督学习从无标记的训练数据中推断结论，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。</p><p>例：聚类分析、主成分分析。</p><p>特点：训练样本数据和待分类的类别已知，但训练样本数据皆为非标签数据。</p><h2 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h2><p>半监督学习的训练数据通常是少量有标记数据及大量未标记数据，介于无监督学习和监督学习之间。</p><p>例：聚类假设、流形假设。</p><p>特点：训练样本数据和待分类的类别已知，然而训练样本既有标签数据，也有非标签数据。</p><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>强化学习的输入数据作为对模型的反馈，强调如何基于环境而行动，以取得最大化的预期利益。</p><p>与监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。</p><p>例：学习下围棋、打星际争霸、DotA2、双人德州扑克。（全部都是1v1的场景，目前强化学习领域对于多人博弈研究的很少）</p><p>特点：决策流程，激励系统，学习一系列的行动。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>迁移学习是通过从已学习的相关任务中迁移其知识来对需要学习的新任务进行提高。</p><p>例：牛津的VGG模型、谷歌的Inception模型和word2vec模型、微软的ResNet模型。</p><p>特点：需求的训练数据集合较小、训练时间较小、可以方便的进行迁移以满足个性化。</p><h1 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h1><p>对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</p><ul><li>任务T：分类，翻译等机器学习的目标任务</li><li>性能度量P：准确率</li><li>经验E：训练集</li></ul><h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><p>欠拟合：模型拟合不够，在训练集上拟合情况很差。往往会出现偏差大、方差小的情况；</p><p>过拟合：模型过度拟合，在训练集上拟合情况很好，但是在测试集上拟合情况很差。往往会出现偏差小、方差大的情况。</p><h2 id="机器学习中，出现欠拟合时，解决办法有："><a href="#机器学习中，出现欠拟合时，解决办法有：" class="headerlink" title="机器学习中，出现欠拟合时，解决办法有："></a>机器学习中，出现欠拟合时，解决办法有：</h2><ol><li>增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间；</li><li>尝试非线性模型，比如核SVM 、决策树、DNN等模型；</li><li>如果有正则项可以减小正则项参数λ；</li><li>Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。</li></ol><h2 id="深度学习中，出现欠拟合时，解决办法有："><a href="#深度学习中，出现欠拟合时，解决办法有：" class="headerlink" title="深度学习中，出现欠拟合时，解决办法有："></a>深度学习中，出现欠拟合时，解决办法有：</h2><ol><li>选择合适的损失函数；</li><li>尝试采用Mini-batch以及Batch Norm的方法进行优化；</li><li>选用其他激活函数；</li><li>采用自适应学习率的优化算法；</li><li>进行优化时考虑Momentum。</li></ol><h2 id="机器学习中，出现过拟合时，解决办法有："><a href="#机器学习中，出现过拟合时，解决办法有：" class="headerlink" title="机器学习中，出现过拟合时，解决办法有："></a>机器学习中，出现过拟合时，解决办法有：</h2><ol><li>交叉检验，通过交叉检验得到较优的模型参数;</li><li>特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</li><li>正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</li><li>如果有正则项则可以考虑增大正则项参数λ;</li><li>增加训练数据有限程度上可以避免过拟合;</li><li>Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等。</li></ol><h2 id="深度学习中，出现过拟合时，解决办法有："><a href="#深度学习中，出现过拟合时，解决办法有：" class="headerlink" title="深度学习中，出现过拟合时，解决办法有："></a>深度学习中，出现过拟合时，解决办法有：</h2><ol><li>早停，训练时可以每过n个Epoch就在验证集上检查误差，如果发现验证误差不降反增就可以停止训练了；</li><li>正则化，和机器学习一样，在深度学习中使用正则化完成权值衰减可以防止过拟合；</li><li>Dropout，Dropout也是一种正则化手段，指暂时丢弃一部分神经元及其连接。随机丢弃神经元可以防止过拟合，同时可以高效地连接不同网络架构。</li></ol><h1 id="误差来源分析"><a href="#误差来源分析" class="headerlink" title="误差来源分析"></a>误差来源分析</h1><h2 id="偏差bias"><a href="#偏差bias" class="headerlink" title="偏差bias"></a>偏差bias</h2><p>期望预测与真实标记的误差，偏差越大偏离理论值越大。</p><p>在一个训练集$D$上模型$f$对测试样本$x$预测输出为$f(x;D)$, 那么学习算法$f$对测试样本$x$的期望预测为：<br>$$<br>\bar{f}(x)=E_D[f(x;D)]<br>$$<br>这里用偏差的平方来表示偏差的计算公式：<br>$$<br>Bias^2(x)=(\bar{f}(x)-\hat{y})^2<br>$$</p><h2 id="方差variance"><a href="#方差variance" class="headerlink" title="方差variance"></a>方差variance</h2><p>预测模型的离散程度，方差越大离散程度越大。使用样本数相同的不同训练集产生的方差为:<br>$$<br>var(x)=E_D[(f(x;D)-\bar{f}(x))^2<br>$$</p><h2 id="噪声noise"><a href="#噪声noise" class="headerlink" title="噪声noise"></a>噪声noise</h2><p>真实标记与数据集中的实际标记间的偏差（$y_D$表示在数据集中的标记，$\hat{y}$表示真实标记，这两个可能不等）：</p><p>$$<br>\epsilon=E_D[(y_D-\hat{y})^2]<br>$$</p><h2 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h2><p>学习器在新样本上的误差称为“泛化误差”。可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). </p><p>以回归任务为例, 学习算法的平方预测误差期望为：</p><p>$$<br>Err(x)=E_D[(y_D-f(x;D))^2]<br>$$</p><p>对算法的期望泛化误差进行分解，就会发现：泛化误差=偏差的平方+方差+噪声:</p><p><img src="/科大软院 - 人工智能期末考试复习/bias-variance-proof.png" alt="bias-variance-proof"></p><p><img src="/科大软院 - 人工智能期末考试复习/bias-variance.png" alt="bias-variance"></p><h1 id="三类数据集"><a href="#三类数据集" class="headerlink" title="三类数据集"></a>三类数据集</h1><ul><li>训练集：用于学习参数</li><li>开发/验证集：用于挑选超参数</li><li>测试集：用于估计泛化误差</li></ul><h1 id="Cross-Validation-–-交叉验证"><a href="#Cross-Validation-–-交叉验证" class="headerlink" title="Cross Validation – 交叉验证"></a>Cross Validation – 交叉验证</h1><p>将数据集D划分成k个大小相似的互斥子集，每次用k-1个子集作为训练集，余下的子集做测试集，最终返回k个训练结果的平均值。交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。适用于数据集不是特别大时。</p><p><img src="/科大软院 - 人工智能期末考试复习/cross validation.png" alt="cross validation"></p><h1 id="参数-v-s-超参数"><a href="#参数-v-s-超参数" class="headerlink" title="参数 v.s. 超参数"></a>参数 v.s. 超参数</h1><p>模型参数是模型内部的配置变量，通过学习算法进行优化。例：神经网络中，层与层之间的权值W与偏置b。</p><p>超参数是一个学习算法的参数。它是不会被学习算法本身影响的，它优于训练，在训练中是保持不变的。例：学习率$\eta$，正则系数$\lambda$，模型阶数，模型类型，batch_size等。</p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>L1-norm: 向量元素绝对值之和，即：</p><p>$$<br>\left | \theta \right |_1=\sum_{i=1}^{n}\left | \theta_i \right |<br>$$</p><p>$$<br>L’(\theta)=L(\theta)+\lambda\left | \theta \right |_1<br>$$</p><p><em>——LASSO model: Tibshirani,1996</em></p><p>L2-norm: 各个元素的平方和，即：</p><p>$$<br>\left | \theta \right |_2=\sum_{i=1}^{n}\theta_i^2<br>$$</p><p>$$<br>L’(\theta)=L(\theta)+\lambda\left | \theta \right |_2<br>$$</p><p><em>——Ridge model: Hoerl,1970</em></p><p>Elastic Net (L1+L2):</p><p>$$<br>L’(\theta)=L(\theta)+\lambda[\rho\left | \theta \right |_1+(1-\rho)\left | \theta \right |_2 ]<br>$$</p><h2 id="正则化的作用："><a href="#正则化的作用：" class="headerlink" title="正则化的作用："></a>正则化的作用：</h2><p>对模型进行降阶，缩小模型空间，以解决过拟合的问题。</p><h1 id="如何加快模型的训练"><a href="#如何加快模型的训练" class="headerlink" title="如何加快模型的训练"></a>如何加快模型的训练</h1><h2 id="特征缩放-标准化"><a href="#特征缩放-标准化" class="headerlink" title="特征缩放/标准化"></a>特征缩放/标准化</h2><h3 id="Feature-Scaling-–-特征缩放-归一化"><a href="#Feature-Scaling-–-特征缩放-归一化" class="headerlink" title="Feature Scaling – 特征缩放/归一化"></a>Feature Scaling – 特征缩放/归一化</h3><p>输入值减去样本中最小值，然后除以样本范围，这样会使得结果永远在0~1的范围内，所有特征参数差不多一个速度优化到最低点。式子如下：</p><p>$$<br>x’=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p><h3 id="Mean-Normalization-–-均值归一化"><a href="#Mean-Normalization-–-均值归一化" class="headerlink" title="Mean Normalization – 均值归一化"></a>Mean Normalization – 均值归一化</h3><p>输入值减去样本均值，然后除以样本范围。式子如下：</p><p>$$<br>x’=\frac{x-min(x)}{max(x)-min(x)}<br>$$</p><h3 id="zero-mean-normalization-0均值标准化"><a href="#zero-mean-normalization-0均值标准化" class="headerlink" title="zero-mean normalization - 0均值标准化"></a><em>zero-mean normalization</em> - 0均值标准化</h3><p>标准化：让输入的值减去样本平均数μ，再除以样本标准差σ。经过这样的处理，数据符合标准正态分布，即均值为0，标准差为1。</p><p>$$<br>{x}’=\frac{x-\mu}{\sigma}<br>$$</p><h2 id="梯度下降的变种"><a href="#梯度下降的变种" class="headerlink" title="梯度下降的变种"></a>梯度下降的变种</h2><h3 id="Gradient-Descent-–-梯度下降"><a href="#Gradient-Descent-–-梯度下降" class="headerlink" title="Gradient Descent – 梯度下降"></a>Gradient Descent – 梯度下降</h3><p>如果需要找到一个函数的局部极小值，必须朝着函数上当前点所对应梯度（或者是近似梯度）的反方向，前进规定步长的距离进行迭代搜索。</p><p>$$<br>w’ \leftarrow w - \eta \frac{\partial L(w,b)}{\partial w}<br>$$</p><p>$$<br>b’ \leftarrow b - \eta \frac{\partial L(w,b)}{\partial b}<br>$$</p><h3 id="为什么要以梯度的反方向为更新方向？"><a href="#为什么要以梯度的反方向为更新方向？" class="headerlink" title="为什么要以梯度的反方向为更新方向？"></a>为什么要以梯度的反方向为更新方向？</h3><p>因为梯度方向是函数方向导数最大的方向，所以沿着梯度方向的反方向更新的话，函数下降的变化率最大。</p><h3 id="Stochastic-Gradient-Descent-–-随机梯度下降"><a href="#Stochastic-Gradient-Descent-–-随机梯度下降" class="headerlink" title="Stochastic Gradient Descent – 随机梯度下降"></a>Stochastic Gradient Descent – 随机梯度下降</h3><p>随机梯度下降的损失函数（使用MSE作为损失函数）：</p><p>$$<br>L^{(i)}(w,b) = \frac{1}{2}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>通过公式可以看到，随机梯度下降每次更新只用到了一个样本，如果这个训练集有m个样本，那么梯度下降更新一次参数，随机梯度下降已经更新了m次参数了。</p><p>随机梯度下降的好处是：随机梯度下降的更新速度更快；</p><p>随机梯度下降所带来的坏处是：随机梯度下降的更新只参考了一个样本，所以更新时候的抖动现象很明显。</p><h3 id="Mini-batch-Gradient-Descent-–-Mini-batch梯度下降"><a href="#Mini-batch-Gradient-Descent-–-Mini-batch梯度下降" class="headerlink" title="Mini-batch Gradient Descent – Mini-batch梯度下降"></a>Mini-batch Gradient Descent – Mini-batch梯度下降</h3><p>Mini-batch梯度下降是梯度下降和随机梯度下降的中和版本，Mini-batch梯度下降每次更新所考虑的样本是可以被指定的，如果总共有m个样本，那就可以在1~m中任意指定。</p><p>如果每次更新时所参考的样本数合适，那么既兼顾了随机梯度下降更新速度快的特性，又兼顾了梯度下降更新的稳定性。</p><h2 id="调整学习率"><a href="#调整学习率" class="headerlink" title="调整学习率"></a>调整学习率</h2><p>当我们在训练过程中，发现loss下降的很慢时，可以适当增大学习率；发现loss不降反增的时候，要降低学习率。</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad算法的学习率会根据迭代次数来放缓学习率，从而达到学习率越来越小的目的。通过公式可以看到，学习率是会一直除以前面所有梯度的平方和再开根号的，这一定是一个大于0的数，所以学习率会越来越小。但是防止一开始的时候梯度就是0，如果让分母变为0会导致错误的，所以后面还要跟一个很小的正数$\epsilon$，最终的式子是这样的：</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2+\epsilon}}g^t<br>$$</p><p>Adagrad算法也有很多不足：</p><ol><li>如果初始的学习率设置过大的话，这个学习率要除以一个较大梯度，那么此算法会对梯度的调节太大；</li><li>在训练的中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束。</li></ol><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>Adagrad算法的改进版RMSprop算法：</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1-\alpha)(g^t)^2}<br>$$</p><p>Adagrad和RMSprop算法这两个算法很相近，不同之处在于RMSprop算法增加了一个衰减系数α来控制历史信息的获取多少。</p><h3 id="SGD-with-Momentum-SGD-M"><a href="#SGD-with-Momentum-SGD-M" class="headerlink" title="SGD with Momentum (SGD-M)"></a>SGD with Momentum (SGD-M)</h3><p>SGD 在遇到沟壑时容易出现抖动现象。为此，可以为其引入动量 Momentum，加速 SGD 在正确方向的下降并抑制震荡。</p><p>$$<br>m_t \leftarrow \gamma m_{t-1} +\eta g^t \qquad w^t \leftarrow w^{t-1} - m_t<br>$$</p><p>这里多了一个$m_t$，可以将其想象为动量或者惯性，意味着参数更新方向不仅由当前的梯度决定，也与此前累积的下降方向有关。如果上一次梯度和只一次同方向，$m_t$会越来越大，参数也会更新越来越快；如果方向不同，$m_t$会比上次更小，参数更新速度减慢。$\gamma$是取上一次更新的动量大小，通常取 0.9 左右。</p><p>这使得参数中那些梯度方向变化不大的维度可以加速更新，并减少梯度方向变化较大的维度上的更新幅度。由此产生了加速收敛和减小震荡的效果。</p><h3 id="SGD-with-Nesterov-NAG"><a href="#SGD-with-Nesterov-NAG" class="headerlink" title="SGD with Nesterov(NAG)"></a>SGD with Nesterov(NAG)</h3><p>$$<br>g^t\leftarrow \frac{\partial L(w^t- \gamma m_{t-1})}{\partial w} \qquad m_t \leftarrow \gamma m_{t-1} +\eta g^t \qquad w^t \leftarrow w^{t-1}-m_t<br>$$</p><p>NAG算法在SGD-M上进一步改进，计算$g^t$时有所不同。简单解释来说就是，在SGD-M算法中，更新参数要用到上一次更新的动量；换句话说，下一次更新也会用到这一次更新的动量。那么可以通过这一次更新的动量大概预估出下一次把参数更新到哪里，然后提前去那个地方看看梯度，如果梯度方向改变很小，那么就知道下一次更新和这一次更新方向差不多，是朝着最低点前进，那么步子就可以迈大一点；如果梯度方向改变很大，那么就知道下一次更新在不断震荡的过程中，那么步子迈小一点，减小震荡幅度。</p><p>这个解释很不严谨，上面的式子可以转换为二阶导的形式，也就是每次更新，要看上一次更新的动量，当前点的梯度值，还有一个二阶导数。有兴趣看看这个文章一起愉快的推公式吧~ </p><h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><h2 id="如何区分回归问题与分类问题"><a href="#如何区分回归问题与分类问题" class="headerlink" title="如何区分回归问题与分类问题"></a>如何区分回归问题与分类问题</h2><p>输出值可以有限枚举出来就是分类问题，否则就是回归问题。</p><h2 id="为什么不可以用线性回归的模型解决分类问题"><a href="#为什么不可以用线性回归的模型解决分类问题" class="headerlink" title="为什么不可以用线性回归的模型解决分类问题"></a>为什么不可以用线性回归的模型解决分类问题</h2><p>多分类问题中，如果使用线性回归模型，按输出值进行分类，那么就必须人为划分分类区间，使得有些分类距离很近，有些分类距离很远，极大影响了分类器性能。</p><h2 id="在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><a href="#在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE" class="headerlink" title="在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE"></a>在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</h2><p>Logistic Regression的假设函数如下：</p><p>$$<br>\sigma(z) = \frac{1}{1+e^{-z}} \quad z(x) = wx+b<br>$$</p><p>σ(z)分别对w和b求导，结果为：</p><p>$$<br>\frac{\partial \sigma(z)}{\partial w} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial w}= \sigma(z)(1-\sigma(z))\times x<br>$$</p><p>$$<br>\frac{\partial \sigma(z)}{\partial b} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial b}= \sigma(z)(1-\sigma(z))<br>$$</p><p>如果使用MSE作为损失函数的话，那写出来是这样的：</p><p>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>当我们使用梯度下降来进行凸优化的时候，分别需要计算L(w,b)对w和b的偏导数：</p><p>$$<br>\frac{\partial L(w,b)}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b)}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))<br>$$</p><p>所以在σ(x)接近于1或者0的时候，也就是预测的结果和真实结果很相近或者很不相近的时候，σ(x)和1-σ(x)中总有一个会特别小，这样会导致梯度很小，从而使得优化速度大大减缓。</p><p>而当使用Cross Entropy作为损失函数时，损失函数为：</p><p>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}(-\hat{y}^{(i)}\log(\sigma_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-\sigma_{w,b}(x^{(i)})))<br>$$</p><p>$L(w,b)$分别对$w$和$b$求偏导，结果如下：</p><p>$$<br>\frac{\partial L(w,b) }{\partial w}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b) }{\partial b}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})<br>$$</p><p>这样梯度始终和预测值与真实值之差挂钩，预测值与真实值偏离很大时，梯度也会很大，偏离很小时，梯度会很小。所以我们更倾向于使用Cross Entropy而不使用MSE。</p><h2 id="级联逻辑回归-Cascading-logistic-regression-model-模型是啥？为什么要引入这个概念？"><a href="#级联逻辑回归-Cascading-logistic-regression-model-模型是啥？为什么要引入这个概念？" class="headerlink" title="级联逻辑回归(Cascading logistic regression model)模型是啥？为什么要引入这个概念？"></a>级联逻辑回归(Cascading logistic regression model)模型是啥？为什么要引入这个概念？</h2><p>级联逻辑回归模型是将很多的逻辑回归接到一起，以进行特征转换再用一个逻辑回归来进行分类。</p><p>级联逻辑回归模型是神经网络的雏形。</p><h2 id="Loss-Function-–-损失函数"><a href="#Loss-Function-–-损失函数" class="headerlink" title="Loss Function – 损失函数"></a>Loss Function – 损失函数</h2><p>回归任务假设函数：</p><p>$$<br>h_{w,b}(x) = wx+b<br>$$</p><p>分类任务假设函数：</p><p>$$<br>h_{w,b}(x) = \sigma(wx+b)<br>$$</p><p>在回归任务中，多使用均方误差作为损失函数：</p><p>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p>在分类任务中，多使用交叉熵作为损失函数：</p><p>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}[-\hat{y}^{(i)}\ln(h_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\ln(1-h_{w,b}(x^{(i)}))]<br>$$</p><h2 id="Sigmoid和Softmax"><a href="#Sigmoid和Softmax" class="headerlink" title="Sigmoid和Softmax"></a>Sigmoid和Softmax</h2><p>Sigmoid Function不具体表示哪一个函数，而是表示一类S型函数，常用的有逻辑函数σ(z)：<br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p>Softmax，或称归一化指数函数，是逻辑函数的一种推广，二分类情况下，Softmax退化为逻辑函数。该函数的形式通常按下面的式子给出：</p><p>$$<br>\sigma(z)_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \quad j = 1,…,K<br>$$</p><h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><h2 id="Deep-Learning三步曲"><a href="#Deep-Learning三步曲" class="headerlink" title="Deep Learning三步曲"></a>Deep Learning三步曲</h2><ul><li>定义一个Model，深度学习里Model是Neural Network Structure；</li><li>定义这个Model好坏，使用合适的Loss Function来衡量损失；</li><li>找出最佳参数，使用反向传播不断优化参数，从而找出最佳参数。</li></ul><h2 id="梯度不稳定问题"><a href="#梯度不稳定问题" class="headerlink" title="梯度不稳定问题"></a>梯度不稳定问题</h2><p>根本原因在于靠近输入层的梯度是来自于靠近输出层上梯度的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。</p><h3 id="Vanishing-Gradient-Problem-梯度消失问题"><a href="#Vanishing-Gradient-Problem-梯度消失问题" class="headerlink" title="Vanishing Gradient Problem - 梯度消失问题"></a>Vanishing Gradient Problem - 梯度消失问题</h3><p>在多层网络中，影响梯度大小的因素主要有两个：权重和激活函数的偏导。深层的梯度是多个激活函数偏导乘积的形式来计算，如果这些激活函数的偏导比较小（小于1）或者为0，那么梯度随时间很容易vanishing。</p><p>反向传播时，需要计算偏导数，如果激活函数是Sigmoid函数，对其求导后，发现Sigmoid函数的导数最大也就0.25（当input=0时），而且很多情况input不会为0的，所以Sigmoid的导数会更小。那么计算靠近输入层参数的偏导数，难免会乘上几次Sigmoid函数的偏导，梯度就这样消失了。</p><h3 id="Exploding-Gradient-Problem-梯度爆炸问题"><a href="#Exploding-Gradient-Problem-梯度爆炸问题" class="headerlink" title="Exploding Gradient Problem - 梯度爆炸问题"></a>Exploding Gradient Problem - 梯度爆炸问题</h3><p>和梯度消失一样，如果这些激活函数的偏导比较大（大于1），那么梯度很有可能就会exploding。这些大于1的偏导会导致靠近输入层的参数变化比较快，靠近输出层的参数相较而言变化慢，导致梯度爆炸的问题。</p><h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><ol><li>重新设计网络模型，减少网络层数有效解决梯度不稳定问题；</li><li>使用 ReLU 激活函数，ReLU求完微分后不会引起梯度消失或爆炸的问题，而且计算速度快，加速了网络的训练；</li><li>使用LSTM，LSTM单元和相关的门类型神经元结构可以减少梯度消失问题；</li><li>使用梯度截断，自定一个阈值，梯度再大也不能超过这个阈值；</li><li>Batch-Norm，Batch-Norm通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题；</li><li>残差网络结构，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分。</li></ol><h3 id="RNN梯度消失与梯度爆炸"><a href="#RNN梯度消失与梯度爆炸" class="headerlink" title="RNN梯度消失与梯度爆炸"></a>RNN梯度消失与梯度爆炸</h3><p><img src="/科大软院 - 人工智能期末考试复习/RNN梯度消失与梯度爆炸.jpg" alt="RNN梯度消失与梯度爆炸"></p><p>对RNN进行优化需要用到BPTT算法，使用$S_i$来表示RNN的记忆状态，权值$W_x$的偏导如下：</p><p>$$<br>\frac{\partial{L_{t}}}{\partial{W_{x}}}=\sum_{k=0}^{t}{\frac{\partial{L_{t}}}{\partial{O_{t}}}\frac{\partial{O_{t}}}{\partial{S_{t}}}}(\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}})\frac{\partial{S_{k}}}{\partial{W_{x}}}<br>$$</p><p>发现其中$\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}}$是一个累乘，如果每一项都小于1，那么乘多了就变0了，如果每一项都大于1，那么乘多了又会很大，所以RNN存在梯度消失和爆炸的原因。</p><h3 id="为什么LSTM可以解决梯度消失的问题"><a href="#为什么LSTM可以解决梯度消失的问题" class="headerlink" title="为什么LSTM可以解决梯度消失的问题"></a>为什么LSTM可以解决梯度消失的问题</h3><p><img src="/科大软院 - 人工智能期末考试复习/LSTM3-focus-C.png" alt="LSTM3-focus-C"></p><p>在LSTM中，也有和RNN一样的记忆部分，叫做细胞状态(LSTMCell)，用$C_i$来表示。从上图可以看到，LSTM的单元状态$C_i$更新公式如图右侧所示，是一个加法而不是乘法，$f_t\times C_{t-1}$表示以前的记忆需要忘记多少；$i_t\times \tilde{C}_t$表示这一次的输入需要添加多少。因为是加法，所以不容易导致$C_i$接近于0的情况。</p><h2 id="Maxout-Function"><a href="#Maxout-Function" class="headerlink" title="Maxout Function"></a>Maxout Function</h2><p>Maxout Function可以理解成一种分段线性函数来近似任意凸函数，因为任意的凸函数都可由分段线性函数来拟合。它在每处都是局部线性的，而一般的激活函数都有明显的曲率。ReLU是Maxout的一种特殊情况。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout也是一种正则化手段，指暂时丢弃一部分神经元及其连接。随机丢弃神经元可以防止过拟合，同时可以高效地连接不同网络架构。</p><p>如果训练时有p%的概率dropout，那么在测试的时候，所有的权值都要乘以1-p%。</p><p>举个栗子：如果训练时有30%的概率dropout，那么在测试的时候，所有的权值都要乘以0.7。如果其中一个权重为10，则要乘以0.7，权值为7。</p><h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><p>CNN和RNN内容这么多我要写啥？？？</p><p>当输入为图像时使用CNN，CNN会自动学习到图像特征。</p><h3 id="与全连接神经网络不同之处："><a href="#与全连接神经网络不同之处：" class="headerlink" title="与全连接神经网络不同之处："></a>与全连接神经网络不同之处：</h3><ol><li>CNN采取稀疏连接的方式；</li><li>权值共享，一个卷积核可以对一张图片很多像素值进行操作。</li></ol><h3 id="CNN特点："><a href="#CNN特点：" class="headerlink" title="CNN特点："></a>CNN特点：</h3><ol><li>一些pattern只和图片局部区域有关；</li><li>图片的不同区域可能会出现的同样的pattern；</li><li>对图片进行降采样处理并不会改变图片的内容。</li></ol><p>卷积层用到1、2两个特点，池化层用到3特点。</p><h3 id="CNN里的超参数："><a href="#CNN里的超参数：" class="headerlink" title="CNN里的超参数："></a>CNN里的超参数：</h3><ul><li>Filter Size - 卷积核的尺寸</li><li>Padding - 边缘填充策略（不知道这个怎么翻才好）</li><li>Stride - 移动步长</li><li>number of filters - 卷积核的个数</li></ul><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><h3 id="什么是序列数据？会举例说明。"><a href="#什么是序列数据？会举例说明。" class="headerlink" title="什么是序列数据？会举例说明。"></a>什么是序列数据？会举例说明。</h3><p>有时间维度的数据称为序列数据。例：音乐，语音。</p><h3 id="即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？"><a href="#即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？" class="headerlink" title="即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？"></a>即使只有一层的RNN模型，仍可能出现梯度消失和梯度爆炸，为什么？</h3><p>见上面的RNN梯度消失与梯度爆炸部分。</p><h3 id="LSTM与一般的RNN相比，优势在哪？"><a href="#LSTM与一般的RNN相比，优势在哪？" class="headerlink" title="LSTM与一般的RNN相比，优势在哪？"></a>LSTM与一般的RNN相比，优势在哪？</h3><ol><li>见上面为什么LSTM可以解决梯度消失的问题。</li><li>LSTM可以保持长时记忆，LSTM的记忆门可以控制记忆存放多久。不过LSTM可以保持长时间记忆根本原因也是因为LSTM解决了梯度消失的问题吧。</li></ol><h3 id="对于给定问题，能判断出是否该使用RNN模型。"><a href="#对于给定问题，能判断出是否该使用RNN模型。" class="headerlink" title="对于给定问题，能判断出是否该使用RNN模型。"></a>对于给定问题，能判断出是否该使用RNN模型。</h3><p>当输入和输出有一个是序列数据时使用RNN模型。</p><h1 id="References："><a href="#References：" class="headerlink" title="References："></a>References：</h1><ol><li><p><a href="https://zhuanlan.zhihu.com/p/26304729" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26304729</a></p></li><li><p><a href="https://feisky.xyz/machine-learning" target="_blank" rel="noopener">https://feisky.xyz/machine-learning</a></p></li><li><p><a href="https://blog.csdn.net/fjssharpsword/article/details/71157798" target="_blank" rel="noopener">https://blog.csdn.net/fjssharpsword/article/details/71157798</a></p></li><li><p><a href="https://www.jianshu.com/p/b8844a62b04a" target="_blank" rel="noopener">https://www.jianshu.com/p/b8844a62b04a</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/32626442" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32626442</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22810533</a></p></li><li><p><a href="https://cloud.tencent.com/developer/article/1013598" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1013598</a></p></li><li><p><a href="https://ziyubiti.github.io/2016/11/06/gradvanish/" target="_blank" rel="noopener">https://ziyubiti.github.io/2016/11/06/gradvanish/</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dnn </tag>
            
            <tag> review for exam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CTC Loss学习笔记</title>
      <link href="/CTC%20Loss%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/CTC%20Loss%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>谨以此文纪念一下我那次晚上11点半查完的实验。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>语音识别时，人说一句“Hello”，尽管发音很标准，但是由于有停顿、换气或是其他原因，音频信息中的“H”音很容易对不上文本信息中的“H”。这就需要预处理对齐问题，但是用人工的方法手动对齐比较音频信息和文本信息，需要耗费大量的人力财力。CTC就是处理这一类对齐问题而生的技术。</p><h2 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h2><p>在语音识别中，CTC会逐帧辨别发的什么音。如果通过发音辨别出了一个字母，那么这一帧就标记为这个字母；如果没声音就标记为blank，表示这是两个单词中间的空格；如果分辨不出来是啥，就标记为ϵ (ctc blank)。所以将“Hello”的音频信息进行辨别后，会得到的结果可能会是：“ϵϵHellϵϵlloϵ”，也可能是“Heeϵϵllϵϵlooϵϵ”，还有其他很多种可能。</p><p>ctc的处理过程分为2步：</p><ol><li>将识别结果中相邻的重复字符删掉，那么“ϵϵHellϵϵlloϵ”就变成了“ϵHelϵloϵ”，“Heeϵϵllϵϵlooϵϵ”就变成了“Heϵlϵloϵ”；</li><li>删去ϵ，“ϵHelϵloϵ”和“Heϵlϵloϵ”就都变成了“Hello”。</li></ol><p>我们的目的是求出P(lable|x)，其中x是输入的数据，lable是标签数据，即正确的输出。P(lable|x)表示的是，如果在输入数据为x的条件下，能够正确输出标签数据的概率有多大，这个概率肯定是越大越好，如果P(lable|x)=1，那么就说明百分之百可以正确输出。</p><p>Loss的定义如下，如果P(lable|x)越接近1，Loss就接近于0；如果P(lable|x)越接近0，Loss就会越大：</p><p>$$<br>Loss=-\ln P(lable|x)<br>$$</p><p>P(lable|x)如何计算呢？从上面ctc的处理过程可以看出来，有很多种序列最终都可以处理为标签数据，“ϵϵHellϵϵlloϵ”就是一条可以被处理为“Hello”的序列，记其为seq，那么产生seq的概率有多少呢？这里$y_c^t$表示在第t时间步生成字符c的概率有多少，那么要生成seq这么长的序列，就要把生成每个字符的概率乘起来。</p><p>$$<br>P(seq|x)=y_\epsilon^1 \times y_\epsilon^2 \times y_H^3 \times y_e^4 \times y_l^5 \times y_l^6 \times y_\epsilon^7 \times y_\epsilon^8 \times y_l^9 \times y_l^{10} \times y_o^{11} \times y_\epsilon^{12}<br>$$</p><p>如果再用π来表示一个序列的集合，这个集合中所有的序列和seq一样，经过ctc处理过后，都可以变为标签数据。那么将这个集合中所有序列的概率加起来，就是P(lable|x)。</p><p>$$<br>P(lable|x)=\sum_{\pi}P(\pi|x)<br>$$</p><p>可是，π集合中有多少序列呢，茫茫多的序列都可以转为lable。如果lable是“cat”的话，那么可以转为“cat”的序列数，可以通过下图可以看出来，白色节点表示识别为字母，黑色节点表示识别不出来是个啥，标记为“ϵ”（图片来自网络）：</p><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_1.png" alt="ctc_loss_1"></p><p>每一条路径都是一种语音识别的结果，而且此结果可以经由ctc处理后转换为标签数据。这才3个字母，就这么条路径，如果成百上千的单词，一一计算每种可能的话，这个计算量很大。</p><h2 id="使用动态规划优化"><a href="#使用动态规划优化" class="headerlink" title="使用动态规划优化"></a>使用动态规划优化</h2><p>动态规划的思想是自顶向下写一个递归式，然后自底向上算出来。</p><p>如上文所述，在t个时间步走到字符“u”的路线有很多条，如果seq是其中一条路线，那么其概率为：</p>$$P(seq|x)=\prod_{i=1}^{t} y^i_{{seq}_i}$$<p>这里定义一个函数α(t,u)，表示在t个时间步走到字符“u”的概率。用π表示所有可以在t个时间步走到字符“u”的路线集合，那么有：</p>$$\alpha(t,u)=\sum_{\pi}\prod_{i=1}^{t} y^i_{\pi_i}$$<p>再看看刚刚开始那个例子“Hello”，如果当前在第t个时间步，判别成了一个字符“l”，那么在第t-1个时间步上，可以是哪些字符呢？</p><p>这要分2种情况讨论：</p><ol><li>如果“Hello”的第一个“l”，也就是”Hello”标红的那个“l”，那么在第t-1个时间步上，可以是“e”、“ϵ”、“l”三种情况；</li></ol><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_2.png" alt="ctc_loss_2"></p><p>所以要求第t个时间步走到“l”的概率，就要先求出在t-1个时间步走到“e”、“ϵ”、“l”的概率，求和之后，再乘以$y_c^t$，即：</p><p>$$<br>\alpha(t,u)=y_u^t[\alpha(t-1,u)+\alpha(t-1,u-1)+\alpha(t-1,u-2)]<br>$$</p><p>这里u-1和u-2表示字符u的上一个字符和上两个字符。</p><ol><li>如果是“Hello”的第二个“l”，也就是“Hello”标红的那个“l”，那么在第t-1个时间步上，只可以是“ϵ”、“l”两种情况。要是算上上面的“l”，那么这两个“l”会合并成1个；如果是在t个时间步上走到了“ϵ”这个字符，那么在t-1个时间步上，也只能是“l”、“ϵ”两种情况。要是也算上上面一个“ϵ”，那么会跨过中间这个“l”，这个字符就不输出了。</li></ol><p><img src="/CTC Loss学习笔记/Users/alan.ke/OneDrive - mail.ustc.edu.cn/blog/source/_posts/CTC Loss学习笔记/ctc_loss_3.png" alt="ctc_loss_3"></p><p>所以第t个时间步走到“l”或”ϵ”，只能横着走过来，或从上一个字符过来，概率即：</p><p>$$<br>\alpha(t,u)=y_u^t[\alpha(t-1,u)+\alpha(t-1,u-1)]<br>$$</p><p>这样递归公式就定义好了，再算一下递归式子的底：</p><p>$$<br>\alpha(1,1)=y_\epsilon^1<br>$$</p><p>$$<br>\alpha(1,2)=y_h^1<br>$$</p><p>$$<br>\alpha(1,u)=0 \quad u&gt;2<br>$$</p><p>从这个底开始，使用递推公式自底向上的计算，直至算出$\alpha(T,lable)+\alpha(T,lable-1)$，T表示总时间步，所有可以经过ctc转化为“Hello”的序列，最后一个字符可以是“ϵ”也可以是“o”。label表示所最后一个字符是“ϵ”，label-1表示最后一个字符是“o”，这样走出的路线都可以经过ctc转化为“Hello”。</p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ctc </tag>
            
            <tag> rnn </tag>
            
            <tag> loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>删数问题的贪心选择策略证明</title>
      <link href="/%E5%88%A0%E6%95%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%B4%AA%E5%BF%83%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5%E8%AF%81%E6%98%8E/"/>
      <url>/%E5%88%A0%E6%95%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%B4%AA%E5%BF%83%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5%E8%AF%81%E6%98%8E/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>题目：在一个n位的正整数A[1…n]中删除其中任意k(k≤n)个数字后，剩下的数字按原次序组成一个新的正整数。对于给定的n位正整数A和k，设计一个贪心算法，使得剩下的数字组成的新数最小。</strong><br><strong>如：A=278693，k=4时最小新数为23，k=3时为263</strong></p></blockquote><a id="more"></a><p>刚刚证明出来贪心选择性质，先写下，之后再慢慢补充全部思路。</p><h2 id="贪心选择策略"><a href="#贪心选择策略" class="headerlink" title="贪心选择策略"></a><strong>贪心选择策略</strong></h2><p>从高位向低位进行搜索：</p><ol><li>如果A[1…n]是一条递增序列，那么就删除最后一个数；</li><li>如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。</li></ol><h2 id="贪心选择性质证明"><a href="#贪心选择性质证明" class="headerlink" title="贪心选择性质证明"></a>贪心选择性质证明</h2><p>假设A中有n个元素，将A中的每个元素进行编号为$a_x(1\leq x\leq n)​$：<br>$$<br>A=[a_1,a_2,…,a_{n-1},a_n]<br>$$</p><ol><li>如果A[1…n]是一条递增序列，那么就删除最后一个数；</li></ol><p>记原本A所代表的数值为$T_A​$，则有：<br>$$<br>T_A=a_1 \times 10^{n-1}+a_2\times10^{n-2}+…+a_{n-1}\times10+a_n<br>$$</p><p>记删除最后一个数后，A变为A’，A’所代表的数值为$T_{A’}$，则有：</p><p>$$<br>T_{A’}=a_1\times10^{n-2}+a_2 \times 10^{n-3}+…+a_{n-2} \times 10+a_{n-1}<br>$$</p><p>如果不删除最后一个数，而是删除另一个数$a_q(1\leq q&lt; n)$，此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有： </p><p>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}+…+a_{n-1} \times 10+a_n<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_q-a_{q+1}) \times 10^{n-q-1}+(a_{q+1}-a_{q+2}) \times 10^{n-q-2}+…+(a_{n-2}-a_{n-1}) \times 10+(a_{n-1}-a_{n})<br>$$</p><p>由于A是一条递增序列，所以有：</p><p>$$<br>a_{q}\leq a_{q+1},a_{q+1}\leq a_{q+2},…,a_{n-2}\leq a_{n-1},a_{n-1}\leq a_{n}<br>$$</p><p>即：</p><p>$$<br>a_{q}-a_{q+1}\leq 0,a_{q+1}-a_{q+2}\leq 0,…,a_{n-2}-a_{n-1}\leq 0,a_{n-1}-a_{n}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’}\leq 0$，那么$T_{A’}$是A删完一个数后，所组成的最小数值。贪心选择是安全的。</p><ol start="2"><li>如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。</li></ol><p>(1) 先考虑一种情况，A中只有两段单调子区间，且高位部分是递增子区间，低位部分是递减子区间。那么A中的数就是先上升再下降，像一座山一样。如果删除第一个递减子序列的首字符是安全的，也就是删除山顶的数是安全的。</p><p>假设在A中，有3个连在一起的数$a_i,a_j,a_k(1\leq i&lt; j&lt; k\leq n)$，其中$(a_1,a_j)$是递增序列，$(a_j,a_n)$是递减序列。</p><p>记原本A所代表的数值为$T_A$，则有：</p><p>$$<br>T_A=a_1 \times 10^{n-1}+a_2 \times 10^{n-2}+…+a_i \times 10^{n-i}+a_j \times 10^{n-j}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_n<br>$$</p><p>删除$a_j$后，A变为A’，A’所代表的数值为$T_{A’}$，则有：</p><p>$$<br>T_{A’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_{n}<br>$$</p><p>如果不删除$a_j$，而是删除另一个数$a_q$。</p><p>a) 当$1\leq q&lt; j$时，此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有：<br>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}+…+a_j \times 10^{n-j}+…+a_{n-1} \times 10+a_{n}<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_q-a_{q+1}) \times 10^{n-q-1}+(a_{q+1}-a_{q+2}) \times 10^{n-q-2}+…+(a_{i}-a_{j}) \times 10^{n-j}<br>$$</p><p>由于$(a_1,a_j)$是一条递增序列，所以有：</p><p>$$<br>a_{q}\leq a_{q+1},a_{q+1}\leq a_{q+2},…,a_{i}\leq a_{j}<br>$$</p><p>即：</p><p>$$<br>a_{q}-a_{q+1}\leq 0,a_{q+1}-a_{q+2}\leq 0,…,a_{i}-a_{j}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’}\leq 0$，那么$T_{A’}$是A删完$a_q(1\leq q &lt; j)$后，所组成的最小数值。贪心选择是安全的。</p><p>(b) 当$j &lt; q\leq n$时，此时A变为A’’’，A’’’所代表的数值为$T_{A’’’}$，则有： </p><p>$$<br>T_{A’’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_j \times 10^{n-j-1}+…+a_{q-1} \times 10^{n-q}+a_{q+1} \times 10^{n-q-1}…+a_{n-1} \times 10+a_{n}<br>$$</p><p>用作差法来比较$T_{A’}$和$T_{A’’’}$的大小：</p><p>$$<br>T_{A’}-T_{A’’}=(a_k-a_j) \times 10^{n-k}+…+(a_{q-1}-a_{q-2}) \times 10^{n-q+1}+(a_{q}-a_{q-1}) \times 10^{n-q}<br>$$</p><p>由于$(a_j,a_{n})$是一条递减序列，所以有：</p><p>$$<br>a_{k}\leq a_{j},a_{q-1}\leq a_{q-2},…,a_{q}\leq a_{q-1}<br>$$</p><p>即：</p><p>$$<br>a_{k}-a_{j}\leq 0,a_{q-1}-a_{q-2}\leq 0,a_{q}-a_{q-1}\leq 0<br>$$</p><p>所以有$T_{A’}-T_{A’’’}\leq 0$，那么$T_{A’}$是A删完$a_q(j&lt; q \leq n)$后，所组成的最小数值。贪心选择是安全的。<br>综上(a)(b)所述，若A中只有两段单调子区间，且高位部分是递增子区间，低位部分是递减子区间，那么删去此递减子区间的首字符后，所组成的数是最小数值。贪心选择是安全的。</p><p>(2) 现在已经证明了如果A中只有一个山，那么删除山顶元素是最合适的。如果A中有多个山峰该如何处理？</p><p>换句话说，如果A中不止有2段单调子区间，而是由很多不同的单调子区间所组成，那应该如何处理？</p><p>这个问题看上去有点复杂了，我画个图吧：</p><p><img src="/删数问题的贪心选择策略证明/2_1.jpg" alt="2_1"></p><p>其实把图画出来后，我尝试着删除第一个山的山顶元素，删完后发现，现在的$a_6=a_5$，而$a_7=a_7$：</p><p><img src="/删数问题的贪心选择策略证明/2_2.jpg" alt="2_2"></p><p>但如果删除第一个山之后的任意一个元素的话，删完后发现$a_6=a_5$，而现在的$a_7=a_6$。</p><p><img src="/删数问题的贪心选择策略证明/2_3.jpg" alt="2_3"></p><p>由于$a_6&gt;a_7​$，所以不管后面删哪个元素，都会比删除$a_6​$所得到的数要大。</p><p>数学证明如下：</p><p>假设在A中，有3个连在一起的数$a_i,a_j,a_k(1\leq i&lt; j&lt; k\leq n)​$，还有另一个数$a_r(k \leq r &lt; n)​$，，其中$(a_1,a_j)​$是递增序列，$(a_j,a_r)​$是递减序列。</p><p>记原本A所代表的数值为$T_A$，则有：</p><p>$$<br>T_A=a_1 \times 10^{n-1}+a_2 \times 10^{n-2}+…+a_i \times 10^{n-i}+a_j \times 10^{n-j}+a_k \times 10^{n-k}+…+a_{n-1} \times 10+a_n<br>$$</p><p>删除$a_j​$后，A变为A’，A’所代表的数值为$T_{A’}​$，则有：</p><p>$$<br>T_{A’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_k \times 10^{n-k}+…<br>$$</p><p>如果不删除$a_j$，而是删除另一个数$a_q(r \leq q \leq n)$。此时A变为A’’，A’’所代表的数值为$T_{A’’}$，则有： </p><p>$$<br>T_{A’’}=a_1 \times 10^{n-2}+a_2 \times 10^{n-3}+…+a_i \times 10^{n-i-1}+a_j \times 10^{n-j-1}+…<br>$$</p><p>发现$T_{A’}$中的$10^{n-k}$的系数是$a_k$，而$T_{A’’}$中的$10^{n-k}$的系数是$a_j$，由于$a_k&lt; a_j$，那么必有$T_{A’}&lt; T_{A’’}$，所以删除第一个山峰的峰值是安全的。也就是如果A[1…n]含有严格递减子序列，那么就删除第一个严格递减子序列的首字符。贪心选择是安全的。</p><p>综上，贪心选择性质成立。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> greedy </tag>
            
            <tag> math proof </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划 - Dynamic Programming</title>
      <link href="/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%20-%20Dynamic%20Programming/"/>
      <url>/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%20-%20Dynamic%20Programming/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Those who cannot remember the past are condemned to repeat.</p><p>-Dynamic Programming</p></blockquote><a id="more"></a><p>今天的算法课讲的就是动态规划，虽早已久仰大名，但始终不太懂背后原理。听完课后赶紧把今天学到的东西记下来。</p><p>动态规划和分治法有类似之处，他们都是通过组合子问题的解来求解原问题。不同之处在于，分治法常常用于将原问题划分为几个<strong>互不相交</strong>的子问题，如果划分后，有很多<strong>完全相同</strong>的子问题，那就使用动态规划的思路比较好。例如快速排序，将一段数组以枢纽值为界一分为二，再将这两段分别排序。将分出来的两段数组分别排序就是互不相交的两个子问题，可以使用分治法来解决。再如求斐波那契数列的第n项问题，我们知道斐波那契数列的规律是<br>$$<br>F(0) = 0 \quad F(1) = 1 \quad F(n) = F(n - 1) + F(n - 2) (n &gt;= 2)<br>$$<br>我们发现求F(n)需要计算F(n-1)和F(n-2)，可是当计算F(n-1)和F(n-2)时，都需要计算F(n-3)，这样就造成了重复计算，大幅降低了效率，而动态规划算法的目的就是避免此类重复计算。</p><p>去年暑假，我好好思考了一下上台阶问题：</p><blockquote><p>有100级台阶，每次只允许上1级台阶或上2级台阶，若要求上完全部台阶，共有多少种上法？</p></blockquote><p>第一次看到还以为是组合数学问题，后来也没做出来。再一想，要不直接枚举吧，那这样的数据规模是惊人的，是指数级的时间复杂度。</p><p>emmm~我们考虑一下分治法吧。假设一下，现在已经站在了第100层台阶上，回忆一下刚刚最后一步我是怎么上来着的。我最后一步可以是从98层一步跨上来的，也可以是从99层踩上一级台阶上来。这时让我们时光倒流，我们可以选择最后一步从98层跨两级上来，也可以选择从99层爬一级上来。</p><p>现在我们将这两种可能分开讨论，如果我限定最后一步只能走一步，那么之前我就必须上到99层，而99层到100层只有1条路可走，上到100层的上法就等于上到99层的上法，所以我现在只要得知上到99层有多少种上法，就知道了上到100层有多少种上法；同理，如果我限定最后一步必须跨两步，那么我之前就必须上到98层，所以我现在只要得知上到98层有多少种上法，98层到100层也只有跨两级这一条路，所以上到100层的上法就等于上到98层的上法了。</p><p>现实里，最后一步可以跨两级，也可以只爬一级，所以上到100层的上法，应该等于上到98层的上法加上上到99层的上法。如果用函数F(n)表示上到第n层一共有多少种上法，那么就有F(100) = F(99) + F(98)，接下来也有F(99) = F(98) + F(97)，F(98) = F(97) + F(96)……。</p><p>这样问题就很简单了，只要这样不断递归下去，总会递归到一个终点，这个终点就是F(1)和F(2)，上一层台阶有多少种上法呢，显然除了一步跨上去没别的方法了吧，所以F(1) = 1；上两层台阶呢？一步一步爬，或是一步跨两个，F(2)应该等于2。那么现在的递归式应该是：<br>$$<br>F(1) = 1 \quad F(2) = 2 \quad F(n) = F(n - 1) + F(n - 2) (n &gt; 2)<br>$$<br>将n取值为100，这样我们就可以算出结果了，用c++写出的代码如下：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> F(n - <span class="number">1</span>) + F(n - <span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这个代码跑很久都是跑不出来的，就是因为重复计算的部分太多了，详细讲来，如果我们要算f(100)，那么我们需要先得知f(99)和f(98)，然后想要得知f(99)和f(98)，都需要先算出f(97)，这样f(97)就计算了两遍，造成了重复计算，浪费了大量时间。上面代码中包含大量重复计算的内容，而动态规划算法就很好的解决了重复计算的问题。</p><p>现在我们不再从100层开始逐层向下递归，而是从第一层开始逐层向上走。什么意思呢？正如上面提到的问题，如果从下往上计算的话，我们先算出了f(97)的值，然后在计算f(99)和f(98)的时候，就可以共享一个计算成果，不需要花两倍的时间来计算。</p><p>从底层开始计算时，我们首先得知f(1) = 1以及f(2) = 2，根据F(n) = F(n - 1) + F(n - 2)的公式，可以很轻松算出F(3) = 3，F(4) = 5，然后继续向下推，直到n到100就可以得到结果。用代码实现的话，可以新开一个数组array，array[n]内存储了F(n)的结果。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">long</span>&gt; <span class="built_in">array</span>(n + <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">array</span>[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">array</span>[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="built_in">array</span>[i] = <span class="built_in">array</span>[i - <span class="number">1</span>] + <span class="built_in">array</span>[i - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">array</span>[n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>这里只需要一个循环就可以得出结果啦~这个代码的时间复杂度是O(n)，但是我们的空间复杂度也达到了O(n)，我们发现很多数据都是只使用了一次就不再使用了，将这个数组改为3个变量循环交替使用可以加大使用效率。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">F</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> A, B, C;</span><br><span class="line">    A = <span class="number">1</span>;</span><br><span class="line">    B = <span class="number">2</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="keyword">return</span> A;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) <span class="keyword">return</span> B;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            C = A + B;</span><br><span class="line">            A = B;</span><br><span class="line">            B = C;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> C;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> result = F(<span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>我们使用3个变量ABC交替使用，只要始终保持ABC的先后顺序就可以啦！这种方法还将空间复杂度降为了O(1)。当然这题我们发现刚好是求斐波那契数列的第101项，如果我们直接使用斐波那契数列求第n项公式可以在常数级的时间内求出结果，但那和动态规划没有关系啦~</p><p>总结一下：线性规划就是先用分治法的方法，从终点往前思考，用递归的方式定义解；然后我们发现其中有很多重复的子问题，这样我们又从起点开始，逐步记录已经求好的子问题的解，然后需要时再拿出来用，以此节约时间。</p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> dynamic programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode 79. Word Search（非递归算法）</title>
      <link href="/LeetCode%2079.%20Word%20Search%20-%20%E9%9D%9E%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95/"/>
      <url>/LeetCode%2079.%20Word%20Search%20-%20%E9%9D%9E%E9%80%92%E5%BD%92%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>上次算法实验课的时候，助教问我可不可以用非递归的方式实现，回去之后折腾了好一会儿终于在LeetCode上AC了我的非递归版本，在这里分享一下。</p><a id="more"></a><h1 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h1><p>Given a 2D board and a word, find if the word exists in the grid.</p><p>The word can be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once.</p><p><strong>Example:</strong></p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">board =</span><br><span class="line">[</span><br><span class="line">  [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;],</span><br><span class="line">  [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;],</span><br><span class="line">  [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">Given word = &quot;ABCCED&quot;, return true.</span><br><span class="line">Given word = &quot;SEE&quot;, return true.</span><br><span class="line">Given word = &quot;ABCB&quot;, return false.</span><br></pre></td></tr></table></figure></div><h1 id="题目大意："><a href="#题目大意：" class="headerlink" title="题目大意："></a>题目大意：</h1><p>给一个char型二维数组和一个word字符串，寻找网格中是否含有word字符串，只能通过相邻（垂直或者水平）的格子连接～</p><h1 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h1><p>此题的要求是在二维的情况下搜索解是否存在，如果使用DFS（深度优先搜索）算法可以很直观明了且迅速得到解。</p><p>DFS算法的基本思路是，当搜索过程遇到分岔路口时，首先选择其中一个路口进入，如果进去之后可以找到解，在此题中就可以直接返回true，声明解是存在的；如果进去之后没有找到解，那么还回到这个分岔路口，选择另外一个路口进入，继续搜解。如果每个路口都进去过了，而且都没有搜到解，那么就可以说解是不存在的。<br>我们将二维数组中的每个点都尝试着作为起点，从这一点试着向上下左右四个方向拓展，一个方向里搜不到解就搜另一个方向，如果四个方向都搜不到解就说明以此点为起点是搜不到解的，换个起点试试~</p><p>递归算法可以看看<a href="https://www.liuchuo.net/archives/3191" target="_blank" rel="noopener">柳神的blog</a>，上次算法实验课的时候，助教问我可不可以用非递归的方式实现，回去之后折腾了好一会儿终于在LeetCode上AC了我的非递归版本，在这里分享一下。</p><p>首先，非递归算法的基本思路和递归算法大致相同，在搜索过程中遇到分岔路口的时候，我们都会选择一个路口进入。当我们使用递归算法时，操作系统会为我们保留这个岔路口的信息，如果我们选错了路，还可以方便的回到这个岔路口；而非递归算法就需要我们自己手动保存分岔路口的信息了。我们使用栈这个数据结构来保存分岔路口的信息，因为栈的先进后出的特性与这种情况十分相符，使用这种特性的目的是为了当做错了某种选择时，我们可以方便的将状态回退。</p><p>举个栗子🌰：比如今天出门要给自己搭配一套衣服，我们要选择衬衫，卫衣以及外套这三件衣服。我们的选择有黑衬衫和白衬衫、红卫衣和绿卫衣、蓝外套和粉外套，然而我的衣品不怎么样，不知道怎么穿才好看，那我只能去尝试一下。我先穿上黑衬衫，再穿上红卫衣，最后穿上蓝外套，对镜子一看，怎么这么丑！肯定是外套的问题，这时我需要换外套，就脱下现在的外套，回到穿着卫衣和衬衫的状态，然后穿上粉外套。对镜子一看，我去，更丑了！肯定是卫衣的问题，那么我要回到选择卫衣的状态，我就需要把外套脱下，再把卫衣脱下，此时只穿着衬衫，这时我就可以重新选择卫衣了。把衣服脱下就是一种状态回退，这种状态回退总是遵循先进后出的顺序，所以使用栈来保存状态回退点再合适不过了。</p><p>接下来直接进入coding时间。我们需要先准备好一个栈<code>stack&lt;unsigned long&gt;</code> 以及访问标记数组<code>vector&lt;vector&lt;bool&gt;&gt; visited;</code></p><p>第一步先把主框架exist函数搭好，之后main函数里就直接调用exist函数得出结果。内部init函数是一个将栈和visited访问标志数组初始化的函数。初始化完成之后，就尝试将board数组中的每个元素设为起点，进行dfs搜索。如果搜索到了就直接返回true；如果搜索完所有点也搜不到就返回false。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">exist</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">  init(board);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; board[<span class="number">0</span>].size(); j++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (DFSTraverse(board, word, i, j))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span> <span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board)</span> </span>&#123;</span><br><span class="line">  visited.resize(board.size(), <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;(board[<span class="number">0</span>].size()));</span><br><span class="line">  dirc.resize(board.size(), <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(board[<span class="number">0</span>].size()));</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>随后就要编写DFSTraverse函数，这样声明函数<code>bool DFSTraverse (vector&lt;vector&lt;char&gt; &gt;  board, string word, int i, int j)</code>， board为需要搜索的二维数组，word是目标字符串，i和j分别是起始元素所在的行和列index（从0开始编号）。递归算法在执行递归程序时，自动保存了两类信息：</p><ol><li>保存分岔口元素的地址，以便于返回到这个分岔路口；</li><li>保存之前所选择的路口，以便于下一次进入另一个路口进行搜索。</li></ol><p>所以，当我们使用dfs的非递归算法时，就需要手动来保存这两类信息。现在我们来看看dfs非递归版本的伪代码模板，但是下面的编码思路不会完全按照这个模板进行，会根据本题情况做适当改动：</p><p><img src="/LeetCode 79. Word Search - 非递归算法/递归模板.png" alt="递归模板"></p><h2 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h2><p>将栈初始化，同时也要将visitded访问标记数据初始化：<br>  <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!s.empty())</span><br><span class="line">    s.pop();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">    fill(visited[i].begin(), visited[i].end(), <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div></p><p>  在本题中，还需要设立一个count变量<code>int count = 0;</code> ，表示目前已经搜索到word字符串的第几个字符。</p><h2 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h2><p>访问起始顶点，在本题中，如果起始顶点和word字符串第一个字符对不上，那么可以直接返回false<code>if (board[i][j] != word[count]) return false;</code>；如果起始顶点是word字符串的第一个字符，那么才将起始顶点改为“已访问”标志，将起始顶点进栈，此时还需要将count变量加一。</p>  <div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (board[i][j] != word[count]) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    count++;</span><br><span class="line">    visited[i][j] = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>  这里遇到了非递归算法的第一个问题：我该如何保存此元素地址。如果我在线性或非线性链表中进行搜索，入栈我只需要将此节点的地址入栈即可，但我现在是在二维数组中搜索，每个元素都由i和j两个坐标表示，难道我要一次入栈2个元素吗？这样也未尝不可，我也曾试过这个方法，每次入栈2个，出栈的时候也一次出2个元素。可是问题在于，我在只想访问栈顶元素而不要求出栈时，只能访问到一个坐标，另一个坐标需要出栈一个元素才能访问到。这就很麻烦了，真让人头大.jpg。</p><p>  随后我又想到另一个方法，我可以按行顺序对每个元素进行排列，如果每行有10个元素，第一行就是0~9，第二行便是10~19……以此类推。这样的话，我就可以使用一个数字表示出这个元素的坐标，需要用到i和j的时候，也可以很轻松的算出i和j，我可真是太机智了。</p><p>  所以按此思路，入栈时应该是<code>s.push(i * board[0].size() + j);</code> 这条语句，i乘以每行元素个数再加上j，想访问栈顶元素时，便可以使用<code>cur_i = s.top() / board[0].size();cur_j = s.top() % board[0].size();</code>这两条语句。其中cur_i和cur_j是存储目前元素坐标的变量。</p><h2 id="step-3"><a href="#step-3" class="headerlink" title="step 3"></a>step 3</h2><h3 id="step-3-0"><a href="#step-3-0" class="headerlink" title="step 3.0"></a>step 3.0</h3><p>这是伪代码里没有写到的，此题不一定要遍历完全部的元素，只要搜索到目标字符串就可以返回true了，所以while循环体内要这样写：</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (!s.empty()) &#123;   </span><br><span class="line">    <span class="keyword">if</span> (count == word.length())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">    &lt;code&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><h3 id="step-3-1"><a href="#step-3-1" class="headerlink" title="step 3.1"></a>step 3.1</h3><p>访问栈顶元素顶点，这里就可以使用上面提到的方法算出栈顶元素的地址了；</p><h3 id="step-3-2"><a href="#step-3-2" class="headerlink" title="step 3.2"></a>step 3.2</h3><p>看看栈顶元素顶点存在未被访问过的邻接点w。这里又面临到非递归算法的第二个问题：如何记录这个顶点已经访问过哪些邻接点，还没有访问过哪些邻接点。因为已经访问过的点如果再访问一遍也是得到一样的结果，很容易陷入死循环。</p><p>对于这个问题，我想过好几个解决方案，最后我在代码中规定每个元素必须按照上下左右的顺序对周边元素进行访问，再使用一个辅助数组dirc<code>vector&lt;vector&lt;int&gt; &gt; dirc;</code>来记录每个元素下一步可以对哪个周边元素进行访问，值可以为1、2、3、4，分别表示可以对上面元素、下面元素、左边元素、右边元素进行访问，若值为5，表示周边元素都访问完了，只能回退到上一个分岔路口，重做选择。我只需要判断dirc中的值，从而决定此元素下一步尝试向哪个方向走。</p><p>当我们决定向一个方向走的时候，我们还需要做3个判断：</p><ol><li>要确定这一步会不会造成数组越界。这里都已向上走为例，向上走时，需要判断<code>cur_i - 1 &gt;= 0</code>；</li><li>要确定下一个访问的元素是否已访问过，如果不判断的话，把路径走成一个环，然后陷入死循环出不来<code>visited[cur_i - 1][cur_j] == false</code>；</li><li>需要判断下一个访问的元素是否与目标字符串里下一个元素相同<code>board[cur_i - 1][cur_j] == word[count]</code>。</li></ol><p>如果通过了上面3个判断，就可以访问邻接点w了，并且将顶点w改为“已访问”标志<code>visited[cur_i - 1][cur_j] = true;</code>，再将顶点w入栈<code>s.push((cur_i - 1) * board[0].size() + cur_j);</code>，count变量也要加一<code>count++;</code>，意味着搜索下一个元素。</p><h3 id="step-3-3"><a href="#step-3-3" class="headerlink" title="step 3.3"></a>step 3.3</h3><p>如果dirc中记录此元素四个方向都访问过了，那么只能回退，需要将此元素出栈<code>s.pop();</code>，访问标记置为“未访问”<code>visited[cur_i][cur_j] = false;</code>，dirc数组内的值重新置为1<code>dirc[cur_i][cur_j] = 1;</code>，count也要减一<code>count--;</code>。</p><h1 id="完整代码："><a href="#完整代码：" class="headerlink" title="完整代码："></a>完整代码：</h1><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="CPP"><figure class="iseeu highlight /cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">long</span>&gt; s;</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt; visited;</span><br><span class="line">    <span class="comment">// dirc表示行走方向, 1：向上，2：向下，3：向左，4：向右，5：回退</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; dirc;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">init</span> <span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board)</span> </span>&#123;</span><br><span class="line">        visited.resize(board.size());</span><br><span class="line">        dirc.resize(board.size());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; visited.size(); i++) &#123;</span><br><span class="line">            visited[i].resize(board[<span class="number">0</span>].size());</span><br><span class="line">            dirc[i].resize(board[<span class="number">0</span>].size());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">DFSTraverse</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> str, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 栈初始化</span></span><br><span class="line">        <span class="keyword">while</span>(!s.empty()) s.pop();</span><br><span class="line">        <span class="comment">// visited标记数组初始化</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">            fill(visited[i].begin(), visited[i].end(), <span class="literal">false</span>);</span><br><span class="line">            fill(dirc[i].begin(), dirc[i].end(), <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (board[i][j] != str[count]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            count++;</span><br><span class="line">            visited[i][j] = <span class="literal">true</span>;</span><br><span class="line">            s.push(i * board[<span class="number">0</span>].size() + j);</span><br><span class="line">            <span class="keyword">int</span> cur_i = i, cur_j = j;</span><br><span class="line">          </span><br><span class="line">            <span class="keyword">while</span> (!s.empty()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (count == str.length()) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                cur_i = s.top() / board[<span class="number">0</span>].size();</span><br><span class="line">                cur_j = s.top() % board[<span class="number">0</span>].size();</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">switch</span> (dirc[cur_i][cur_j]) &#123;            </span><br><span class="line">                    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                        <span class="comment">// 上</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_i - <span class="number">1</span> &gt;= <span class="number">0</span></span><br><span class="line">                            &amp;&amp; visited[cur_i - <span class="number">1</span>][cur_j] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i - <span class="number">1</span>][cur_j] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i - <span class="number">1</span>][cur_j] = <span class="literal">true</span>;</span><br><span class="line">                            s.push((cur_i - <span class="number">1</span>) * board[<span class="number">0</span>].size() + cur_j);</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                        <span class="comment">// 下</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_i + <span class="number">1</span> &lt; board.size()</span><br><span class="line">                            &amp;&amp; visited[cur_i + <span class="number">1</span>][cur_j] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i + <span class="number">1</span>][cur_j] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i + <span class="number">1</span>][cur_j] = <span class="literal">true</span>;</span><br><span class="line">                            s.push((cur_i + <span class="number">1</span>) * board[<span class="number">0</span>].size() + cur_j);</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">                        <span class="comment">// 左</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_j - <span class="number">1</span> &gt;= <span class="number">0</span></span><br><span class="line">                            &amp;&amp; visited[cur_i][cur_j - <span class="number">1</span>] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i][cur_j - <span class="number">1</span>] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i][cur_j - <span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">                            s.push(cur_i * board[<span class="number">0</span>].size() + (cur_j - <span class="number">1</span>));</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">                        <span class="comment">// 右</span></span><br><span class="line">                        <span class="keyword">if</span> (cur_j + <span class="number">1</span> &lt; board[<span class="number">0</span>].size()</span><br><span class="line">                            &amp;&amp; visited[cur_i][cur_j + <span class="number">1</span>] == <span class="literal">false</span></span><br><span class="line">                            &amp;&amp; board[cur_i][cur_j + <span class="number">1</span>] == str[count]) &#123;</span><br><span class="line">                            visited[cur_i][cur_j + <span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">                            s.push(cur_i * board[<span class="number">0</span>].size() + (cur_j + <span class="number">1</span>));</span><br><span class="line">                            count++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        dirc[cur_i][cur_j]++;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">                        s.pop();</span><br><span class="line">                        visited[cur_i][cur_j] = <span class="literal">false</span>;</span><br><span class="line">                        dirc[cur_i][cur_j] = <span class="number">1</span>;</span><br><span class="line">                        count--;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">default</span>:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">exist</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">        init(board);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; board.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; board[<span class="number">0</span>].size(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (DFSTraverse(board, word, i, j))</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">test</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt; &gt; &amp;board, <span class="built_in">string</span> word)</span> </span>&#123;</span><br><span class="line">        init(board);</span><br><span class="line">        <span class="keyword">return</span> DFSTraverse(board, word, <span class="number">1</span>, <span class="number">4</span>) ? <span class="literal">true</span> : <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科大软院 - 人工智能期中考试复习</title>
      <link href="/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E4%B8%AD%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/"/>
      <url>/%E7%A7%91%E5%A4%A7%E8%BD%AF%E9%99%A2%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E4%B8%AD%E8%80%83%E8%AF%95%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰</p><a id="more"></a><h2 id="1、机器学习的定义"><a href="#1、机器学习的定义" class="headerlink" title="1、机器学习的定义"></a><strong>1、机器学习的定义</strong></h2><p><strong>对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，经过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</strong></p><p><strong>任务T：Testing Set - 测试集</strong></p><p><strong>性能度量P：Loss Function - 损失函数</strong></p><p><strong>经验E：Training Set - 训练集</strong></p><h2 id="2、Loss-Function-Cost-Function-损失函数"><a href="#2、Loss-Function-Cost-Function-损失函数" class="headerlink" title="2、Loss Function/Cost Function - 损失函数"></a><strong>2、Loss Function/Cost Function - 损失函数</strong></h2><p><strong>假设函数：</strong><br>$$<br>h_{w,b}(x) = wx+b<br>$$<br><strong>在回归任务中，多使用均方误差作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>在分类任务中，多使用交叉熵作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}[-\hat{y}^{(i)}\log(h_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-h_{w,b}(x^{(i)}))]<br>$$</p><h2 id="3、Sigmoid-Function和Softmax-Function"><a href="#3、Sigmoid-Function和Softmax-Function" class="headerlink" title="3、Sigmoid Function和Softmax Function"></a><strong>3、Sigmoid Function和Softmax Function</strong></h2><p><strong>Sigmoid Function不具体表示哪一个函数，而是表示一类S型函数，常用的有逻辑函数σ(z)：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$<br><strong>Softmax Function，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z “压缩”到另一个K维实向量 σ(z)  中，使得每一个元素的范围都在(0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：</strong><br>$$<br>\sigma(z)_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \quad j = 1,…,K<br>$$</p><h2 id="4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><a href="#4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE" class="headerlink" title="4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE"></a><strong>4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</strong></h2><p><strong>Logistic Regression的假设函数如下：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}} \quad z(x) = wx+b<br>$$</p><p><strong>σ(z)分别对w和b求导，结果为：</strong><br>$$<br>\frac{\partial \sigma(z)}{\partial w} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial w}= \sigma(z)(1-\sigma(z))*x<br>$$</p><p>$$<br>\frac{\partial \sigma(z)}{\partial b} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial b}= \sigma(z)(1-\sigma(z))<br>$$</p><p><strong>如果使用MSE作为损失函数的话，那写出来是这样的：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>当我们使用梯度下降来进行凸优化的时候，分别需要计算L(w,b)对w和b的偏导数：</strong></p><p>$$<br>\frac{\partial L(w,b)}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b)}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))<br>$$</p><p><strong>所以在σ(x)接近于1或者0的时候，也就是预测的结果和真实结果很相近或者很不相近的时候，σ(x)和1-σ(x)中总有一个会特别小，这样会导致梯度很小，从而使得优化速度大大减缓。</strong></p><p><strong>而当使用Cross Entropy作为损失函数时，损失函数为：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}(-\hat{y}^{(i)}\log(\sigma_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-\sigma_{w,b}(x^{(i)})))<br>$$</p><p><strong>L(w,b)分别对w和b求偏导，结果如下：</strong><br>$$<br>\frac{\partial L(w,b) }{\partial w}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}<br>$$</p><p>$$<br>\frac{\partial L(w,b) }{\partial b}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})<br>$$</p><p><strong>这样梯度始终和预测值与真实值之差挂钩，预测值与真实值偏离很大时，梯度也会很大，偏离很小时，梯度会很小。所以我们更倾向于使用Cross Entropy而不使用MSE。函数图如下所示：</strong><br><img src="/科大软院 - 人工智能期中考试复习/1_1.png" alt="Total Loss 函数图"></p><h2 id="5、一堆优化方法"><a href="#5、一堆优化方法" class="headerlink" title="5、一堆优化方法"></a><strong>5、一堆优化方法</strong></h2><p><strong>这堆优化方法只有一个目标：寻求合适的w和b（两个参数情况），使得L(w,b)损失函数的值最小。</strong></p><h3 id="1-Gradient-Descent-梯度下降"><a href="#1-Gradient-Descent-梯度下降" class="headerlink" title="(1) Gradient Descent - 梯度下降"></a><strong>(1) Gradient Descent - 梯度下降</strong></h3><p><strong>如果需要找到一个函数的局部极小值，必须朝着函数上当前点所对应梯度（或者是近似梯度）的反方向，前进规定步长的距离进行迭代搜索。</strong></p><p>$$<br>w’ \leftarrow w - \eta \frac{\partial L(w,b)}{\partial w}<br>$$</p><p>$$<br>b’ \leftarrow b - \eta \frac{\partial L(w,b)}{\partial b}<br>$$</p><p><strong>tips: 这里不可以将更新好的w’代入到L(w,b)中然后计算b’，参数w和b都必须等计算好后一起更新。</strong></p><p><strong>η代表学习率，部分决定了每一次更新的步长大小，如果学习率太低会导致更新十分缓慢，学习率太高又会导致步长太大全场乱跑。</strong></p><h4 id="为什么要以梯度的反方向为更新方向？"><a href="#为什么要以梯度的反方向为更新方向？" class="headerlink" title="为什么要以梯度的反方向为更新方向？"></a><strong>为什么要以梯度的反方向为更新方向？</strong></h4><p><strong>因为梯度方向是函数方向导数最大的方向，所以沿着梯度方向的反方向更新的话，函数下降的变化率最大。（感谢zzj在评论中指正）</strong></p><h3 id="2-Stochastic-Gradient-Descent-随机梯度下降"><a href="#2-Stochastic-Gradient-Descent-随机梯度下降" class="headerlink" title="(2) Stochastic Gradient Descent - 随机梯度下降"></a><strong>(2) Stochastic Gradient Descent - 随机梯度下降</strong></h3><p><strong>随机梯度下降的损失函数要这样定义：</strong></p><p>$$<br>L^{(i)}(w,b) = \frac{1}{2}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p><p><strong>对比梯度下降的损失函数，发现这里少了求和以及求平均值的过程，因为这里只有一个样本作为参考，这个损失函数也是关于这一个样本的损失函数。</strong></p><p><strong>可以这样来理解：训练集里有20个样本，现在我就当作我只拥有这20个样本的其中一个样本，然后使用这一个样本更新参数，然后再挑另一个样本，更新参数，以此类推，直到所有样本都被用完后，这一轮随机梯度下降就结束了。</strong></p><p><strong>和梯度下降比较一下，随机梯度下降的好处是：梯度下降每次更新都用到了所有的样本，也就是使用所有样本的信息进行一次参数更新。而随机梯度下降每次更新只用到了一个样本，如果这个训练集有m个样本，那么梯度下降更新一次参数，随机梯度下降已经更新了m次参数了。所以随机梯度下降的更新频率是梯度下降的m倍，更新速度更快；</strong></p><p><strong>而随机梯度下降所带来的坏处是：随机梯度下降的更新只参考了一个样本，这种参考有点管中窥豹了，是不可能顾全大局的，所以更新时候的抖动现象很明显，就是参数的前进方向乱七八糟的，但是总体来说还是向着最低点前进。</strong></p><h3 id="3-Mini-batch-Gradient-Descent-Mini-batch梯度下降"><a href="#3-Mini-batch-Gradient-Descent-Mini-batch梯度下降" class="headerlink" title="(3) Mini-batch Gradient Descent - Mini-batch梯度下降"></a><strong>(3) Mini-batch Gradient Descent - Mini-batch梯度下降</strong></h3><p><strong>Mini-batch梯度下降是梯度下降和随机梯度下降的中和版本，随机梯度下降每次更新只考虑一个样本，梯度下降每次更新考虑所有样本，而Mini-batch梯度下降每次更新所考虑的样本是可以被指定的，如果总共有m个样本，那就可以在1~m中任意指定。</strong></p><p><strong>如果每次更新时所参考的样本数合适，那么既兼顾了随机梯度下降更新速度快的特性，又兼顾了梯度下降更新的稳定性。</strong></p><p><strong>可以说梯度下降和随机梯度下降都是Mini-batch梯度下降的一个特例，使用梯度下降时，指定每次更新参考全部样本，而使用随机梯度下降时，每次更新只参考1个样本。</strong></p><p><strong>注意：虽然随机梯度下降和Mini-batch梯度下降都是基于一部分数据进行参数更新，但是更新完后查看损失函数是基于全部训练数据所得出的训练误差。</strong></p><h3 id="4-Adagrad算法"><a href="#4-Adagrad算法" class="headerlink" title="(4) Adagrad算法"></a><strong>(4) Adagrad算法</strong></h3><p><strong>像Adagrad这一类自适应算法都是使用了自适应的学习率，他们都有一个基本思路：在整个训练过程中使用同一个学习率是不合适的，因为在训练开始时，损失值肯定是比较大的，所以需要较大的学习率，而训练快要结束时，越来越接近最低点了，此时需要较小学习率。所以这类算法会依据某些因素，在迭代过程中，逐渐减小学习率。</strong></p><p><strong>比如可以按下面这个公式来设计自适应的学习率，其中t代表了迭代次数：</strong></p><p>$$<br>\eta^t=\frac{\eta}{\sqrt{t+1}}<br>$$</p><p><strong>这样学习率就会根据迭代次数来放缓学习率，从而达到学习率越来越小的目的。但是这样还是不够的，因为还要考虑到具体的函数情况。Adagrad算法不仅要求学习率跟着迭代次数变化，就是按上面的公式算出$\eta^t$，还要再除以之前所有梯度的平方平均数，$g^t$代表了第t次迭代时的梯度。</strong></p><p>$$<br>w^1 \leftarrow w^0 - \frac{\eta^0}{\sigma^0}g^0 \qquad \sigma^0 = \sqrt{(g^0)^2}<br>$$</p><p>$$<br>w^2 \leftarrow w^1 - \frac{\eta^1}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\frac{1}{2}[(g^0)^2+(g^1)^2]}<br>$$</p><p>$$<br>w^3 \leftarrow w^2 - \frac{\eta^2}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\frac{1}{3}[(g^0)^2+(g^1)^2 + (g^2)^2]}<br>$$</p><p>$$<br>……<br>$$</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}<br>$$</p><p><strong>其中有$\eta^t=\frac{\eta}{\sqrt{t+1}}​$代入到最后一个式子就可以得到：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\frac{\eta}{\sqrt{t+1}}}{\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p><p><strong>同时约去$\frac{1}{\sqrt{t+1}}$后就可以得到：</strong></p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p><p><strong>这就是Adagrad算法的公式了，我们可以看到这个式子中，学习率是会一直除以前面所有梯度的平方和再开根号的，这一定是一个大于0的数，所以学习率会越来越小。但是防止一开始的时候梯度就是0，如果让分母变为0会导致错误的，所以后面还要跟一个很小的正数$\epsilon​$，最终的式子是这样的：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2+\epsilon}}g^t<br>$$</p><p><strong>Adagrad算法也有很多不足：</strong></p><p><strong>a) 如果初始的学习率设置过大的话，这个学习率要除以一个较大梯度，那么此算法会对梯度的调节太大；</strong><br><strong>b) 在训练的中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束。</strong></p><h3 id="5-RMSprop算法"><a href="#5-RMSprop算法" class="headerlink" title="(5) *RMSprop算法"></a><strong>(5) *RMSprop算法</strong></h3><p><strong>我感觉最多考到Adagrad算法就行了，RMSprop应该考不到。</strong></p><p><strong>在凸优化问题上，Adagrad算法具有很好的效果，但是在神经网络情况下，很多问题都是非凸优化问题，即损失函数有很多局部最小值，有了Adagrad算法的改进版RMSprop算法，RMSprop算法就像是真实物理世界中一个小球在山坡上向下滑，如果滑落到一个山谷中，小球是不会立刻停在这里的，由于具有惯性的原因，小球会继续向前冲，如果惯性足够的话，可能会再次冲出山头，更有可能会落到另一个更低的山谷中。而传统的梯度下降法只会落在一个山谷中，没有机会冲出来。</strong><br>$$<br>w^1 \leftarrow w^0 - \frac{\eta}{\sigma^0}g^0 \qquad \sigma^0 = g^0<br>$$</p><p>$$<br>w^2 \leftarrow w^1 - \frac{\eta}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\alpha(\sigma^0)^2 + (1-\alpha)(g^1)^2}<br>$$</p><p>$$<br>w^3 \leftarrow w^2 - \frac{\eta}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\alpha(\sigma^1)^2 + (1-\alpha)(g^2)^2}<br>$$</p><p>$$<br>……<br>$$</p><p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1-\alpha)(g^t)^2}<br>$$</p><p><strong>Adagrad和RMSprop算法这两个算法很相近，不同之处在于RMSprop算法增加了一个衰减系数α来控制历史信息的获取多少。</strong></p><h2 id="6、Cross-Validation-交叉验证"><a href="#6、Cross-Validation-交叉验证" class="headerlink" title="6、Cross Validation - 交叉验证"></a><strong>6、Cross Validation - 交叉验证</strong></h2><p><strong>将数据集D划分成k个大小相似的互斥子集，每次用k-1个子集作为训练集，余下的子集做测试集，最终返回k个训练结果的平均值。交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_2.png" alt="cross validation"></p><h3 id="为什么要用交叉验证-Cross-Validation）"><a href="#为什么要用交叉验证-Cross-Validation）" class="headerlink" title="为什么要用交叉验证(Cross-Validation）"></a><strong>为什么要用交叉验证(Cross-Validation）</strong></h3><p><strong>a) 交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合；</strong><br><strong>b) 还可以从有限的数据中获取尽可能多的有效信息。</strong></p><p><strong>注意：交叉验证使用的仅仅是训练集！</strong></p><h2 id="7、偏差、方差、噪声以及泛化误差"><a href="#7、偏差、方差、噪声以及泛化误差" class="headerlink" title="7、偏差、方差、噪声以及泛化误差"></a><strong>7、偏差、方差、噪声以及泛化误差</strong></h2><h3 id="1-偏差"><a href="#1-偏差" class="headerlink" title="(1) 偏差"></a><strong>(1) 偏差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_3.png" alt="bias"></p><p><strong>偏差bias： 期望预测与真实标记的误差，偏差越大偏离理论值越大。</strong></p><p><strong>上面的ppt里，左边的图模型阶数小，所以不可能很好的拟合真实情况，所作出的预测与真实情况偏离程度大，所以偏差就大；而右边的图里模型很复杂，足够模拟出真实情况，所以与真实情况偏离程度小，偏差就小。</strong></p><p><strong>在一个训练集$D$上模型$f$对测试样本$x$预测输出为$f(x;D)$, 那么学习算法$f$对测试样本$x$的期望预测为：</strong></p><p>$$<br>\bar{f}(x)=E_D[f(x;D)]<br>$$<br><strong>这里用偏差的平方来表示偏差的计算公式：</strong></p><p>$$<br>Bias^2(x)=(\bar{f}(x)-\hat{y})^2<br>$$</p><h3 id="2-方差"><a href="#2-方差" class="headerlink" title="(2) 方差"></a><strong>(2) 方差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_4.png" alt="variance"></p><p><strong>方差variance：预测模型的离散程度，方差越大离散程度越大。</strong></p><p><strong>就像上面ppt的左图，因为模型阶数小，所以模型都很集中；而右边的模型阶数大，有时能拟合的很好，有时拟合的不好，不稳定因素太大了。</strong></p><p><strong>使用样本数相同的不同训练集产生的方差为:</strong></p><p>$$<br>var(x)=E_D[(f(x;D)-\bar{f}(x))^2]<br>$$</p><h3 id="3-噪声"><a href="#3-噪声" class="headerlink" title="(3) *噪声"></a><strong>(3) *噪声</strong></h3><p><strong>不可能考</strong></p><p><strong>噪声为真实标记与数据集中的实际标记间的偏差$y_D$表示在数据集中的标记，$\hat{y}$表示真实标记，这两个可能不等）：</strong></p><p>$$<br>\epsilon=E_D[(y_D-\hat{y})^2]<br>$$</p><h3 id="4-泛化误差"><a href="#4-泛化误差" class="headerlink" title="(4) *泛化误差"></a><strong>(4) *泛化误差</strong></h3><p><strong>也不可能考</strong></p><p><strong>学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). 在估计学习算法性能的过程中, 我们主要关注偏差与方差。因为噪声属于不可约减的误差 (irreducible error)。</strong></p><p><strong>下面来用公式推导泛化误差与偏差与方差, 噪声之间的关系。</strong></p><p><strong>以回归任务为例, 学习算法的平方预测误差期望为：</strong></p><p>$$<br>Err(x)=E_D[(y_D-f(x;D))^2]<br>$$</p><p><strong>对算法的期望泛化误差进行分解，就会发现 泛化误差=偏差的平方+方差+噪声:</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_5.png" alt="1_5"></p><p><img src="/科大软院 - 人工智能期中考试复习/1_6.png" alt="1_6"></p><h2 id="8、欠拟合和过拟合"><a href="#8、欠拟合和过拟合" class="headerlink" title="8、欠拟合和过拟合"></a><strong>8、欠拟合和过拟合</strong></h2><p><strong>欠拟合：模型拟合不够，在训练集上拟合情况很差。往往会出现偏差大、方差小的情况；</strong></p><p><strong>过拟合：模型过度拟合，在训练集上拟合情况很好，但是在测试集上拟合情况很差。往往会出现偏差小、方差大的情况。</strong></p><p><strong>出现欠拟合时，解决办法有：</strong></p><p><strong>a) 增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间;</strong><br><strong>b) 尝试非线性模型，比如核SVM 、决策树、DNN等模型;</strong><br><strong>c) 如果有正则项可以减小正则项参数λ；</strong><br><strong>d) Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。</strong></p><p><strong>出现过拟合时，解决办法有：</strong></p><p><strong>a) 交叉检验，通过交叉检验得到较优的模型参数;</strong><br><strong>b) 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</strong><br><strong>c) 正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</strong><br><strong>d) 如果有正则项则可以考虑增大正则项参数 λ;</strong><br><strong>e) 增加训练数据可以有限的避免过拟合;</strong><br><strong>f) Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等。</strong></p><h2 id="9、L1范数和L2范数"><a href="#9、L1范数和L2范数" class="headerlink" title="9、L1范数和L2范数"></a><strong>9、L1范数和L2范数</strong></h2><p><strong>本来以为会很快写完这个部分的，没想到这里的知识点看了一晚上，更没想到这里的知识完完整整拿出来，完全可以写一篇长长的博文。如果想好好学习这里的知识的话，我推荐<a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">这个博客</a>，或者是周老师的西瓜书第252页。这里我就介绍一点最基本的知识了。</strong></p><p><strong>L1范数：向量元素绝对值之和，即：</strong></p><p>$$<br>\left | \theta \right |_1=\sum_{i=1}^{n}\left | \theta_i \right |<br>$$</p><p><strong>L2范数：这里我查了好久的资料，我发现网上所有的资料，包括周老师的西瓜书以及邱锡鹏教授的《神经网络与深度学习》都是说，L2范数是各个元素的平方和再求平方根，即：</strong></p><p>$$<br>\left | \theta \right |_2=\sqrt{\sum_{i=1}^{n}\theta_i^2}<br>$$</p><p><strong>但是吴恩达教授、李宏毅教授课上讲的版本，以及我在学校课上（我们学校的课不就是照搬李宏毅教授的课嘛）所记录的版本，都是写的L2范数为各个元素的平方和，不再求平方根，即：</strong></p><p>$$<br>\left | \theta \right |_2=\sum_{i=1}^{n}\theta_i^2<br>$$</p><p><strong>吴恩达教授：</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_7.png" alt="1_7"></p><p><strong>李宏毅教授：</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_8.png" alt="1_8"></p><p><strong>我的笔记就不拿出来了，和前两位的差不多。在这里我还是使用这个不开根号的版本。</strong></p><h3 id="1-L1范数的作用"><a href="#1-L1范数的作用" class="headerlink" title="(1) L1范数的作用"></a><strong>(1) L1范数的作用</strong></h3><p><strong>L1范数可以产生稀疏权值矩阵，一定程度上也可以防止过拟合。</strong></p><p><strong>稀疏权值矩阵的意思是，这个矩阵中大部分元素都是0，只有很小一部分元素不为0。试想一下当我们在看周杰伦演唱会时，只会把注意力放在周董身上，不太会关注伴舞小姐姐，更不可能去关注旁边又矮又胖的音响（这一句考试别写）。而在计算机视觉领域也一样，传进来一张图片，这个图片有很多很多的像素，但是机器真正要关注的只有其中一部分元素，比如一张田地里的农民，或是草地里的狗狗之类的，其他不重要的信息就直接不看了，所以和主题无关的信息都会乘上0，有意义的信息才会保留。自然语言处理也是一样，有很多无意义的词都会乘上0，只挑选有意义的信息保留下来。关于具体怎么实现的不讲了，去看周老师的西瓜书吧。</strong></p><h3 id="2-L2范数的作用"><a href="#2-L2范数的作用" class="headerlink" title="(2) L2范数的作用"></a><strong>(2) L2范数的作用</strong></h3><p><strong>L2范数主要用于防止模型过拟合。</strong></p><p><strong>L2范数比较重要一点，L2范数的作用是权值衰减，缩小各个权值，使得函数尽可能比不加L2范数平滑很多，增大模型的泛化能力。如果损失函数加上L2范数后，是这样：</strong></p><p>$$<br>L(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]<br>$$</p><p><strong>当使用梯度下降时，得出的式子是这样的：</strong><br>$$<br>\theta_j’ \leftarrow (1-\eta\frac{\lambda}{m})\theta_j - \eta\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})x^{(i)}<br>$$</p><p><strong>每次参数更新时，都要先把原来的参数乘上一个介于0和1的数，所以参数都会先缩小一点点，再进行更新。一般来说$1-\eta\frac{\lambda}{m}$都会是一个很接近1的数，别看一个数打九九折后没什么变化，可是迭代次数多了后，这个权值是会衰减的很严重的。通过这种方法会有效降低权值，使得函数更为平滑，使得模型的泛化能力更强。</strong></p><h2 id="10、优化技巧"><a href="#10、优化技巧" class="headerlink" title="10、优化技巧"></a><strong>10、优化技巧</strong></h2><p><strong>训练集中常常有些特征的范围差别很大，这样会导致在优化时，不同的参数优化速度差别很大。比如房子的大小从10~2000平米都有可能，而房间数只能处在1~10这个范围内，在进行优化时，房间数的参数很快就找到了最低点，而房子大小的参数还离最低点很远，这样的话，房间数的参数就在最低点来回波动，等待房子大小的参数优化好。如果能尽量让所有参数同时优化好，那么会大大提高优化速度。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1_优化.png" alt="1_优化"></p><h3 id="1-Feature-Scaling-特征缩放-归一化"><a href="#1-Feature-Scaling-特征缩放-归一化" class="headerlink" title="(1) Feature Scaling - 特征缩放/归一化"></a><strong>(1) Feature Scaling - 特征缩放/归一化</strong></h3><p><strong>归一化的思路是：让输入的值减去样本中最小的值，然后除以样本范围（也就是样本最大值减去样本最小值），这样会使得结果永远在0~1的范围内，所有特征参数差不多一个速度优化到最低点。式子如下：</strong></p><p>$$<br>x’=\frac{x-min}{max-min}<br>$$</p><h3 id="2-Mean-Normalization-均值标准化"><a href="#2-Mean-Normalization-均值标准化" class="headerlink" title="(2) Mean Normalization - 均值标准化"></a><strong>(2) Mean Normalization - 均值标准化</strong></h3><p><strong>均值标准化的思路是：让输入的值减去样本平均数μ，再除以样本标准差σ。经过这样的处理，数据的均值会是0，大小在-1~1之间。均值标准化和归一化一样，也有去除不同特征量纲不同的问题，另外机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布，所以数据以0为中心左右分布会带来很多便利。</strong><br>$$<br>{x}’=\frac{x-\mu}{\sigma}<br>$$</p><h2 id="11、怎么调节学习率"><a href="#11、怎么调节学习率" class="headerlink" title="11、怎么调节学习率"></a><strong>11、怎么调节学习率</strong></h2><p><strong>当我们在训练过程中，发现loss下降的很慢时，可以适当增大学习率；发现loss不降反增的时候，要降低学习率。</strong></p><p><strong>如果我们使用的是Adagrad算法，那么一开始学习率可以设置的相对大一点。</strong></p><h2 id="12、逻辑回归的局限性-深度神经网络的兴起导火索"><a href="#12、逻辑回归的局限性-深度神经网络的兴起导火索" class="headerlink" title="12、逻辑回归的局限性 - 深度神经网络的兴起导火索"></a><strong>12、逻辑回归的局限性 - 深度神经网络的兴起导火索</strong></h2><p><strong>逻辑回归不可以直接处理线性不可分的问题，例如下图所示的异或问题。处理方法是先通过一个函数，将原来的值投影转换，变成线性可分问题，再使用逻辑回归来处理。这一步叫做特征变换，先进行特征变换，再进行逻辑回归，这就是两层神经网络的雏形，也是神经网络的兴起的导火索。</strong></p><p><img src="/科大软院 - 人工智能期中考试复习/1深度学习导火索.jpg" alt="1深度学习导火索"></p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dnn </tag>
            
            <tag> review for exam </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于TensorFlow的CAPTCHA注册码识别实验</title>
      <link href="/%E5%9F%BA%E4%BA%8ETensorFlow%E7%9A%84CAPTCHA%E6%B3%A8%E5%86%8C%E7%A0%81%E8%AF%86%E5%88%AB%E5%AE%9E%E9%AA%8C/"/>
      <url>/%E5%9F%BA%E4%BA%8ETensorFlow%E7%9A%84CAPTCHA%E6%B3%A8%E5%86%8C%E7%A0%81%E8%AF%86%E5%88%AB%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<ul><li>实验任务：使用TensorFlow实现CAPTCHA注册码的识别</li><li>基本思路：采用 Captcha 库生成验证码，将验证码识别问题转化为分类问题，采用 CNN 网络模型进行训练，最终实现对验证码的破解。</li><li>实验步骤：获取验证码训练集 → 构建卷积神经网络和全连接神经网络 → 定义损失函数以及优化方式 → 进行训练并保存训练结果。</li></ul><a id="more"></a><h2 id="获取验证码训练集"><a href="#获取验证码训练集" class="headerlink" title="获取验证码训练集"></a>获取验证码训练集</h2><p>使用captcha库来生成验证码，captcha库可以使用anaconda很轻松下载，captcha库不仅可以用于生成图片验证码，还可以用来生成语音验证码。</p><ol><li>我们将ImageCaptcha类实例化为image对象，构造函数的参数里指定宽度和高度。</li></ol><p>image = ImageCaptcha(width=self.width, height=self.height)</p><ol start="2"><li><p>存储训练数据。</p><p>我们创建两个矩阵X和Y用以表示训练集的输入部分和输出部分，也就是验证码的图片数据存储以及验证码上所写的内容存储。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.zeros([batch_size, self.height, self.width, <span class="number">1</span>])</span><br><span class="line">Y = np.zeros([batch_size, self.char_num, self.classes])</span><br></pre></td></tr></table></figure></div><p>这两个矩阵的第一维参数都是batch_size，表示这一组训练集的个数。</p><p>X的第二和第三位参数分别表示验证码的高度和宽度，最后一个参数表示这是1通道的黑白图片。</p><p>Y的第二位参数表示此验证码上写了几个字符，第三位参数表示每个字符共有多少种选择，这是一条长为self.classes的向量，以独热编码的形式记录信息。</p></li><li><p>生成验证码</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        captcha_str = <span class="string">''</span>.join(random.sample(self.characters, self.char_num))</span><br><span class="line">        img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br><span class="line">        img = np.array(img.getdata())</span><br><span class="line">        X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">            Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br><span class="line">    Y = np.reshape(Y, (batch_size, self.char_num * self.classes))</span><br><span class="line">    <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure></div><p>首先，这个总体的无限循环是一个训练集的生成器，执行此代码后，会在最后的yield语句返回训练集X和Y，然后循环结束。下次再想生成验证码训练集时，会从yield语句（最后一句）开始，回到开头再执行一次循环。</p><p>这样的好处是，以前都是将所有的训练数据全部准备好，然后手动分出很多batch，一个一个的训练，这样的话要求把所有的训练数据全部装入内存，很浪费内存空间。而使用这种生成器语句，如果我训练哪一个batch的数据，就立即生成数据并装入内存，用完再撤出内存，紧接着训练下一个batch的数据时，再立即生成然后将其装入内存。分批装入内存，节省了大量的内存空间。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random.sample(self.characters, self.char_num)</span><br></pre></td></tr></table></figure></div><p>此代码生成一个验证码字符串的随机变量，self.characters为62位的字符串（0~9A~Za~z），self.char_num=4（生成4个字符）。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br></pre></td></tr></table></figure></div><p>这句代码使用的是ImageCaptcha类的内置方法，将字符串变为图片。convert(‘L’)：表示生成的是灰度图片，就是通道数为1的黑白图片。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure></div><p>每个像素值都要除以255，这是为了归一化处理，因为灰度的范围是0~255，这里除以255就让每个像素的值在0~1之间，目的是为了加快收敛速度。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">    Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br></pre></td></tr></table></figure></div><p>这里用以生成对应的测试集Y，j和ch用以遍历刚刚生成的随机字符串，j记录index（0~3，表示第几个字符），ch记录字符串中的字符。找到Y的第i条数据中的第j个字符，然后把62长度的向量和ch相关的那个置为1。</p></li></ol><h2 id="构建卷积神经网络和全连接神经网络"><a href="#构建卷积神经网络和全连接神经网络" class="headerlink" title="构建卷积神经网络和全连接神经网络"></a>构建卷积神经网络和全连接神经网络</h2><ol><li><p>conv2D</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self, x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></div><p>这是2维卷积函数。x表示传入的待处理图片，W表示卷积核，strides=[1, 1, 1, 1]，其中第二个和第三个1分别表示x方向步长和y方向步长，padding=’SAME’表示边界处理策略设为’SAME’，这样卷积处理完图片大小不变。</p></li><li><p>max_pool_2x2</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></div><p>这是2x2最大值池化函数。x表示待被池化处理的图片，ksize=[1, 2, 2, 1]，其中第二个和第三个2分别表示池化窗口高度和池化窗口宽度，strides和padding意义同上。</p></li><li><p>weight_variable</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></div><p>这就是一个很有意思的函数了，用这个函数的目的是从<strong>截断</strong>的正态分布中输出随机值。听着名字就不知道是个什么鬼，我们先看看正太分布的一个性质：</p><p>在正态分布的曲线中，横轴区间（μ-σ，μ+σ）内的面积为68.268949%。</p><p>横轴区间（μ-2σ，μ+2σ）内的面积为95.449974%。</p><p>横轴区间（μ-3σ，μ+3σ）内的面积为99.730020%。</p><p>X落在（μ-3σ，μ+3σ）以外的概率小于千分之三，在实际问题中常认为相应的事件是不会发生的，基本上可以把区间（μ-3σ，μ+3σ）看作是随机变量X实际可能的取值区间，这称之为正态分布的“3σ”原则。</p><p><img src="/基于TensorFlow的CAPTCHA注册码识别实验/captcha_1.jpg" alt="正态分布"></p><p>而从<strong>截断</strong>的正态分布中输出随机值的意思就是，同样也是随机生成的值，但是生成的值必须服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。换句话说，就是如果随机生成的值如果落在了(μ-2σ,μ+2σ)之外，就重新生成值，这样就保证了随机生成的值都在均值附近。</p><p>扯了这么多，其实就是随机生成一组数，要求不要离中心点μ太远。</p></li><li><p>bias_variable</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></div><p>生成一组全部都是0.1的常量数，没啥好说的。</p></li><li><p>构建第一层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv1 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = self.bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(tf.nn.bias_add(self.conv2d(x_images, w_conv1), b_conv1))</span><br><span class="line">h_pool1 = self.max_pool_2x2(h_conv1)</span><br><span class="line">h_dropout1 = tf.nn.dropout(h_pool1, keep_prob)</span><br><span class="line">conv_width = math.ceil(self.width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(self.height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv1是卷积核，可以理解为一共有32个卷积核，每个卷积核的尺寸是(5, 5, 1)，即长度和宽度都是5，通道是1。每个卷积核对图片处理完就会产生一张特征图，32个卷积核对图片处理完后就会产生32个特征图，将这些特征图叠加排列，那么原本通道数为1的图片现在通道数变为图片的个数，也就是32。图片的尺寸变化为(?, 60, 160, 1) –&gt; (?, 60, 160, 32)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 60, 160, 32) → (?, 30, 80, 32)。</p><p>随后又进行了一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第二层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv2 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout1, w_conv2), b_conv2))</span><br><span class="line">h_pool2 = self.max_pool_2x2(h_conv2)</span><br><span class="line">h_dropout2 = tf.nn.dropout(h_pool2, keep_prob)</span><br><span class="line">conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv2是第二层卷积神经网络的卷积核，共有64个，每个卷积核的尺寸是(5, 5, 32)，处理之后图片的尺寸变化为(?, 30, 80, 32) → (?, 30, 80, 64)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 30, 80, 64) → (?, 15, 40, 64)。</p><p>再进行一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第三层卷积神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_conv3 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b_conv3 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv3 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout2, w_conv3), b_conv3))</span><br><span class="line">h_pool3 = self.max_pool_2x2(h_conv3)</span><br><span class="line">h_dropout3 = tf.nn.dropout(h_pool3, keep_prob)</span><br><span class="line">conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br></pre></td></tr></table></figure></div><p>w_conv3是第三层卷积神经网络的卷积核，共有64个，每个卷积核的尺寸是(5, 5, 64)，处理之后图片的尺寸变化为(?, 15, 40, 64) → (?, 15, 40, 64)。</p><p>随后又对图片进行一次池化处理，池化窗口为2×2，所以图片的长度和宽度都会变为原来的一半。图片的尺寸变化为(?, 15, 40, 64) → (?, 8, 20, 64)，这里的15 / 2 = 8，是因为边界策略为SAME，那么遇到剩下还有不足4个像素的时候同样采取一次最大值池化处理。</p><p>再进行一次dropout以防止过拟合，同时也是为了加大个别神经元的训练强度。</p></li><li><p>构建第一层全连接神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conv_width = int(conv_width)</span><br><span class="line">conv_height = int(conv_height)</span><br><span class="line">w_fc1 = self.weight_variable([<span class="number">64</span> * conv_width * conv_height, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = self.bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_dropout3_flat = tf.reshape(h_dropout3, [<span class="number">-1</span>, <span class="number">64</span> * conv_width * conv_height])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_dropout3_flat, w_fc1), b_fc1))</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure></div><p>这里就把刚刚卷积神经网络的输出作为传统神经网络的输入了，w_fc1(10240, 1024)和b_fc1(1024)分别是这一层神经网络的参数以及bias。上面代码第五行将卷积神经网络的输出数据由(?, 8, 20, 64)转为了(?, 64 <em> 20 </em> 8)，可以很明显感觉出来把所有的数据拉成了一条一维向量，然后经过矩阵处理，这里的数据变为了(1024, 1)的形状。</p></li><li><p>构建第二层全连接神经网络</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w_fc2 = self.weight_variable([<span class="number">1024</span>, self.char_num * self.classes])</span><br><span class="line">b_fc2 = self.bias_variable([self.char_num * self.classes])</span><br><span class="line">y_conv = tf.add(tf.matmul(h_fc1_drop, w_fc2), b_fc2)</span><br></pre></td></tr></table></figure></div><p>再连接一次神经网络，这次不再需要添加激励函数了ReLu了，因为已经到达输出层，线性相加后直接输出就可以了，结果保存在y_conv变量里，最后将y_conv返回给调用函数。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_conv = model.create_model(x, keep_prob)</span><br></pre></td></tr></table></figure></div><p>这是外层函数调用model后所得到的训练结果。</p></li></ol><h2 id="定义损失函数以及优化方式"><a href="#定义损失函数以及优化方式" class="headerlink" title="定义损失函数以及优化方式"></a>定义损失函数以及优化方式</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br></pre></td></tr></table></figure></div><p>由于识别验证码本质上是对验证码中的信息进行分类，所以我们这里使用cross_entropy的方法来衡量损失。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></div><p>优化方式选择的是AdamOptimizer，学习率设置比较小，为1e-4，防止学习的太快而训练不好。</p><h2 id="进行训练并保存训练结果"><a href="#进行训练并保存训练结果" class="headerlink" title="进行训练并保存训练结果"></a>进行训练并保存训练结果</h2><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br><span class="line">        _, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line">        print(<span class="string">'step:%d,loss:%f'</span> % (step, loss))</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">            acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">            <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">                saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        step += <span class="number">1</span></span><br></pre></td></tr></table></figure></div><p>从第二行开始进入训练部分，首先我们需要初始化变量，然后进入一个循环，直到训练的准确度高于99%才会停止训练，否则会一直训练下去。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br></pre></td></tr></table></figure></div><p>用于生成训练集，每次生成64条训练数据，输入部分保存在batch_x里，输出部分保存在batch_y里，每次循环到这里就会再次读入64条训练数据。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure></div><p>开始训练，loss记录损失，_变量不记录任何东西，我debug下来每次_里面都是空，可能只是为了语法的需要，这里必须有个变量接受点什么。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">    acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">    print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">    <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">        saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure></div><p>每训练一百次，就会进行一次Test。具体操作是，生成100条测试数据，和之前生成训练数据是一个方法，然后直接进行测试，如果准确度高于99%那么就停止训练，把训练好的模型放在”capcha_model.ckpt”这个文件里面，可以供以后使用。如果没有达到99%的准确度就继续训练。</p><h2 id="六、完整代码"><a href="#六、完整代码" class="headerlink" title="六、完整代码"></a>六、完整代码</h2><h3 id="train-captcha-py"><a href="#train-captcha-py" class="headerlink" title="train_captcha.py"></a>train_captcha.py</h3><p>训练的主代码</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> generate_captcha</span><br><span class="line"><span class="keyword">import</span> captcha_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    captcha = generate_captcha.GenerateCaptcha()  <span class="comment"># 创建一个验证码对象</span></span><br><span class="line">    width, height, char_num, characters, classes = captcha.get_parameter()  <span class="comment"># 获取此验证码对象的各种属性</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># x：训练集输入占位符</span></span><br><span class="line">    <span class="comment"># y：训练集输出占位符</span></span><br><span class="line">    <span class="comment"># keep_prob：dropout处理保留此神经元的概率占位符</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, height, width, <span class="number">1</span>])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, char_num * classes])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成训练模型对象</span></span><br><span class="line">    model = captcha_model.CaptchaModel(width, height, char_num, classes)</span><br><span class="line">    <span class="comment"># y_conv: 模型训练完的结果</span></span><br><span class="line">    y_conv = model.create_model(x, keep_prob)</span><br><span class="line">    <span class="comment"># 损失函数，用cross_entropy来描述损失</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line">    <span class="comment"># 优化器，选用AdamOptimizer</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将训练完的结果reshape为(?, 4, 62)的形状</span></span><br><span class="line">    predict = tf.reshape(y_conv, [<span class="number">-1</span>, char_num, classes])</span><br><span class="line">    <span class="comment"># 将真实值也reshape为(?, 4, 62)的形状</span></span><br><span class="line">    real = tf.reshape(y_, [<span class="number">-1</span>, char_num, classes])</span><br><span class="line">    <span class="comment"># 计算预测正确的个数</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(predict, <span class="number">2</span>), tf.argmax(real, <span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 转型为浮点数格式</span></span><br><span class="line">    correct_prediction = tf.cast(correct_prediction, tf.float32)</span><br><span class="line">    <span class="comment"># 求均值，就是准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(correct_prediction)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 等模型训练好后，可以用saver保存模型，便于以后使用</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># batch_x和batch_y存储64条训练数据的输入和输出值</span></span><br><span class="line">            batch_x, batch_y = next(captcha.gen_captcha(<span class="number">64</span>))</span><br><span class="line">            <span class="comment"># loss存储本次训练的损失值</span></span><br><span class="line">            _, loss = sess.run([train_step, cross_entropy], feed_dict=&#123;x: batch_x, y_: batch_y, keep_prob: <span class="number">0.75</span>&#125;)</span><br><span class="line">            print(<span class="string">'step:%d,loss:%f'</span> % (step, loss))</span><br><span class="line">            <span class="comment"># 每进行100次，就进行一次测试</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 获取100条测试数据</span></span><br><span class="line">                batch_x_test, batch_y_test = next(captcha.gen_captcha(<span class="number">100</span>))</span><br><span class="line">                acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y_: batch_y_test, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">                print(<span class="string">'###############################################step:%d,accuracy:%f'</span> % (step, acc))</span><br><span class="line">                <span class="comment"># 如果测试准确度在99%以上就退出，否则继续训练</span></span><br><span class="line">                <span class="keyword">if</span> acc &gt; <span class="number">0.99</span>:</span><br><span class="line">                    saver.save(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            step += <span class="number">1</span></span><br></pre></td></tr></table></figure></div><h3 id="generate-captcha-py"><a href="#generate-captcha-py" class="headerlink" title="generate_captcha.py"></a>generate_captcha.py</h3><p>生成验证码的类</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> captcha.image <span class="keyword">import</span> ImageCaptcha</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GenerateCaptcha</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 width=<span class="number">160</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 height=<span class="number">60</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 char_num=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 characters=string.digits + string.ascii_uppercase + string.ascii_lowercase)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        GenerateCaptcha类的构造函数</span></span><br><span class="line"><span class="string">        :param width: 验证码图片的宽</span></span><br><span class="line"><span class="string">        :param height: 验证码图片的高</span></span><br><span class="line"><span class="string">        :param char_num: 验证码字符个数</span></span><br><span class="line"><span class="string">        :param characters: 验证码组成字符串，其中包含总数字串(0~9)+大写字母串(A~Z)+小写字母串(a~z)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">        self.char_num = char_num</span><br><span class="line">        self.characters = characters</span><br><span class="line">        self.classes = len(characters)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_captcha</span><span class="params">(self, batch_size=<span class="number">50</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        生成一组验证码训练数据</span></span><br><span class="line"><span class="string">        :param batch_size:用以指定这一组训练数据的个数</span></span><br><span class="line"><span class="string">        :return:X（这一组训练数据的输入X）, Y（这一组训练数据的输出Y）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 验证码图片对象，这里只限定了尺寸，还没有写内容</span></span><br><span class="line">        image = ImageCaptcha(width=self.width, height=self.height)</span><br><span class="line">        <span class="comment"># X:用以存放训练数据的输入部分</span></span><br><span class="line">        <span class="comment"># batch_size: 这一组训练数据共有batch_size个</span></span><br><span class="line">        <span class="comment"># self.height: 每一个训练数据（验证码）的高度</span></span><br><span class="line">        <span class="comment"># self.width: 每一个训练数据（验证码）的宽度</span></span><br><span class="line">        <span class="comment"># 1: 每一个训练数据（验证码）的通道数，因为是黑白图片所以为1</span></span><br><span class="line">        X = np.zeros([batch_size, self.height, self.width, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Y:用以存放训练数据的输出部分</span></span><br><span class="line">        <span class="comment"># batch_size: 这一组训练数据共有batch_size个</span></span><br><span class="line">        <span class="comment"># self.char_num: 每一个训练数据（验证码）的内容中包含几个字符</span></span><br><span class="line">        <span class="comment"># self.classes: 这个字符的内容是什么（这是一个长度为62的向量，因为10个数字加26个大写字母加26个小写字母长度为62，只有一个值为1，表示这个字符内容，其他都是0）</span></span><br><span class="line">        Y = np.zeros([batch_size, self.char_num, self.classes])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">                <span class="comment"># random.sample(self.characters, self.char_num)用于生成一个验证码字符串随机变量</span></span><br><span class="line">                <span class="comment"># self.characters：62位字符串（0~9A~Za~z）</span></span><br><span class="line">                <span class="comment"># self.char_num：4（生成4个字符）</span></span><br><span class="line">                <span class="comment"># captcha_str用以将转好的随机变量转为字符串类型</span></span><br><span class="line">                captcha_str = <span class="string">''</span>.join(random.sample(self.characters, self.char_num))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 用内置方法，将字符串变为图片</span></span><br><span class="line">                <span class="comment"># convert('L')：表示生成的是灰度图片，就是通道数为1的黑白图片</span></span><br><span class="line">                img = image.generate_image(captcha_str).convert(<span class="string">'L'</span>)</span><br><span class="line">                <span class="comment"># 将生成好的图片数据转成np.array类型</span></span><br><span class="line">                img = np.array(img.getdata())</span><br><span class="line">                <span class="comment"># reshape一下此图片的格式，让他的高度和宽度符合要求</span></span><br><span class="line">                <span class="comment"># 每个像素值都要除以255，这是为了归一化处理，因为灰度的范围是0~255，这里除以255就让每个像素的值在0~1之间，加快收敛速度</span></span><br><span class="line">                <span class="comment"># 将此图片信息放入X的第i个位置，表示这是训练集的第i条数据</span></span><br><span class="line">                X[i] = np.reshape(img, [self.height, self.width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 这里用以生成对应的测试集</span></span><br><span class="line">                <span class="comment"># j和ch用以遍历刚刚生成的随机字符串，j记录index（0~3，表示第几个字符），ch记录字符串中的字符</span></span><br><span class="line">                <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(captcha_str):</span><br><span class="line">                    <span class="comment"># 找到Y的第i条数据中，第j个字符，然后把62长度的向量和ch相关的那个置位1</span></span><br><span class="line">                    Y[i, j, self.characters.find(ch)] = <span class="number">1</span></span><br><span class="line">            <span class="comment"># 重新整理一下Y，第一维度依然是batch_size，后面把char_num和classes相乘，整合成一条向量</span></span><br><span class="line">            Y = np.reshape(Y, (batch_size, self.char_num * self.classes))</span><br><span class="line">            <span class="keyword">yield</span> X, Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_parameter</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.width, self.height, self.char_num, self.characters, self.classes</span><br></pre></td></tr></table></figure></div><h3 id="captcha-model-py"><a href="#captcha-model-py" class="headerlink" title="captcha_model.py"></a>captcha_model.py</h3><p>训练模型的类</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 width=<span class="number">160</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 height=<span class="number">60</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 char_num=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 classes=<span class="number">62</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        训练模型类的构造函数</span></span><br><span class="line"><span class="string">        :param width: 验证码宽度</span></span><br><span class="line"><span class="string">        :param height: 验证码高度</span></span><br><span class="line"><span class="string">        :param char_num: 验证码内字符数</span></span><br><span class="line"><span class="string">        :param classes: 验证码待选字符个数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">        self.char_num = char_num</span><br><span class="line">        self.classes = classes</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self, x, W)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        2维卷积函数</span></span><br><span class="line"><span class="string">        strides=[1, 1（x方向步长）, 1（y方向步长）, 1]</span></span><br><span class="line"><span class="string">        padding='SAME'（边界处理策略，设为'SAME'卷积处理完图片大小不变）</span></span><br><span class="line"><span class="string">        :param x: 被卷积处理的图片</span></span><br><span class="line"><span class="string">        :param W: 卷积核</span></span><br><span class="line"><span class="string">        :return: 卷积处理完的图片</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        2x2最大值池化函数</span></span><br><span class="line"><span class="string">        ksize=[1, 2（池化窗口高度）, 2(池化窗口高度), 1]</span></span><br><span class="line"><span class="string">        strides=[1, 1（x方向步长）, 1（y方向步长）, 1]</span></span><br><span class="line"><span class="string">        padding='SAME'</span></span><br><span class="line"><span class="string">        :param x: 待被池化处理的图片</span></span><br><span class="line"><span class="string">        :return: 处理好的图片</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="comment"># 截断式正态分布生成变量</span></span><br><span class="line">        initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self, shape)</span>:</span></span><br><span class="line">        <span class="comment"># 生成一组常量</span></span><br><span class="line">        initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">        <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(self, x_images, keep_prob)</span>:</span></span><br><span class="line">        <span class="comment"># first layer</span></span><br><span class="line">        w_conv1 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">        b_conv1 = self.bias_variable([<span class="number">32</span>])</span><br><span class="line">        <span class="comment"># 第一次卷积 (?, 60, 160, 1) --&gt; (?, 60, 160, 32)</span></span><br><span class="line">        h_conv1 = tf.nn.relu(tf.nn.bias_add(self.conv2d(x_images, w_conv1), b_conv1))</span><br><span class="line">        <span class="comment"># 第一次池化 (?, 60, 160, 32) --&gt; (?, 30, 80, 32)</span></span><br><span class="line">        h_pool1 = self.max_pool_2x2(h_conv1)</span><br><span class="line">        <span class="comment"># 第一层dropout</span></span><br><span class="line">        h_dropout1 = tf.nn.dropout(h_pool1, keep_prob)</span><br><span class="line">        conv_width = math.ceil(self.width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(self.height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># second layer</span></span><br><span class="line">        w_conv2 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">        b_conv2 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">        <span class="comment"># 第二次卷积 (?, 30, 80, 32) --&gt; (?, 30, 80, 64)</span></span><br><span class="line">        h_conv2 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout1, w_conv2), b_conv2))</span><br><span class="line">        <span class="comment"># 第二次池化 (?, 30, 80, 64) --&gt; (?, 15, 40, 64)</span></span><br><span class="line">        h_pool2 = self.max_pool_2x2(h_conv2)</span><br><span class="line">        <span class="comment"># 第二层dropout</span></span><br><span class="line">        h_dropout2 = tf.nn.dropout(h_pool2, keep_prob)</span><br><span class="line">        conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># third layer</span></span><br><span class="line">        w_conv3 = self.weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">        b_conv3 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line">        <span class="comment"># 第三次卷积 (?, 15, 40, 64) --&gt; (?, 15, 40, 64)</span></span><br><span class="line">        h_conv3 = tf.nn.relu(tf.nn.bias_add(self.conv2d(h_dropout2, w_conv3), b_conv3))</span><br><span class="line">        <span class="comment"># 第三次池化 (?, 15, 40, 64) --&gt; (?, 8, 20, 64)</span></span><br><span class="line">        h_pool3 = self.max_pool_2x2(h_conv3)</span><br><span class="line">        <span class="comment"># 第三层dropout</span></span><br><span class="line">        h_dropout3 = tf.nn.dropout(h_pool3, keep_prob)</span><br><span class="line">        conv_width = math.ceil(conv_width / <span class="number">2</span>)</span><br><span class="line">        conv_height = math.ceil(conv_height / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一层全连接神经网络</span></span><br><span class="line">        conv_width = int(conv_width)</span><br><span class="line">        conv_height = int(conv_height)</span><br><span class="line">        <span class="comment"># 第一层权值 (64 * 20 * 8, 1024)</span></span><br><span class="line">        w_fc1 = self.weight_variable([<span class="number">64</span> * conv_width * conv_height, <span class="number">1024</span>])</span><br><span class="line">        <span class="comment"># 第一层bias (1, 1024)</span></span><br><span class="line">        b_fc1 = self.bias_variable([<span class="number">1024</span>])</span><br><span class="line">        <span class="comment"># 将CNN的输出结果展开成一条线，shape为(64 * 20 * 8, 1)</span></span><br><span class="line">        h_dropout3_flat = tf.reshape(h_dropout3, [<span class="number">-1</span>, <span class="number">64</span> * conv_width * conv_height])</span><br><span class="line">        <span class="comment"># 神经网络处理，激发函数选择relu (64 * 20 * 8, 1) --&gt; (1024, 1)</span></span><br><span class="line">        h_fc1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h_dropout3_flat, w_fc1), b_fc1))</span><br><span class="line">        <span class="comment"># dropout处理</span></span><br><span class="line">        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># second fully layer</span></span><br><span class="line">        w_fc2 = self.weight_variable([<span class="number">1024</span>, self.char_num * self.classes])</span><br><span class="line">        b_fc2 = self.bias_variable([self.char_num * self.classes])</span><br><span class="line">        <span class="comment"># 神经网络处理, (1024, 1) --&gt; (4 * 62, 1)</span></span><br><span class="line">        y_conv = tf.add(tf.matmul(h_fc1_drop, w_fc2), b_fc2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_conv</span><br></pre></td></tr></table></figure></div><h3 id="predict-captcha-py"><a href="#predict-captcha-py" class="headerlink" title="predict_captcha.py"></a>predict_captcha.py</h3><p>我们把训练好的模型保存起来，之后可以使用此代码去识别任意的验证码，只要图片输入的要求符合高60宽160且1通道黑白图片即可。这个类上面我没讲解，我也没机会试，因为训练时间太久了我还没训出来呢，据说GPU训练需要4~5个小时，CPU训练的话需要20小时，大家训练好了可以拿去试试。</p><div class="highlight-wrap"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PYTHON"><figure class="iseeu highlight /python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> generate_captcha</span><br><span class="line"><span class="keyword">import</span> captcha_model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    captcha = generate_captcha.GenerateCaptcha()</span><br><span class="line">    width, height, char_num, characters, classes = captcha.get_parameter()</span><br><span class="line"></span><br><span class="line">    gray_image = Image.open(sys.argv[<span class="number">1</span>]).convert(<span class="string">'L'</span>)</span><br><span class="line">    img = np.array(gray_image.getdata())</span><br><span class="line">    test_x = np.reshape(img, [height, width, <span class="number">1</span>]) / <span class="number">255.0</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, height, width, <span class="number">1</span>])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">    model = captcha_model.CaptchaModel(width, height, char_num, classes)</span><br><span class="line">    y_conv = model.create_model(x,keep_prob)</span><br><span class="line">    predict = tf.argmax(tf.reshape(y_conv, [<span class="number">-1</span>,char_num, classes]),<span class="number">2</span>)</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.95</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">False</span>, gpu_options=gpu_options)) <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        saver.restore(sess, <span class="string">"capcha_model.ckpt"</span>)</span><br><span class="line">        pre_list = sess.run(predict, feed_dict=&#123;x: [test_x], keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pre_list:</span><br><span class="line">            s = <span class="string">''</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">                s += characters[j]</span><br><span class="line">            print(s)</span><br></pre></td></tr></table></figure></div>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lab </tag>
            
            <tag> dnn </tag>
            
            <tag> tensorflow </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">





















  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Georgia:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰">
<meta name="keywords" content="dnn,review for exam">
<meta property="og:type" content="article">
<meta property="og:title" content="科大软院 - 人工智能期中考试复习">
<meta property="og:url" content="https://www.keyanjie.net/科大软院 - 人工智能期中考试复习/index.html">
<meta property="og:site_name" content="Alan&#39;s Notebook🦍">
<meta property="og:description" content="中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_1.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_2.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_3.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_4.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_5.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_6.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_7.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_8.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_优化.png">
<meta property="og:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1深度学习导火索.jpg">
<meta property="og:updated_time" content="2019-04-03T08:13:30.149Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="科大软院 - 人工智能期中考试复习">
<meta name="twitter:description" content="中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰">
<meta name="twitter:image" content="https://www.keyanjie.net/科大软院%20-%20人工智能期中考试复习/1_1.png">





  
  
  <link rel="canonical" href="https://www.keyanjie.net/科大软院 - 人工智能期中考试复习/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>科大软院 - 人工智能期中考试复习 | Alan's Notebook🦍</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="site-title">Alan's Notebook🦍</span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home🧀">

    
    
    

    

    <a href="/" rel="section">home🧀</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about🥤">

    
    
    

    

    <a href="/about/" rel="section">about🥤</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories🥪">

    
    
    

    

    <a href="/categories/" rel="section">categories🥪</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags🍨">

    
    
    

    

    <a href="/tags/" rel="section">tags🍨</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives🍯">

    
    
    

    

    <a href="/archives/" rel="section">archives🍯</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-friend-links🍻">

    
    
    

    

    <a href="/friendLinks/" rel="section">friend links🍻</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.keyanjie.net/科大软院 - 人工智能期中考试复习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Alan Ke">
      <meta itemprop="description" content="Have a nice coding!🍻">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alan's Notebook🦍">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">科大软院 - 人工智能期中考试复习

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-02 16:01:52" itemprop="dateCreated datePublished" datetime="2018-11-02T16:01:52+08:00">2018-11-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-03 16:13:30" itemprop="dateModified" datetime="2019-04-03T16:13:30+08:00">2019-04-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/科大软院 - 人工智能期中考试复习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/科大软院 - 人工智能期中考试复习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>中国科学技术大学软件学院 人工智能2018年秋课程期中考试复习🍰</p>
<a id="more"></a>
<h2 id="1、机器学习的定义"><a href="#1、机器学习的定义" class="headerlink" title="1、机器学习的定义"></a><strong>1、机器学习的定义</strong></h2><p><strong>对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，经过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。</strong></p>
<p><strong>任务T：Testing Set - 测试集</strong></p>
<p><strong>性能度量P：Loss Function - 损失函数</strong></p>
<p><strong>经验E：Training Set - 训练集</strong></p>
<h2 id="2、Loss-Function-Cost-Function-损失函数"><a href="#2、Loss-Function-Cost-Function-损失函数" class="headerlink" title="2、Loss Function/Cost Function - 损失函数"></a><strong>2、Loss Function/Cost Function - 损失函数</strong></h2><p><strong>假设函数：</strong><br>$$<br>h_{w,b}(x) = wx+b<br>$$<br><strong>在回归任务中，多使用均方误差作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>在分类任务中，多使用交叉熵作为损失函数：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}[-\hat{y}^{(i)}\log(h_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-h_{w,b}(x^{(i)}))]<br>$$</p>
<h2 id="3、Sigmoid-Function和Softmax-Function"><a href="#3、Sigmoid-Function和Softmax-Function" class="headerlink" title="3、Sigmoid Function和Softmax Function"></a><strong>3、Sigmoid Function和Softmax Function</strong></h2><p><strong>Sigmoid Function不具体表示哪一个函数，而是表示一类S型函数，常用的有逻辑函数σ(z)：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$<br><strong>Softmax Function，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z “压缩”到另一个K维实向量 σ(z)  中，使得每一个元素的范围都在(0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：</strong><br>$$<br>\sigma(z)_{j} = \frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}} \quad j = 1,…,K<br>$$</p>
<h2 id="4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><a href="#4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE" class="headerlink" title="4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE"></a><strong>4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</strong></h2><p><strong>Logistic Regression的假设函数如下：</strong><br>$$<br>\sigma(z) = \frac{1}{1+e^{-z}} \quad z(x) = wx+b<br>$$</p>
<p><strong>σ(z)分别对w和b求导，结果为：</strong><br>$$<br>\frac{\partial \sigma(z)}{\partial w} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial w}= \sigma(z)(1-\sigma(z))*x<br>$$</p>
<p>$$<br>\frac{\partial \sigma(z)}{\partial b} = \frac{\mathrm{d} \sigma(z)}{\mathrm{d} z} \frac{\partial z}{\partial b}= \sigma(z)(1-\sigma(z))<br>$$</p>
<p><strong>如果使用MSE作为损失函数的话，那写出来是这样的：</strong><br>$$<br>L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$<br><strong>当我们使用梯度下降来进行凸优化的时候，分别需要计算L(w,b)对w和b的偏导数：</strong></p>
<p>$$<br>\frac{\partial L(w,b)}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))x^{(i)}<br>$$</p>
<p>$$<br>\frac{\partial L(w,b)}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)}) - \hat{y}^{(i)})\sigma_{w,b}(x^{(i)})(1-\sigma_{w,b}(x^{(i)}))<br>$$</p>
<p><strong>所以在σ(x)接近于1或者0的时候，也就是预测的结果和真实结果很相近或者很不相近的时候，σ(x)和1-σ(x)中总有一个会特别小，这样会导致梯度很小，从而使得优化速度大大减缓。</strong></p>
<p><strong>而当使用Cross Entropy作为损失函数时，损失函数为：</strong><br>$$<br>L(w,b) = \frac{1}{m}\sum_{i=1}^{m}(-\hat{y}^{(i)}\log(\sigma_{w,b}(x^{(i)})) - (1-\hat{y}^{(i)})\log(1-\sigma_{w,b}(x^{(i)})))<br>$$</p>
<p><strong>L(w,b)分别对w和b求偏导，结果如下：</strong><br>$$<br>\frac{\partial L(w,b) }{\partial w}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})x^{(i)}<br>$$</p>
<p>$$<br>\frac{\partial L(w,b) }{\partial b}= \frac{1}{m}\sum_{i=1}^{m}(\sigma_{w,b}(x^{(i)})-\hat{y}^{(i)})<br>$$</p>
<p><strong>这样梯度始终和预测值与真实值之差挂钩，预测值与真实值偏离很大时，梯度也会很大，偏离很小时，梯度会很小。所以我们更倾向于使用Cross Entropy而不使用MSE。函数图如下所示：</strong><br><img src="/科大软院 - 人工智能期中考试复习/1_1.png" alt="Total Loss 函数图"></p>
<h2 id="5、一堆优化方法"><a href="#5、一堆优化方法" class="headerlink" title="5、一堆优化方法"></a><strong>5、一堆优化方法</strong></h2><p><strong>这堆优化方法只有一个目标：寻求合适的w和b（两个参数情况），使得L(w,b)损失函数的值最小。</strong></p>
<h3 id="1-Gradient-Descent-梯度下降"><a href="#1-Gradient-Descent-梯度下降" class="headerlink" title="(1) Gradient Descent - 梯度下降"></a><strong>(1) Gradient Descent - 梯度下降</strong></h3><p><strong>如果需要找到一个函数的局部极小值，必须朝着函数上当前点所对应梯度（或者是近似梯度）的反方向，前进规定步长的距离进行迭代搜索。</strong></p>
<p>$$<br>w’ \leftarrow w - \eta \frac{\partial L(w,b)}{\partial w}<br>$$</p>
<p>$$<br>b’ \leftarrow b - \eta \frac{\partial L(w,b)}{\partial b}<br>$$</p>
<p><strong>tips: 这里不可以将更新好的w’代入到L(w,b)中然后计算b’，参数w和b都必须等计算好后一起更新。</strong></p>
<p><strong>η代表学习率，部分决定了每一次更新的步长大小，如果学习率太低会导致更新十分缓慢，学习率太高又会导致步长太大全场乱跑。</strong></p>
<h4 id="为什么要以梯度的反方向为更新方向？"><a href="#为什么要以梯度的反方向为更新方向？" class="headerlink" title="为什么要以梯度的反方向为更新方向？"></a><strong>为什么要以梯度的反方向为更新方向？</strong></h4><p><strong>因为梯度方向是函数方向导数最大的方向，所以沿着梯度方向的反方向更新的话，函数下降的变化率最大。（感谢zzj在评论中指正）</strong></p>
<h3 id="2-Stochastic-Gradient-Descent-随机梯度下降"><a href="#2-Stochastic-Gradient-Descent-随机梯度下降" class="headerlink" title="(2) Stochastic Gradient Descent - 随机梯度下降"></a><strong>(2) Stochastic Gradient Descent - 随机梯度下降</strong></h3><p><strong>随机梯度下降的损失函数要这样定义：</strong></p>
<p>$$<br>L^{(i)}(w,b) = \frac{1}{2}(h_{w,b}(x^{(i)}) - \hat{y}^{(i)})^2<br>$$</p>
<p><strong>对比梯度下降的损失函数，发现这里少了求和以及求平均值的过程，因为这里只有一个样本作为参考，这个损失函数也是关于这一个样本的损失函数。</strong></p>
<p><strong>可以这样来理解：训练集里有20个样本，现在我就当作我只拥有这20个样本的其中一个样本，然后使用这一个样本更新参数，然后再挑另一个样本，更新参数，以此类推，直到所有样本都被用完后，这一轮随机梯度下降就结束了。</strong></p>
<p><strong>和梯度下降比较一下，随机梯度下降的好处是：梯度下降每次更新都用到了所有的样本，也就是使用所有样本的信息进行一次参数更新。而随机梯度下降每次更新只用到了一个样本，如果这个训练集有m个样本，那么梯度下降更新一次参数，随机梯度下降已经更新了m次参数了。所以随机梯度下降的更新频率是梯度下降的m倍，更新速度更快；</strong></p>
<p><strong>而随机梯度下降所带来的坏处是：随机梯度下降的更新只参考了一个样本，这种参考有点管中窥豹了，是不可能顾全大局的，所以更新时候的抖动现象很明显，就是参数的前进方向乱七八糟的，但是总体来说还是向着最低点前进。</strong></p>
<h3 id="3-Mini-batch-Gradient-Descent-Mini-batch梯度下降"><a href="#3-Mini-batch-Gradient-Descent-Mini-batch梯度下降" class="headerlink" title="(3) Mini-batch Gradient Descent - Mini-batch梯度下降"></a><strong>(3) Mini-batch Gradient Descent - Mini-batch梯度下降</strong></h3><p><strong>Mini-batch梯度下降是梯度下降和随机梯度下降的中和版本，随机梯度下降每次更新只考虑一个样本，梯度下降每次更新考虑所有样本，而Mini-batch梯度下降每次更新所考虑的样本是可以被指定的，如果总共有m个样本，那就可以在1~m中任意指定。</strong></p>
<p><strong>如果每次更新时所参考的样本数合适，那么既兼顾了随机梯度下降更新速度快的特性，又兼顾了梯度下降更新的稳定性。</strong></p>
<p><strong>可以说梯度下降和随机梯度下降都是Mini-batch梯度下降的一个特例，使用梯度下降时，指定每次更新参考全部样本，而使用随机梯度下降时，每次更新只参考1个样本。</strong></p>
<p><strong>注意：虽然随机梯度下降和Mini-batch梯度下降都是基于一部分数据进行参数更新，但是更新完后查看损失函数是基于全部训练数据所得出的训练误差。</strong></p>
<h3 id="4-Adagrad算法"><a href="#4-Adagrad算法" class="headerlink" title="(4) Adagrad算法"></a><strong>(4) Adagrad算法</strong></h3><p><strong>像Adagrad这一类自适应算法都是使用了自适应的学习率，他们都有一个基本思路：在整个训练过程中使用同一个学习率是不合适的，因为在训练开始时，损失值肯定是比较大的，所以需要较大的学习率，而训练快要结束时，越来越接近最低点了，此时需要较小学习率。所以这类算法会依据某些因素，在迭代过程中，逐渐减小学习率。</strong></p>
<p><strong>比如可以按下面这个公式来设计自适应的学习率，其中t代表了迭代次数：</strong></p>
<p>$$<br>\eta^t=\frac{\eta}{\sqrt{t+1}}<br>$$</p>
<p><strong>这样学习率就会根据迭代次数来放缓学习率，从而达到学习率越来越小的目的。但是这样还是不够的，因为还要考虑到具体的函数情况。Adagrad算法不仅要求学习率跟着迭代次数变化，就是按上面的公式算出$\eta^t$，还要再除以之前所有梯度的平方平均数，$g^t$代表了第t次迭代时的梯度。</strong></p>
<p>$$<br>w^1 \leftarrow w^0 - \frac{\eta^0}{\sigma^0}g^0 \qquad \sigma^0 = \sqrt{(g^0)^2}<br>$$</p>
<p>$$<br>w^2 \leftarrow w^1 - \frac{\eta^1}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\frac{1}{2}[(g^0)^2+(g^1)^2]}<br>$$</p>
<p>$$<br>w^3 \leftarrow w^2 - \frac{\eta^2}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\frac{1}{3}[(g^0)^2+(g^1)^2 + (g^2)^2]}<br>$$</p>
<p>$$<br>……<br>$$</p>
<p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}<br>$$</p>
<p><strong>其中有$\eta^t=\frac{\eta}{\sqrt{t+1}}​$代入到最后一个式子就可以得到：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\frac{\eta}{\sqrt{t+1}}}{\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p>
<p><strong>同时约去$\frac{1}{\sqrt{t+1}}$后就可以得到：</strong></p>
<p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}}g^t<br>$$</p>
<p><strong>这就是Adagrad算法的公式了，我们可以看到这个式子中，学习率是会一直除以前面所有梯度的平方和再开根号的，这一定是一个大于0的数，所以学习率会越来越小。但是防止一开始的时候梯度就是0，如果让分母变为0会导致错误的，所以后面还要跟一个很小的正数$\epsilon​$，最终的式子是这样的：</strong><br>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2+\epsilon}}g^t<br>$$</p>
<p><strong>Adagrad算法也有很多不足：</strong></p>
<p><strong>a) 如果初始的学习率设置过大的话，这个学习率要除以一个较大梯度，那么此算法会对梯度的调节太大；</strong><br><strong>b) 在训练的中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束。</strong></p>
<h3 id="5-RMSprop算法"><a href="#5-RMSprop算法" class="headerlink" title="(5) *RMSprop算法"></a><strong>(5) *RMSprop算法</strong></h3><p><strong>我感觉最多考到Adagrad算法就行了，RMSprop应该考不到。</strong></p>
<p><strong>在凸优化问题上，Adagrad算法具有很好的效果，但是在神经网络情况下，很多问题都是非凸优化问题，即损失函数有很多局部最小值，有了Adagrad算法的改进版RMSprop算法，RMSprop算法就像是真实物理世界中一个小球在山坡上向下滑，如果滑落到一个山谷中，小球是不会立刻停在这里的，由于具有惯性的原因，小球会继续向前冲，如果惯性足够的话，可能会再次冲出山头，更有可能会落到另一个更低的山谷中。而传统的梯度下降法只会落在一个山谷中，没有机会冲出来。</strong><br>$$<br>w^1 \leftarrow w^0 - \frac{\eta}{\sigma^0}g^0 \qquad \sigma^0 = g^0<br>$$</p>
<p>$$<br>w^2 \leftarrow w^1 - \frac{\eta}{\sigma^1}g^1 \qquad \sigma^1 = \sqrt{\alpha(\sigma^0)^2 + (1-\alpha)(g^1)^2}<br>$$</p>
<p>$$<br>w^3 \leftarrow w^2 - \frac{\eta}{\sigma^2}g^2 \qquad \sigma^2 = \sqrt{\alpha(\sigma^1)^2 + (1-\alpha)(g^2)^2}<br>$$</p>
<p>$$<br>……<br>$$</p>
<p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sigma^t}g^t \qquad \sigma^t = \sqrt{\alpha(\sigma^{t-1})^2 + (1-\alpha)(g^t)^2}<br>$$</p>
<p><strong>Adagrad和RMSprop算法这两个算法很相近，不同之处在于RMSprop算法增加了一个衰减系数α来控制历史信息的获取多少。</strong></p>
<h2 id="6、Cross-Validation-交叉验证"><a href="#6、Cross-Validation-交叉验证" class="headerlink" title="6、Cross Validation - 交叉验证"></a><strong>6、Cross Validation - 交叉验证</strong></h2><p><strong>将数据集D划分成k个大小相似的互斥子集，每次用k-1个子集作为训练集，余下的子集做测试集，最终返回k个训练结果的平均值。交叉验证法评估结果的稳定性和保真性很大程度上取决于k的取值。</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_2.png" alt="cross validation"></p>
<h3 id="为什么要用交叉验证-Cross-Validation）"><a href="#为什么要用交叉验证-Cross-Validation）" class="headerlink" title="为什么要用交叉验证(Cross-Validation）"></a><strong>为什么要用交叉验证(Cross-Validation）</strong></h3><p><strong>a) 交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合；</strong><br><strong>b) 还可以从有限的数据中获取尽可能多的有效信息。</strong></p>
<p><strong>注意：交叉验证使用的仅仅是训练集！</strong></p>
<h2 id="7、偏差、方差、噪声以及泛化误差"><a href="#7、偏差、方差、噪声以及泛化误差" class="headerlink" title="7、偏差、方差、噪声以及泛化误差"></a><strong>7、偏差、方差、噪声以及泛化误差</strong></h2><h3 id="1-偏差"><a href="#1-偏差" class="headerlink" title="(1) 偏差"></a><strong>(1) 偏差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_3.png" alt="bias"></p>
<p><strong>偏差bias： 期望预测与真实标记的误差，偏差越大偏离理论值越大。</strong></p>
<p><strong>上面的ppt里，左边的图模型阶数小，所以不可能很好的拟合真实情况，所作出的预测与真实情况偏离程度大，所以偏差就大；而右边的图里模型很复杂，足够模拟出真实情况，所以与真实情况偏离程度小，偏差就小。</strong></p>
<p><strong>在一个训练集$D$上模型$f$对测试样本$x$预测输出为$f(x;D)$, 那么学习算法$f$对测试样本$x$的期望预测为：</strong></p>
<p>$$<br>\bar{f}(x)=E_D[f(x;D)]<br>$$<br><strong>这里用偏差的平方来表示偏差的计算公式：</strong></p>
<p>$$<br>Bias^2(x)=(\bar{f}(x)-\hat{y})^2<br>$$</p>
<h3 id="2-方差"><a href="#2-方差" class="headerlink" title="(2) 方差"></a><strong>(2) 方差</strong></h3><p><img src="/科大软院 - 人工智能期中考试复习/1_4.png" alt="variance"></p>
<p><strong>方差variance：预测模型的离散程度，方差越大离散程度越大。</strong></p>
<p><strong>就像上面ppt的左图，因为模型阶数小，所以模型都很集中；而右边的模型阶数大，有时能拟合的很好，有时拟合的不好，不稳定因素太大了。</strong></p>
<p><strong>使用样本数相同的不同训练集产生的方差为:</strong></p>
<p>$$<br>var(x)=E_D[(f(x;D)-\bar{f}(x))^2]<br>$$</p>
<h3 id="3-噪声"><a href="#3-噪声" class="headerlink" title="(3) *噪声"></a><strong>(3) *噪声</strong></h3><p><strong>不可能考</strong></p>
<p><strong>噪声为真实标记与数据集中的实际标记间的偏差$y_D$表示在数据集中的标记，$\hat{y}$表示真实标记，这两个可能不等）：</strong></p>
<p>$$<br>\epsilon=E_D[(y_D-\hat{y})^2]<br>$$</p>
<h3 id="4-泛化误差"><a href="#4-泛化误差" class="headerlink" title="(4) *泛化误差"></a><strong>(4) *泛化误差</strong></h3><p><strong>也不可能考</strong></p>
<p><strong>学习算法的预测误差, 或者说泛化误差(generalization error)可以分解为三个部分: 偏差(bias), 方差(variance) 和噪声(noise). 在估计学习算法性能的过程中, 我们主要关注偏差与方差。因为噪声属于不可约减的误差 (irreducible error)。</strong></p>
<p><strong>下面来用公式推导泛化误差与偏差与方差, 噪声之间的关系。</strong></p>
<p><strong>以回归任务为例, 学习算法的平方预测误差期望为：</strong></p>
<p>$$<br>Err(x)=E_D[(y_D-f(x;D))^2]<br>$$</p>
<p><strong>对算法的期望泛化误差进行分解，就会发现 泛化误差=偏差的平方+方差+噪声:</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_5.png" alt="1_5"></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_6.png" alt="1_6"></p>
<h2 id="8、欠拟合和过拟合"><a href="#8、欠拟合和过拟合" class="headerlink" title="8、欠拟合和过拟合"></a><strong>8、欠拟合和过拟合</strong></h2><p><strong>欠拟合：模型拟合不够，在训练集上拟合情况很差。往往会出现偏差大、方差小的情况；</strong></p>
<p><strong>过拟合：模型过度拟合，在训练集上拟合情况很好，但是在测试集上拟合情况很差。往往会出现偏差小、方差大的情况。</strong></p>
<p><strong>出现欠拟合时，解决办法有：</strong></p>
<p><strong>a) 增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间;</strong><br><strong>b) 尝试非线性模型，比如核SVM 、决策树、DNN等模型;</strong><br><strong>c) 如果有正则项可以减小正则项参数λ；</strong><br><strong>d) Boosting，Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。</strong></p>
<p><strong>出现过拟合时，解决办法有：</strong></p>
<p><strong>a) 交叉检验，通过交叉检验得到较优的模型参数;</strong><br><strong>b) 特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;</strong><br><strong>c) 正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;</strong><br><strong>d) 如果有正则项则可以考虑增大正则项参数 λ;</strong><br><strong>e) 增加训练数据可以有限的避免过拟合;</strong><br><strong>f) Bagging，将多个弱学习器Bagging 一下效果会好很多，比如随机森林等。</strong></p>
<h2 id="9、L1范数和L2范数"><a href="#9、L1范数和L2范数" class="headerlink" title="9、L1范数和L2范数"></a><strong>9、L1范数和L2范数</strong></h2><p><strong>本来以为会很快写完这个部分的，没想到这里的知识点看了一晚上，更没想到这里的知识完完整整拿出来，完全可以写一篇长长的博文。如果想好好学习这里的知识的话，我推荐<a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">这个博客</a>，或者是周老师的西瓜书第252页。这里我就介绍一点最基本的知识了。</strong></p>
<p><strong>L1范数：向量元素绝对值之和，即：</strong></p>
<p>$$<br>\left | \theta \right |_1=\sum_{i=1}^{n}\left | \theta_i \right |<br>$$</p>
<p><strong>L2范数：这里我查了好久的资料，我发现网上所有的资料，包括周老师的西瓜书以及邱锡鹏教授的《神经网络与深度学习》都是说，L2范数是各个元素的平方和再求平方根，即：</strong></p>
<p>$$<br>\left | \theta \right |_2=\sqrt{\sum_{i=1}^{n}\theta_i^2}<br>$$</p>
<p><strong>但是吴恩达教授、李宏毅教授课上讲的版本，以及我在学校课上（我们学校的课不就是照搬李宏毅教授的课嘛）所记录的版本，都是写的L2范数为各个元素的平方和，不再求平方根，即：</strong></p>
<p>$$<br>\left | \theta \right |_2=\sum_{i=1}^{n}\theta_i^2<br>$$</p>
<p><strong>吴恩达教授：</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_7.png" alt="1_7"></p>
<p><strong>李宏毅教授：</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_8.png" alt="1_8"></p>
<p><strong>我的笔记就不拿出来了，和前两位的差不多。在这里我还是使用这个不开根号的版本。</strong></p>
<h3 id="1-L1范数的作用"><a href="#1-L1范数的作用" class="headerlink" title="(1) L1范数的作用"></a><strong>(1) L1范数的作用</strong></h3><p><strong>L1范数可以产生稀疏权值矩阵，一定程度上也可以防止过拟合。</strong></p>
<p><strong>稀疏权值矩阵的意思是，这个矩阵中大部分元素都是0，只有很小一部分元素不为0。试想一下当我们在看周杰伦演唱会时，只会把注意力放在周董身上，不太会关注伴舞小姐姐，更不可能去关注旁边又矮又胖的音响（这一句考试别写）。而在计算机视觉领域也一样，传进来一张图片，这个图片有很多很多的像素，但是机器真正要关注的只有其中一部分元素，比如一张田地里的农民，或是草地里的狗狗之类的，其他不重要的信息就直接不看了，所以和主题无关的信息都会乘上0，有意义的信息才会保留。自然语言处理也是一样，有很多无意义的词都会乘上0，只挑选有意义的信息保留下来。关于具体怎么实现的不讲了，去看周老师的西瓜书吧。</strong></p>
<h3 id="2-L2范数的作用"><a href="#2-L2范数的作用" class="headerlink" title="(2) L2范数的作用"></a><strong>(2) L2范数的作用</strong></h3><p><strong>L2范数主要用于防止模型过拟合。</strong></p>
<p><strong>L2范数比较重要一点，L2范数的作用是权值衰减，缩小各个权值，使得函数尽可能比不加L2范数平滑很多，增大模型的泛化能力。如果损失函数加上L2范数后，是这样：</strong></p>
<p>$$<br>L(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta_j^2]<br>$$</p>
<p><strong>当使用梯度下降时，得出的式子是这样的：</strong><br>$$<br>\theta_j’ \leftarrow (1-\eta\frac{\lambda}{m})\theta_j - \eta\frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - \hat{y}^{(i)})x^{(i)}<br>$$</p>
<p><strong>每次参数更新时，都要先把原来的参数乘上一个介于0和1的数，所以参数都会先缩小一点点，再进行更新。一般来说$1-\eta\frac{\lambda}{m}$都会是一个很接近1的数，别看一个数打九九折后没什么变化，可是迭代次数多了后，这个权值是会衰减的很严重的。通过这种方法会有效降低权值，使得函数更为平滑，使得模型的泛化能力更强。</strong></p>
<h2 id="10、优化技巧"><a href="#10、优化技巧" class="headerlink" title="10、优化技巧"></a><strong>10、优化技巧</strong></h2><p><strong>训练集中常常有些特征的范围差别很大，这样会导致在优化时，不同的参数优化速度差别很大。比如房子的大小从10~2000平米都有可能，而房间数只能处在1~10这个范围内，在进行优化时，房间数的参数很快就找到了最低点，而房子大小的参数还离最低点很远，这样的话，房间数的参数就在最低点来回波动，等待房子大小的参数优化好。如果能尽量让所有参数同时优化好，那么会大大提高优化速度。</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1_优化.png" alt="1_优化"></p>
<h3 id="1-Feature-Scaling-特征缩放-归一化"><a href="#1-Feature-Scaling-特征缩放-归一化" class="headerlink" title="(1) Feature Scaling - 特征缩放/归一化"></a><strong>(1) Feature Scaling - 特征缩放/归一化</strong></h3><p><strong>归一化的思路是：让输入的值减去样本中最小的值，然后除以样本范围（也就是样本最大值减去样本最小值），这样会使得结果永远在0~1的范围内，所有特征参数差不多一个速度优化到最低点。式子如下：</strong></p>
<p>$$<br>x’=\frac{x-min}{max-min}<br>$$</p>
<h3 id="2-Mean-Normalization-均值标准化"><a href="#2-Mean-Normalization-均值标准化" class="headerlink" title="(2) Mean Normalization - 均值标准化"></a><strong>(2) Mean Normalization - 均值标准化</strong></h3><p><strong>均值标准化的思路是：让输入的值减去样本平均数μ，再除以样本标准差σ。经过这样的处理，数据的均值会是0，大小在-1~1之间。均值标准化和归一化一样，也有去除不同特征量纲不同的问题，另外机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布，所以数据以0为中心左右分布会带来很多便利。</strong><br>$$<br>{x}’=\frac{x-\mu}{\sigma}<br>$$</p>
<h2 id="11、怎么调节学习率"><a href="#11、怎么调节学习率" class="headerlink" title="11、怎么调节学习率"></a><strong>11、怎么调节学习率</strong></h2><p><strong>当我们在训练过程中，发现loss下降的很慢时，可以适当增大学习率；发现loss不降反增的时候，要降低学习率。</strong></p>
<p><strong>如果我们使用的是Adagrad算法，那么一开始学习率可以设置的相对大一点。</strong></p>
<h2 id="12、逻辑回归的局限性-深度神经网络的兴起导火索"><a href="#12、逻辑回归的局限性-深度神经网络的兴起导火索" class="headerlink" title="12、逻辑回归的局限性 - 深度神经网络的兴起导火索"></a><strong>12、逻辑回归的局限性 - 深度神经网络的兴起导火索</strong></h2><p><strong>逻辑回归不可以直接处理线性不可分的问题，例如下图所示的异或问题。处理方法是先通过一个函数，将原来的值投影转换，变成线性可分问题，再使用逻辑回归来处理。这一步叫做特征变换，先进行特征变换，再进行逻辑回归，这就是两层神经网络的雏形，也是神经网络的兴起的导火索。</strong></p>
<p><img src="/科大软院 - 人工智能期中考试复习/1深度学习导火索.jpg" alt="1深度学习导火索"></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/dnn/" rel="tag"># dnn</a>
          
            <a href="/tags/review-for-exam/" rel="tag"># review for exam</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/基于TensorFlow的CAPTCHA注册码识别实验/" rel="next" title="基于TensorFlow的CAPTCHA注册码识别实验">
                <i class="fa fa-chevron-left"></i> 基于TensorFlow的CAPTCHA注册码识别实验
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/LeetCode 79. Word Search - 非递归算法/" rel="prev" title="LeetCode 79. Word Search（非递归算法）">
                LeetCode 79. Word Search（非递归算法） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Alan Ke">
            
              <p class="site-author-name" itemprop="name">Alan Ke</p>
              <div class="site-description motion-element" itemprop="description">Have a nice coding!🍻</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/ustc-keyanjie" title="GitHub &rarr; https://github.com/ustc-keyanjie" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:keyanjie@mail.ustc.edu.cn" title="E-Mail &rarr; mailto:keyanjie@mail.ustc.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、机器学习的定义"><span class="nav-number">1.</span> <span class="nav-text">1、机器学习的定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Loss-Function-Cost-Function-损失函数"><span class="nav-number">2.</span> <span class="nav-text">2、Loss Function/Cost Function - 损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、Sigmoid-Function和Softmax-Function"><span class="nav-number">3.</span> <span class="nav-text">3、Sigmoid Function和Softmax Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、在Logistic-Regression中，为何使用Cross-Entropy作为损失函数而不使用MSE"><span class="nav-number">4.</span> <span class="nav-text">4、在Logistic Regression中，为何使用Cross Entropy作为损失函数而不使用MSE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、一堆优化方法"><span class="nav-number">5.</span> <span class="nav-text">5、一堆优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Gradient-Descent-梯度下降"><span class="nav-number">5.1.</span> <span class="nav-text">(1) Gradient Descent - 梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要以梯度的反方向为更新方向？"><span class="nav-number">5.1.1.</span> <span class="nav-text">为什么要以梯度的反方向为更新方向？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Stochastic-Gradient-Descent-随机梯度下降"><span class="nav-number">5.2.</span> <span class="nav-text">(2) Stochastic Gradient Descent - 随机梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Mini-batch-Gradient-Descent-Mini-batch梯度下降"><span class="nav-number">5.3.</span> <span class="nav-text">(3) Mini-batch Gradient Descent - Mini-batch梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Adagrad算法"><span class="nav-number">5.4.</span> <span class="nav-text">(4) Adagrad算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-RMSprop算法"><span class="nav-number">5.5.</span> <span class="nav-text">(5) *RMSprop算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、Cross-Validation-交叉验证"><span class="nav-number">6.</span> <span class="nav-text">6、Cross Validation - 交叉验证</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要用交叉验证-Cross-Validation）"><span class="nav-number">6.1.</span> <span class="nav-text">为什么要用交叉验证(Cross-Validation）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、偏差、方差、噪声以及泛化误差"><span class="nav-number">7.</span> <span class="nav-text">7、偏差、方差、噪声以及泛化误差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-偏差"><span class="nav-number">7.1.</span> <span class="nav-text">(1) 偏差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-方差"><span class="nav-number">7.2.</span> <span class="nav-text">(2) 方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-噪声"><span class="nav-number">7.3.</span> <span class="nav-text">(3) *噪声</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-泛化误差"><span class="nav-number">7.4.</span> <span class="nav-text">(4) *泛化误差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8、欠拟合和过拟合"><span class="nav-number">8.</span> <span class="nav-text">8、欠拟合和过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9、L1范数和L2范数"><span class="nav-number">9.</span> <span class="nav-text">9、L1范数和L2范数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-L1范数的作用"><span class="nav-number">9.1.</span> <span class="nav-text">(1) L1范数的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-L2范数的作用"><span class="nav-number">9.2.</span> <span class="nav-text">(2) L2范数的作用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10、优化技巧"><span class="nav-number">10.</span> <span class="nav-text">10、优化技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Feature-Scaling-特征缩放-归一化"><span class="nav-number">10.1.</span> <span class="nav-text">(1) Feature Scaling - 特征缩放/归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Mean-Normalization-均值标准化"><span class="nav-number">10.2.</span> <span class="nav-text">(2) Mean Normalization - 均值标准化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11、怎么调节学习率"><span class="nav-number">11.</span> <span class="nav-text">11、怎么调节学习率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12、逻辑回归的局限性-深度神经网络的兴起导火索"><span class="nav-number">12.</span> <span class="nav-text">12、逻辑回归的局限性 - 深度神经网络的兴起导火索</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alan Ke</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: true,
    appId: 'PLu5Kuwb1g68XrJHrzyCLtKY-gzGzoHsz',
    appKey: 'KvAe30NuziJaOBuwsu5OMR5l',
    placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  

    <!-- 代码块复制功能 -->
  <script type="text/javascript" src="/js/src/clipboard.min.js"></script>  
  <script type="text/javascript" src="/js/src/clipboard-use.js"></script>
  
<!--   <div class="bg_content">
    <canvas id="canvas"></canvas>
  </div>
  <script type="text/javascript" src="/js/src/dynamic_bg.js"></script> -->
</body>
</html>
